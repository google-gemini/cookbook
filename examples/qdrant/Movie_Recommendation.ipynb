{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTx8eQlc3cP-"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "4HZoi8yf4GEU"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9I7LG483nXB"
      },
      "source": [
        "# Gemini API: Similarity Search using Qdrant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awKO767lQIWh"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/qdrant/Qdrant_similarity_search.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1xoF_bU4NCP"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWedABji6bXJ"
      },
      "source": [
        "The [Gemini API](https://ai.google.dev/models/gemini) provides access to a family of generative AI models for generating content and solving problems. These models are designed and trained to handle both text and images as input.\n",
        "\n",
        "[Qdrant](https://qdrant.tech/) is a vector similarity search engine that offers an easy-to-use API for managing, storing, and searching vectors, with an additional payload. It is a production-ready service.\n",
        "\n",
        "In this notebook, you'll learn how to perform a similarity search on data from a website with the help of Gemini API and Qdrant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIAarGkG8VwC"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, you must install the packages and set the necessary environment variables.\n",
        "\n",
        "### Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70wYOKUC8q1m"
      },
      "source": [
        "Install google's python client SDK for the Gemini API, `google-genai`. Next, install Qdrant's Python client SDK, `qdrant-client`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mnQbBnA1GKha"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.25.1 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q \"google-genai>=1.0.0\"\n",
        "%pip install -q protobuf==4.25.1 qdrant-client[fastembed]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eomJzCa6lb90"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v-JZzORUpVR2"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai_client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSI_-yD-f6RA"
      },
      "source": [
        "### Importing and Cleaning Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8n-X5X7cBWVL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-be1b43b299ac>:6: DeprecationWarning: load_dataset is deprecated and will be removed in future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/asaniczka/tmdb-movies-dataset-2023-930k-movies?dataset_version_number=538&file_name=TMDB_movie_dataset_v11.csv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 522M/522M [00:10<00:00, 51.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "file_path = \"TMDB_movie_dataset_v11.csv\"\n",
        "\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"asaniczka/tmdb-movies-dataset-2023-930k-movies\",\n",
        "  file_path,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dLAP-cw9DS3O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Columns:\n",
            "Index(['id', 'title', 'vote_average', 'vote_count', 'status', 'release_date',\n",
            "       'revenue', 'runtime', 'adult', 'backdrop_path', 'budget', 'homepage',\n",
            "       'imdb_id', 'original_language', 'original_title', 'overview',\n",
            "       'popularity', 'poster_path', 'tagline', 'genres',\n",
            "       'production_companies', 'production_countries', 'spoken_languages',\n",
            "       'keywords'],\n",
            "      dtype='object')\n",
            "\n",
            "Missing Values per Column:\n",
            "id                            0\n",
            "title                        13\n",
            "vote_average                  0\n",
            "vote_count                    0\n",
            "status                        0\n",
            "release_date             216944\n",
            "revenue                       0\n",
            "runtime                       0\n",
            "adult                         0\n",
            "backdrop_path            891119\n",
            "budget                        0\n",
            "homepage                1079187\n",
            "imdb_id                  587175\n",
            "original_language             0\n",
            "original_title               13\n",
            "overview                 253712\n",
            "popularity                    0\n",
            "poster_path              393699\n",
            "tagline                 1036977\n",
            "genres                   496475\n",
            "production_companies     669475\n",
            "production_countries     548565\n",
            "spoken_languages         527432\n",
            "keywords                 887596\n",
            "dtype: int64\n",
            "\n",
            "Number of rows: 1206010\n",
            "Number of unique IDs: 1205120\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nDataset Columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "print(\"\\nMissing Values per Column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(f\"\\nNumber of rows: {len(df)}\")\n",
        "print(f\"Number of unique IDs: {df['id'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FvanvDRbRGz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original rows: 1206010\n",
            "Rows before dropping missing title: 1206010\n",
            "Rows after dropping missing title and dropping missing (genres and overview): 1072918\n",
            "\n",
            "Sample data after cleaning (keeping missing overviews):\n",
            "       id            title                                           overview  \\\n",
            "0   27205        Inception  Cobb, a skilled thief who commits corporate es...   \n",
            "1  157336     Interstellar  The adventures of a group of explorers who mak...   \n",
            "2     155  The Dark Knight  Batman raises the stakes in his war on crime. ...   \n",
            "3   19995           Avatar  In the 22nd century, a paraplegic Marine is di...   \n",
            "4   24428     The Avengers  When an unexpected enemy emerges and threatens...   \n",
            "\n",
            "                                        genres  \\\n",
            "0           Action, Science Fiction, Adventure   \n",
            "1            Adventure, Drama, Science Fiction   \n",
            "2               Drama, Action, Crime, Thriller   \n",
            "3  Action, Adventure, Fantasy, Science Fiction   \n",
            "4           Science Fiction, Action, Adventure   \n",
            "\n",
            "                                            keywords  \\\n",
            "0  rescue, mission, dream, airplane, paris, franc...   \n",
            "1  rescue, future, spacecraft, race against time,...   \n",
            "2  joker, sadism, chaos, secret identity, crime f...   \n",
            "3  future, society, culture clash, space travel, ...   \n",
            "4  new york city, superhero, shield, based on com...   \n",
            "\n",
            "                                             tagline  release_year  \n",
            "0               Your mind is the scene of the crime.        2010.0  \n",
            "1  Mankind was born on Earth. It was never meant ...        2014.0  \n",
            "2                  Welcome to a world without rules.        2008.0  \n",
            "3                        Enter the world of Pandora.        2009.0  \n",
            "4                            Some assembly required.        2012.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "\n",
        "columns_to_keep = ['id', 'title', 'overview', 'genres', 'keywords', 'tagline', 'release_date']\n",
        "\n",
        "df_relevant = df[columns_to_keep].copy()\n",
        "\n",
        "print(f\"Rows before dropping missing title: {len(df_relevant)}\")\n",
        "df_relevant.dropna(subset=['title'], inplace=True)\n",
        "df_relevant = df_relevant[~(df_relevant['genres'].isna() & df_relevant['overview'].isna())]\n",
        "print(f\"Rows after dropping missing title and dropping missing (genres and overview): {len(df_relevant)}\")\n",
        "\n",
        "text_cols_to_fill = ['overview', 'genres', 'keywords', 'tagline']\n",
        "for col in text_cols_to_fill:\n",
        "    df_relevant[col] = df_relevant[col].fillna('')\n",
        "\n",
        "\n",
        "def get_year(date_str):\n",
        "    if pd.isna(date_str) or not isinstance(date_str, str) or len(date_str) < 4:\n",
        "        return None\n",
        "    try:\n",
        "        return int(date_str[:4])\n",
        "    except (ValueError, TypeError):\n",
        "        return None\n",
        "\n",
        "df_relevant['release_year'] = df_relevant['release_date'].apply(get_year)\n",
        "\n",
        "print(\"\\nSample data after cleaning (keeping missing overviews):\")\n",
        "print(df_relevant[['id', 'title', 'overview', 'genres', 'keywords', 'tagline', 'release_year']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "POQgvioiZdYe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       id            title                                 text_for_embedding\n",
            "0   27205        Inception  Title: Inception\\nOverview: Cobb, a skilled th...\n",
            "1  157336     Interstellar  Title: Interstellar\\nOverview: The adventures ...\n",
            "2     155  The Dark Knight  Title: The Dark Knight\\nOverview: Batman raise...\n",
            "3   19995           Avatar  Title: Avatar\\nOverview: In the 22nd century, ...\n",
            "4   24428     The Avengers  Title: The Avengers\\nOverview: When an unexpec...\n"
          ]
        }
      ],
      "source": [
        "def create_embedding_text(row):\n",
        "    \"\"\"Combines available movie metadata into a single string for embedding.\"\"\"\n",
        "    # We can use title directly as we know all entries have title\n",
        "    title_str = f\"Title: {row['title']}\"\n",
        "    overview_str = f\"Overview: {row['overview']}\" if row['overview'] else \"\"\n",
        "    year_str = f\"Release Year: {int(row['release_year'])}\" if pd.notna(row['release_year']) else \"\"\n",
        "    genre_str = f\"Genres: {row['genres']}\" if row['genres'] else \"\"\n",
        "    keywords_str = f\"Keywords: {row['keywords']}\" if row['keywords'] else \"\"\n",
        "    tagline_str = f\"Tagline: {row['tagline']}\" if row['tagline'] else \"\"\n",
        "\n",
        "    parts = [\n",
        "        title_str,\n",
        "        overview_str,\n",
        "        year_str,\n",
        "        genre_str,\n",
        "        keywords_str,\n",
        "        tagline_str\n",
        "    ]\n",
        "    return \"\\n\".join(part for part in parts if part)\n",
        "\n",
        "df_relevant['text_for_embedding'] = df_relevant.apply(create_embedding_text, axis=1)\n",
        "\n",
        "# We can observe how data is now structured in df_relevant\n",
        "print(df_relevant[['id', 'title', 'text_for_embedding']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9aASY5Uf_RO"
      },
      "source": [
        "### Sampling 5k movies out of the 1M collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U7xpaVeLZxYc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Taking a random sample of 5000 movies for development.\n",
            "Working with 5000 movies for the next steps.\n",
            "             id                              title  release_year\n",
            "1063931  178009               Fathers of the Sport        2008.0\n",
            "427990    76618  Dragon Tales: It's Cool to be Me!        2002.0\n",
            "19431     38006             The Abominable Snowman        1957.0\n",
            "123459   131907                      God Is on Air        2002.0\n",
            "1140151  746134                  Run, Jackson, Run        1972.0\n",
            "\n",
            "Final sample DataFrame structure for embedding/indexing:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 5000 entries, 1063931 to 988347\n",
            "Data columns (total 8 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   id                  5000 non-null   int64  \n",
            " 1   text_for_embedding  5000 non-null   object \n",
            " 2   title               5000 non-null   object \n",
            " 3   overview            5000 non-null   object \n",
            " 4   genres              5000 non-null   object \n",
            " 5   keywords            5000 non-null   object \n",
            " 6   tagline             5000 non-null   object \n",
            " 7   release_year        4328 non-null   float64\n",
            "dtypes: float64(1), int64(1), object(6)\n",
            "memory usage: 351.6+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "SAMPLE_SIZE = 5000\n",
        "\n",
        "if len(df_relevant) > SAMPLE_SIZE:\n",
        "    print(f\"\\nTaking a random sample of {SAMPLE_SIZE} movies for development.\")\n",
        "    df_sample = df_relevant.sample(n=SAMPLE_SIZE, random_state=42)\n",
        "else:\n",
        "    print(f\"\\nCleaned dataset size ({len(df_relevant)}) is smaller than or equal to SAMPLE_SIZE. Using the full cleaned dataset.\")\n",
        "    df_sample = df_relevant\n",
        "\n",
        "print(f\"Working with {len(df_sample)} movies for the next steps.\")\n",
        "print(df_sample[['id', 'title', 'release_year']].head())\n",
        "\n",
        "columns_for_payload = ['title', 'overview', 'genres', 'keywords', 'tagline', 'release_year']\n",
        "columns_final = ['id', 'text_for_embedding'] + columns_for_payload\n",
        "df_sample = df_sample[columns_final]\n",
        "\n",
        "print(\"\\nFinal sample DataFrame structure for embedding/indexing:\")\n",
        "print(df_sample.info())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OQqy7lxgajVI"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient, models\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "COLLECTION_NAME = \"tmdb_movies_sample\"\n",
        "\n",
        "VECTOR_SIZE = 768\n",
        "DISTANCE_METRIC = models.Distance.COSINE\n",
        "\n",
        "\n",
        "client = QdrantClient(\":memory:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If3cSgEHgGlJ"
      },
      "source": [
        "Extracing embeddings of movies in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qWODHe__bAeO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch embedding function 'get_embeddings_batch' defined using model: models/embedding-001\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from google.api_core import exceptions, retry\n",
        "\n",
        "MODEL_FOR_EMBEDDING = \"models/embedding-001\"\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "QDRANT_BATCH_SIZE = 768\n",
        "\n",
        "\n",
        "@retry.Retry(timeout=3000)\n",
        "def get_embeddings_batch(texts: list[str], task_type=\"RETRIEVAL_DOCUMENT\") -> list[list[float]] | None:\n",
        "    \"\"\"\n",
        "    Generates embeddings for a batch of texts using Gemini API with retry.\n",
        "\n",
        "    Args:\n",
        "        texts: A list of strings to embed.\n",
        "        task_type: The task type for the embedding model.\n",
        "\n",
        "    Returns:\n",
        "        A list of embedding vectors (list of floats), or None if a non-retryable error occurs.\n",
        "    \"\"\"\n",
        "    if not texts:\n",
        "        return []\n",
        "    try:\n",
        "        response = genai_client.models.embed_content(\n",
        "          model=MODEL_FOR_EMBEDDING,\n",
        "          contents=texts,\n",
        "          config={\n",
        "            \"task_type\":task_type,\n",
        "          }\n",
        "        )\n",
        "        return response.embeddings\n",
        "    except exceptions.RetryError as e:\n",
        "        print(f\"Embedding batch failed after retries: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "print(f\"Batch embedding function 'get_embeddings_batch' defined using model: {MODEL_FOR_EMBEDDING}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-LbEZvKghOW"
      },
      "source": [
        "### Creating collections for storing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "m0TDkDU7bPq5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Existing collection 'tmdb_movies_sample' deleted.\n",
            "Collection 'tmdb_movies_sample' created successfully.\n"
          ]
        }
      ],
      "source": [
        "# In case someone tries running the whole notebook again we want to create the collection again\n",
        "\n",
        "try:\n",
        "    client.delete_collection(collection_name=COLLECTION_NAME)\n",
        "    print(f\"Existing collection '{COLLECTION_NAME}' deleted.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error deleting collection (it might not exist): {e}\")\n",
        "\n",
        "try:\n",
        "    client.create_collection(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        vectors_config=models.VectorParams(\n",
        "            size=VECTOR_SIZE,\n",
        "            distance=DISTANCE_METRIC\n",
        "        )\n",
        "    )\n",
        "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating collection: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YhcyKADBbXze"
      },
      "outputs": [],
      "source": [
        "def create_payload(row, payload_columns):\n",
        "    payload = {}\n",
        "    for col in payload_columns:\n",
        "        value = row[col]\n",
        "        if pd.isna(value):\n",
        "            payload[col] = None\n",
        "        elif isinstance(value, (np.int64, np.int32)):\n",
        "            payload[col] = int(value)\n",
        "        elif isinstance(value, (np.float64, np.float32)):\n",
        "             payload[col] = float(value)\n",
        "        else:\n",
        "            payload[col] = value\n",
        "    return payload\n",
        "\n",
        "payload_columns = [\n",
        "    'title', 'overview', 'genres', 'keywords', 'tagline', 'release_year'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jp-omB2jbfpS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting batch embedding and indexing process for 5000 movies...\n",
            "Using Gemini Batch Size: 100, Qdrant Upsert Batch Size: 768\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2b1b1fa7f89454aa919f156d88e4238",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing Batches:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Batch embedding and indexing finished.\n",
            "Total items processed (attempted embedding): 5000\n",
            "Total items failed embedding: 0\n",
            "Total points successfully prepared for upsert: 5000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Starting batch embedding and indexing process for {len(df_sample)} movies...\")\n",
        "print(f\"Using Gemini Batch Size: {BATCH_SIZE}, Qdrant Upsert Batch Size: {QDRANT_BATCH_SIZE}\")\n",
        "\n",
        "points_to_upsert_buffer = []\n",
        "total_processed = 0\n",
        "total_failed_embedding = 0\n",
        "total_upserted = 0\n",
        "\n",
        "num_batches = (len(df_sample) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "for i in tqdm(range(0, len(df_sample), BATCH_SIZE), total=num_batches, desc=\"Processing Batches\"):\n",
        "\n",
        "    batch_df = df_sample.iloc[i : i + BATCH_SIZE]\n",
        "    batch_texts = batch_df['text_for_embedding'].tolist()\n",
        "    batch_ids = batch_df['id'].tolist()\n",
        "\n",
        "    if not batch_texts:\n",
        "        continue\n",
        "\n",
        "    batch_embeddings = get_embeddings_batch(batch_texts, task_type=\"RETRIEVAL_DOCUMENT\")\n",
        "\n",
        "    if batch_embeddings and len(batch_embeddings) == len(batch_texts):\n",
        "        for j in range(len(batch_ids)):\n",
        "            item_id = batch_ids[j]\n",
        "            item_embedding = batch_embeddings[j]\n",
        "            row_data = batch_df.iloc[j]\n",
        "\n",
        "            payload = create_payload(row_data, payload_columns)\n",
        "\n",
        "            point = models.PointStruct(\n",
        "                id=int(item_id),\n",
        "                vector=item_embedding.values,\n",
        "                payload=payload\n",
        "            )\n",
        "            points_to_upsert_buffer.append(point)\n",
        "\n",
        "        total_processed += len(batch_ids)\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to get embeddings for batch starting at index {i}. Skipping {len(batch_ids)} items.\")\n",
        "        total_failed_embedding += len(batch_ids)\n",
        "        continue\n",
        "\n",
        "    if len(points_to_upsert_buffer) >= QDRANT_BATCH_SIZE or (i + BATCH_SIZE >= len(df_sample)):\n",
        "        if points_to_upsert_buffer:\n",
        "            try:\n",
        "                client.upsert(\n",
        "                    collection_name=COLLECTION_NAME,\n",
        "                    points=points_to_upsert_buffer,\n",
        "                    wait=False\n",
        "                )\n",
        "                total_upserted += len(points_to_upsert_buffer)\n",
        "                points_to_upsert_buffer = []\n",
        "            except Exception as e:\n",
        "                print(f\"Error upserting chunk to Qdrant: {e}\")\n",
        "                points_to_upsert_buffer = []\n",
        "                time.sleep(5)\n",
        "                # Following best practices we should take pauses before accessing collection again after erros\n",
        "\n",
        "if points_to_upsert_buffer:\n",
        "    print(f\"Upserting final remaining chunk of {len(points_to_upsert_buffer)} points.\")\n",
        "    try:\n",
        "        client.upsert(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            points=points_to_upsert_buffer,\n",
        "            wait=True\n",
        "        )\n",
        "        total_upserted += len(points_to_upsert_buffer)\n",
        "        points_to_upsert_buffer = []\n",
        "    except Exception as e:\n",
        "        print(f\"Error upserting final chunk: {e}\")\n",
        "\n",
        "print(\"\\nBatch embedding and indexing finished.\")\n",
        "print(f\"Total items processed (attempted embedding): {total_processed}\")\n",
        "print(f\"Total items failed embedding: {total_failed_embedding}\")\n",
        "print(f\"Total points successfully prepared for upsert: {total_upserted}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "v6l-OtOFsmeW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification: Collection 'tmdb_movies_sample' now contains 5000 points.\n"
          ]
        }
      ],
      "source": [
        "# Waiting for collection to settle\n",
        "time.sleep(5)\n",
        "\n",
        "try:\n",
        "    count = client.count(collection_name=COLLECTION_NAME, exact=True)\n",
        "    print(f\"\\nVerification: Collection '{COLLECTION_NAME}' now contains {count.count} points.\") # it should print 5000\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error verifying collection count: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i3gAZ30qtGZo"
      },
      "outputs": [],
      "source": [
        "def recommend_movies(query_text, top_k=5):\n",
        "    \"\"\"\n",
        "    Finds movies similar to the query_text using the Qdrant index.\n",
        "\n",
        "    Args:\n",
        "        query_text (str): The user's query (e.g., movie title, description, theme).\n",
        "        top_k (int): The maximum number of recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary contains the\n",
        "              payload (movie details) and similarity score of a recommended movie.\n",
        "              Returns an empty list if query embedding fails or no results found.\n",
        "    \"\"\"\n",
        "    print(f\"\\nSearching for recommendations based on: '{query_text}'\")\n",
        "\n",
        "    query_embedding = get_embeddings_batch(query_text, task_type=\"RETRIEVAL_QUERY\")[0].values\n",
        "\n",
        "    if query_embedding is None:\n",
        "        print(\"Error: Could not generate embedding for the query.\")\n",
        "        return []\n",
        "\n",
        "    try:\n",
        "        search_result = client.search(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            query_vector=query_embedding,\n",
        "            limit=top_k,\n",
        "            with_payload=True\n",
        "        )\n",
        "\n",
        "        recommendations = []\n",
        "        if search_result:\n",
        "            print(f\"Found {len(search_result)} potential recommendations:\")\n",
        "            for hit in search_result:\n",
        "                recommendation = {\n",
        "                    \"id\": hit.id,\n",
        "                    \"score\": hit.score,\n",
        "                    \"payload\": hit.payload\n",
        "                }\n",
        "                recommendations.append(recommendation)\n",
        "        else:\n",
        "            print(\"No recommendations found matching the query.\")\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Qdrant search: {e}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "T3aRrQZltXM5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching for recommendations based on: 'spy and action based movies'\n",
            "Found 5 potential recommendations:\n",
            "\n",
            "--- Recommendations ---\n",
            "  - Score: 0.6738\n",
            "    Title: Goldsnake: Anonima Killers\n",
            "    Genre: Action, Adventure\n",
            "    Year: 1966.0\n",
            "----------\n",
            "  - Score: 0.6693\n",
            "    Title: Assassination in Rome\n",
            "    Genre: Comedy, Crime, Mystery, Romance, Thriller\n",
            "    Year: 1965.0\n",
            "----------\n",
            "  - Score: 0.6620\n",
            "    Title: Himmat\n",
            "    Genre: Action\n",
            "    Year: 1996.0\n",
            "----------\n",
            "  - Score: 0.6615\n",
            "    Title: The Detonator\n",
            "    Genre: Action, Thriller\n",
            "    Year: 2006.0\n",
            "----------\n",
            "  - Score: 0.6581\n",
            "    Title: Monarch\n",
            "    Genre: Action\n",
            "    Year: 2015.0\n",
            "----------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-ddc6d020dad6>:23: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
            "  search_result = client.search(\n"
          ]
        }
      ],
      "source": [
        "query = \"spy and action based movies\"\n",
        "recommendations = recommend_movies(query, top_k=5)\n",
        "\n",
        "if recommendations:\n",
        "    print(\"\\n--- Recommendations ---\")\n",
        "    for rec in recommendations:\n",
        "        print(f\"  - Score: {rec['score']:.4f}\")\n",
        "        print(f\"    Title: {rec['payload'].get('title', 'N/A')}\")\n",
        "        print(f\"    Genre: {rec['payload'].get('genres', 'N/A')}\")\n",
        "        print(f\"    Year: {rec['payload'].get('release_year', 'N/A')}\")\n",
        "        print(\"-\" * 10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Movie_Recommendation.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
