{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWESX0tpdrE-"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YQvTrJpxzRlJ"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hp_P0cDzTWp"
      },
      "source": [
        "# Gemini 2.5 - Multimodal live API: Tool use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLW8VU78zZOc"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7f4kFby0E6j"
      },
      "source": [
        "This notebook provides examples of how to use tools with the multimodal live API with [Gemini 2.5](https://ai.google.dev/gemini-api/docs/models/gemini-v2).\n",
        "\n",
        "The API provides Google Search, Code Execution and Function Calling tools. The earlier Gemini models supported versions of these tools. The biggest change with Gemini 2.5 (in the Live API) is that, basically, all the tools are handled by Code Execution. With that change, you can use **multiple tools** in a single API call, and the model can use multiple tools in a single code execution block.  \n",
        "\n",
        "This tutorial assumes you are familiar with the Live API, as described in the [this tutorial](../quickstarts/Get_started_LiveAPI.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfk6YY3G5kqp"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5027929de8f"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://ai.google.dev/gemini-api/docs/sdks)** provides programmatic access to Gemini 2.5 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "46zEFO2a9FFd"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIfnvCn9HvH"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A1pkoyZb9Jm3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y13XaCvLY136"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "The client will pickup your API key from the environment variable.\n",
        "To use the live API you need to set the client version to `v1alpha`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HghvVpbU0Uap"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOov6dpG99rY"
      },
      "source": [
        "### Select a model\n",
        "\n",
        "Either select the latest stable model or one of the preview ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27Fikag0xSaB"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-live-001\" # @param [\"gemini-2.0-flash-live-001\", \"gemini-live-2.5-flash-preview\",\"gemini-2.5-flash-preview-native-audio-dialog\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLU9brx6p5YS"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMG4iLu5ZLgc"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import contextlib\n",
        "import json\n",
        "import wave\n",
        "\n",
        "from IPython.display import display, Markdown, Audio, HTML\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrb4aX5KqKKX"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmfQ-NvFI7Ct"
      },
      "source": [
        "You're going to use the Live API's audio output, the easiest way hear it in Colab is to write the `PCM` data out as a `WAV` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2aGpzlR-60Q"
      },
      "outputs": [],
      "source": [
        "@contextlib.contextmanager\n",
        "def wave_file(filename, channels=1, rate=24000, sample_width=2):\n",
        "    with wave.open(filename, \"wb\") as wf:\n",
        "        wf.setnchannels(channels)\n",
        "        wf.setsampwidth(sample_width)\n",
        "        wf.setframerate(rate)\n",
        "        yield wf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfdD9mVxqatm"
      },
      "source": [
        "Use a logger so it's easier to switch on/off debugging messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgHJgpV9Zw4E"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = logging.getLogger('Live')\n",
        "logger.setLevel('INFO')\n",
        "#logger.setLevel('DEBUG')  # Switch between \"INFO\" and \"DEBUG\" to toggle debug messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hiaxgUCZSYJ"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQoca-W7ri0y"
      },
      "source": [
        "Most of the Live API setup will be similar to the [starter tutorial](../quickstarts/Get_started_LiveAPI.ipynb). Since this tutorial doesn't focus on the realtime interactivity of the API, the code has been simplified: This code uses the Live API, but it only sends a single text prompt, and listens for a single turn of replies.\n",
        "\n",
        "You can set `modality=\"AUDIO\"` on any of the examples to get the spoken version of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwLZrmW5zR_P"
      },
      "outputs": [],
      "source": [
        "n = 0\n",
        "async def run(prompt, modality=\"TEXT\", tools=None):\n",
        "  global n\n",
        "  if tools is None:\n",
        "    tools=[]\n",
        "\n",
        "  config = {\n",
        "          \"tools\": tools,\n",
        "          \"response_modalities\": [modality]\n",
        "  }\n",
        "\n",
        "  async with client.aio.live.connect(model=MODEL_ID, config=config) as session:\n",
        "    display(Markdown(prompt))\n",
        "    display(Markdown('-------------------------------'))\n",
        "    await session.send_client_content(\n",
        "      turns={\"role\": \"user\", \"parts\": [{\"text\": prompt}]}, turn_complete=True\n",
        "    )\n",
        "\n",
        "    audio = False\n",
        "    filename = f'audio_{n}.wav'\n",
        "    with wave_file(filename) as wf:\n",
        "      async for response in session.receive():\n",
        "        logger.debug(str(response))\n",
        "        if response.server_content and response.server_content.model_turn and response.server_content.model_turn.parts and hasattr(response.server_content.model_turn.parts[0], 'text'):\n",
        "          if text := response.server_content.model_turn.parts[0].text:\n",
        "            display(Markdown(text))\n",
        "            continue\n",
        "\n",
        "        if response.server_content and response.server_content.model_turn and response.server_content.model_turn.parts and hasattr(response.server_content.model_turn.parts[0], 'data'):\n",
        "          if data := response.server_content.model_turn.parts[0].data:\n",
        "            print('.', end='')\n",
        "            wf.writeframes(data)\n",
        "            audio = True\n",
        "            continue\n",
        "\n",
        "        server_content = response.server_content\n",
        "        if server_content is not None:\n",
        "          handle_server_content(wf, server_content)\n",
        "          continue\n",
        "\n",
        "        tool_call = response.tool_call\n",
        "        if tool_call is not None:\n",
        "          await handle_tool_call(session, tool_call)\n",
        "\n",
        "\n",
        "  if audio:\n",
        "    display(Audio(filename, autoplay=True))\n",
        "    n = n+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrvxzrf0ERR"
      },
      "source": [
        "Since this tutorial demonstrates several tools, you'll need more code to handle the different types of objects it returns.\n",
        "\n",
        "- The `code_execution` tool can return `executable_code` and `code_execution_result` parts.\n",
        "- The `google_search` tool may attach a `grounding_metadata` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CypjqSb-0C-Q"
      },
      "outputs": [],
      "source": [
        "def handle_server_content(wf, server_content):\n",
        "  model_turn = server_content.model_turn\n",
        "  if model_turn:\n",
        "    for part in model_turn.parts:\n",
        "      executable_code = part.executable_code\n",
        "      if executable_code is not None:\n",
        "        display(Markdown('-------------------------------'))\n",
        "        display(Markdown(f'``` python\\n{executable_code.code}\\n```'))\n",
        "        display(Markdown('-------------------------------'))\n",
        "\n",
        "      code_execution_result = part.code_execution_result\n",
        "      if code_execution_result is not None:\n",
        "        display(Markdown('-------------------------------'))\n",
        "        display(Markdown(f'``` \\n{code_execution_result.output}\\n```'))\n",
        "        display(Markdown('-------------------------------'))\n",
        "\n",
        "  grounding_metadata = getattr(server_content, 'grounding_metadata', None)\n",
        "  if grounding_metadata is not None:\n",
        "    display(\n",
        "        HTML(grounding_metadata.search_entry_point.rendered_content))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPnXSNZ5rydM"
      },
      "source": [
        "- Finally, with the `function_declarations` tool, the API may return `tool_call` objects. To keep this code minimal, the `tool_call` handler just replies to every function call with a response of `\"ok\"`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmTKF_DtrY4U"
      },
      "outputs": [],
      "source": [
        "async def handle_tool_call(session, tool_call):\n",
        "  print(\"Tool call:\")\n",
        "  function_responses = []\n",
        "  for fc in tool_call.function_calls:\n",
        "    function_response = types.FunctionResponse(\n",
        "        id=fc.id,\n",
        "        name=fc.name,\n",
        "        response={\"result\": \"ok\"},\n",
        "    )\n",
        "    function_responses.append(function_response)\n",
        "  print('>>> ', function_responses)\n",
        "  await session.send_tool_response(function_responses=function_responses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcNu3zUNsI_p"
      },
      "source": [
        "Try running it for a first time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss9I0MRdHbP2",
        "outputId": "da3656e6-a53d-4a4d-ff41-328e677309cc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Hello?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Hello!",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " How can I help you today?\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "await run(prompt=\"Hello?\", tools=None, modality = \"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_BFBLLGp-Ye"
      },
      "source": [
        "## Simple function call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMq795G6t2hA"
      },
      "source": [
        "The function calling feature of the API Can handle a wide variety of functions. Support in the SDK is still under construction. So keep this simple just send a minimal function definition: Just the function's name.\n",
        "\n",
        "Note that in the live API function calls are independent of the chat turns. The conversation can continue while a function call is being processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y00qqZZt5L-"
      },
      "outputs": [],
      "source": [
        "turn_on_the_lights = {'name': 'turn_on_the_lights'}\n",
        "turn_off_the_lights = {'name': 'turn_off_the_lights'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dCjPmz8nEbv",
        "outputId": "390de100-cefd-42f8-f41d-d7a9a636d030"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Turn on the lights",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\nprint(default_api.turn_on_the_lights())\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-17689669929346906532',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \n{'result': 'ok'}\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "OK. I'",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "ve turned on the lights.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"Turn on the lights\"\n",
        "\n",
        "tools = [\n",
        "    {'function_declarations': [turn_on_the_lights, turn_off_the_lights]}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality = \"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmPB49ZodNKr"
      },
      "source": [
        "## Async function calling\n",
        "\n",
        "**Async function calling** lets the model manage its function calls asynchronously and without blocking the user input.\n",
        "\n",
        "You can decide how the model will behave when the function call ends between saying nothing, interrupting what it's doing or waiting to finish its current task.\n",
        "\n",
        "The next cells are going to use a slightly updated code to use the Live API so that the session stays open for 20s and accepts multiple requests that are sent to the model every 10s. Expand the next cell if you are curious about this implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M4TCYuB7XQXn"
      },
      "outputs": [],
      "source": [
        "# @title Live class with multiple messages (just run this cell)\n",
        "\n",
        "import collections.abc\n",
        "import inspect\n",
        "from asyncio.exceptions import CancelledError\n",
        "import traceback\n",
        "\n",
        "class Live:\n",
        "  def __init__(self, client):\n",
        "    self.client = client\n",
        "\n",
        "\n",
        "  async def run(self, config, functions=None, messages=None):\n",
        "    self.config = config\n",
        "    self.send_queue = asyncio.Queue()\n",
        "    self.tool_call_queue = asyncio.Queue()\n",
        "\n",
        "    try:\n",
        "      async with (\n",
        "            client.aio.live.connect(model=MODEL_ID, config=config) as session,\n",
        "            asyncio.TaskGroup() as tg\n",
        "      ):\n",
        "        self.session = session\n",
        "        recv_task = tg.create_task(self._recv())\n",
        "        send_task = tg.create_task(self._send())\n",
        "        tool_call_task = tg.create_task(self._run_tool_calls(functions))\n",
        "        read_text= tg.create_task(self._read_text(messages))\n",
        "\n",
        "\n",
        "        await read_text\n",
        "        await asyncio.sleep(20) # Keeping the socket open for 20s to wait for the FC and different messages\n",
        "\n",
        "        raise CancelledError\n",
        "    except CancelledError:\n",
        "      pass\n",
        "    except ExceptionGroup as EG:\n",
        "      traceback.print_exception(EG)\n",
        "\n",
        "  async def _recv(self):\n",
        "    try:\n",
        "      mode = None\n",
        "      while True:\n",
        "        async for response in self.session.receive():\n",
        "          logger.debug(str(response))\n",
        "          if response.server_content and response.server_content.model_turn and response.server_content.model_turn.parts and hasattr(response.server_content.model_turn.parts[0], 'text'):\n",
        "            if text := response.server_content.model_turn.parts[0].text:\n",
        "              if mode != 'text':\n",
        "                mode = 'text'\n",
        "                print()\n",
        "              print(text)\n",
        "          else:\n",
        "            if mode == 'text':\n",
        "              mode = 'other'\n",
        "              print()\n",
        "            print(f'<<<  {response.model_dump_json(exclude_none=True)}\\n')\n",
        "\n",
        "          tool_call = response.tool_call\n",
        "          if tool_call is not None:\n",
        "            await self.tool_call_queue.put(tool_call)\n",
        "\n",
        "    except asyncio.CancelledError:\n",
        "      pass\n",
        "\n",
        "  async def _send(self):\n",
        "    while True:\n",
        "      msg = await self.send_queue.get()\n",
        "      print(f'>>> {repr(msg)}\\n')\n",
        "      await self.session.send_client_content(turns=msg,turn_complete=True)\n",
        "\n",
        "  async def _run_tool_calls(self, functions):\n",
        "    while True:\n",
        "      tool_call = await self.tool_call_queue.get()\n",
        "      for fc in tool_call.function_calls:\n",
        "        fun = functions[fc.name]\n",
        "        called = fun(**fc.args)\n",
        "        if inspect.iscoroutine(called):\n",
        "          print(f'>> Starting {fc.name}\\n')\n",
        "          result = await called\n",
        "          print(f'>> Done {fc.name} >>> {repr(result)}\\n')\n",
        "          result = self._wrap_function_result(fc, result)\n",
        "          await self.session.send_tool_response(function_responses=[result])\n",
        "        elif isinstance(called, collections.abc.AsyncIterable):\n",
        "          async for result in called:\n",
        "            result.will_continue=True\n",
        "            result = self._wrap_function_result(fc, result)\n",
        "            print(f\">>> {repr(result)}\\n\")\n",
        "            await self.session.send_tool_response(function_responses=[result])\n",
        "\n",
        "          result = self._wrap_function_result(\n",
        "              fc,\n",
        "              types.FunctionResponse(will_continue=False)\n",
        "          )\n",
        "          print(f\">>> {repr(result)}\\n\")\n",
        "          await self.session.send_tool_response(\n",
        "              function_responses=[result]\n",
        "          )\n",
        "\n",
        "\n",
        "        else:\n",
        "          raise TypeError(f\"expected {fc.name} to return a coroutine, or an \"\n",
        "                          f\"AsyncIterable, got {type(fun)}\")\n",
        "\n",
        "  def _wrap_function_result(self, fc, result):\n",
        "    if result is None:\n",
        "      return types.FunctionResponse(\n",
        "          name=fc.name,\n",
        "          id=fc.id,\n",
        "          response={'result': 'ok'}\n",
        "      )\n",
        "    elif isinstance(result, types.FunctionResponse):\n",
        "      result.name = fc.name\n",
        "      result.id = fc.id\n",
        "      return result\n",
        "    else:\n",
        "      return types.FunctionResponse(\n",
        "          name=fc.name,\n",
        "          id=fc.id,\n",
        "          response= {'result': result}\n",
        "      )\n",
        "\n",
        "  async def _read_text(self, messages):\n",
        "    if messages:\n",
        "        for n, message in enumerate(messages):\n",
        "            await self.send_queue.put({\n",
        "                'role': 'user',\n",
        "                'parts': [{'text': message}]\n",
        "            })\n",
        "            if n+1 < len(messages):\n",
        "              await asyncio.sleep(5)\n",
        "    else:\n",
        "        while True:\n",
        "            message = await asyncio.to_thread(input, \"message > \")\n",
        "            if message.lower() == \"q\":\n",
        "                break\n",
        "            await self.send_queue.put({\n",
        "                'role': 'user',\n",
        "                'parts': [{'text': message}]\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVStsmfrZX9x"
      },
      "source": [
        "### Default behavior: Blocking\n",
        "\n",
        "Let's start with the default behavior. First define a mock weather function that simulates compute time by waiting 10s.\n",
        "\n",
        "The default behavior functions as a FIFO queue: the function call is added to a queue, and any subsequent requests are queued (blocked) behind it until it finishes processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSp2PU0MbfBR",
        "outputId": "6008ea77-d20d-4bef-fc21-15f2db00d87a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> {'role': 'user', 'parts': [{'text': \"What's the weather in Vegas?\"}]}\n",
            "\n",
            "<<<  {\"tool_call\":{\"function_calls\":[{\"id\":\"function-call-11363802694464679510\",\"args\":{},\"name\":\"get_weather_vegas\"}]}}\n",
            "\n",
            ">> Starting get_weather_vegas\n",
            "\n",
            ">>> {'role': 'user', 'parts': [{'text': 'In the meantime tell me about the Paris casino'}]}\n",
            "\n",
            ">> Done get_weather_vegas >>> {'weather': 'Sunny, 42 degrees'}\n",
            "\n",
            "\n",
            "It\n",
            "'s sunny and 42 degrees in Vegas.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":175,\"response_token_count\":27,\"total_token_count\":202,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":175}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":27}]}}\n",
            "\n",
            "\n",
            "The\n",
            " Paris Las Vegas is a hotel and casino located on the Las Vegas Strip in Paradise\n",
            ", Nevada. As its name suggests, its theme is the city of Paris, France; it includes a 541-foot (165 m)\n",
            " replica of the Eiffel Tower, a sign in the shape of the Montgolfier balloon, a two-thirds size Arc de Triomphe, a replica of La\n",
            " Fontaine des Mers, and a theatre called Le Théâtre des Arts. The front of the hotel is intentionally designed to resemble the Louvre Museum and the Paris Opera House.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":197,\"response_token_count\":117,\"total_token_count\":314,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":197}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":117}]}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mock function, takes 10s to process\n",
        "async def get_weather_vegas():\n",
        "  await asyncio.sleep(10)\n",
        "  return {'weather': \"Sunny, 42 degrees\"}\n",
        "\n",
        "# multiple prompts, they are going to be asked with 5s delay between each of them.\n",
        "questions = [\n",
        "    \"What's the weather in Vegas?\",\n",
        "    \"In the meantime tell me about the Paris casino\"\n",
        "]\n",
        "\n",
        "await Live(client).run(\n",
        "    messages=questions,\n",
        "    functions={\n",
        "        'get_weather_vegas': get_weather_vegas,\n",
        "    },\n",
        "    config={\n",
        "        \"response_modalities\": [\"TEXT\"],\n",
        "        \"tools\": [\n",
        "            {\n",
        "                'function_declarations': [\n",
        "                    {'name': 'get_weather_vegas',  \"behavior\": \"UNSPECIFIED\"}, # This is default behavior, equivalent to BLOCKING\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siPiuJ8LW0BC"
      },
      "source": [
        "As you can see, the model called the `get_weather_vegas` function right away, but then the second question was ignored as the model was still waiting for the function call results. It only started to answer the second question after answering the function call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI2LZVgHXNLB"
      },
      "source": [
        "### **Interrupt**: stop what you're doing and handle this result\n",
        "\n",
        "This time, `behavior` is set as `NON_BLOCKING`, which means it will use async function calling.\n",
        "\n",
        "When you do, you need to define what the model will do when it will get the result of the function call. This is managed inside of the function, or within your script that handles the funcion calls (since Automatic function calling is not available) by adding a `scheduling` value in the `FunctionResponse`.\n",
        "\n",
        "This time the `scheduling` behavior is \"**`Interrupt`**\", which means that as soon as it gets a response, the model will stop what it's saying and process the response right away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7-Sq7VpXNLC",
        "outputId": "f619e046-57c4-474f-cd17-6555c24697bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> {'role': 'user', 'parts': [{'text': \"What's the weather in Vegas?\"}]}\n",
            "\n",
            "<<<  {\"tool_call\":{\"function_calls\":[{\"id\":\"function-call-4047123869918212059\",\"args\":{},\"name\":\"get_weather_vegas\"}]}}\n",
            "\n",
            ">> Starting get_weather_vegas\n",
            "\n",
            "\n",
            "The\n",
            " weather in Vegas request is running. I'll let you know when it's done.\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":457,\"response_token_count\":34,\"total_token_count\":491,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":457}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":34}]}}\n",
            "\n",
            ">>> {'role': 'user', 'parts': [{'text': \"In the meantime tell me what you know about the Paris casino and all there's to do and see in it. Then continue to tell me about the Vegas casinos until I tell you to stom talking. Don't ask me, just talk non-stopThen can you tell me what's your favorite cirque du soleil show?\"}]}\n",
            "\n",
            "\n",
            "Okay\n",
            ", I understand. I will provide information about the Paris Las Vegas casino, then\n",
            " continue with other Vegas casinos until you tell me to stop. Finally, I will answer your question about my favorite Cirque du Soleil show. Here we go:\n",
            "\n",
            "\n",
            "**Paris Las Vegas:**\n",
            "\n",
            "The Paris Las Vegas is a hotel and casino located on the Las Vegas Strip in Paradise, Nevada. It is owned and operated by Caesars Entertainment\n",
            " Corporation. As its name suggests, the theme is Paris, France.\n",
            "\n",
            "*   **Eiffel Tower:** A half-scale replica of the Eiffel Tower is\n",
            " a prominent feature, offering stunning views of the Strip. Visitors can ascend to the top for panoramic vistas.\n",
            "*   **Arc de Triomphe:** Another\n",
            " iconic French landmark, a replica of the Arc de Triomphe, stands near the entrance.\n",
            "*   **Hotel Rooms:** The rooms are designed with French\n",
            "-inspired decor.\n",
            "*   **Casino:** A full casino with a variety of table games, slot machines, and a sportsbook.\n",
            "*   **Restaurants:**\n",
            " Many restaurants, from casual to fine dining, featuring French cuisine. Popular choices include Eiffel Tower Restaurant (inside the tower), Mon Ami Gabi (French bistro with a Strip\n",
            "-side patio), and Gordon Ramsay Steak.\n",
            "*   **Shopping:** Shops with French-themed merchandise and designer brands.\n",
            "*   **Shows:** Paris\n",
            " Las Vegas has hosted a variety of shows.\n",
            "*   **Pool:** A rooftop pool with a Parisian garden setting.\n",
            "\n",
            "**Other Vegas Casinos:**\n",
            "\n",
            "Let\n",
            "'s move on to other casinos on the Las Vegas Strip.\n",
            "\n",
            "**Bellagio:** Known for its elegance and luxury, the Bellagio features:\n",
            "\n",
            "*   **\n",
            "Fountains of Bellagio:** A mesmerizing water show choreographed to music and lights.\n",
            "*   **Bellagio Conservatory & Botanical Garden:** A beautiful display that changes seasonally.\n",
            "\n",
            "*   **Casino:** High-end casino with a wide variety of games.\n",
            "*   **Restaurants:** Fine dining options, including Picasso and Le Cirque.\n",
            "\n",
            "*   **Shows:** \"O\" by Cirque du Soleil.\n",
            "*   **Gallery of Fine Art:** Featuring works by renowned artists.\n",
            "\n",
            "**The Venetian\n",
            " and The Palazzo:** These sister resorts are connected and share a Venetian theme:\n",
            "\n",
            "*   **Grand Canal Shoppes:** Indoor canals with gondola rides.\n",
            "*\n",
            "   **Casino:** Large casino floor with various games.\n",
            "*   **Restaurants:** A wide array of dining options, including celebrity chef restaurants.\n",
            "*   **Shows\n",
            ":** Variety of shows and entertainment.\n",
            "*   **Madame Tussauds Las Vegas:** Wax museum.\n",
            "\n",
            "**Caesars Palace:** Known for its Roman\n",
            " theme and grandeur:\n",
            "\n",
            "*   **The Colosseum:** A large theater that hosts headlining performers.\n",
            "*   **Casino:** Expansive casino floor.\n",
            "*   \n",
            "**The Forum Shops at Caesars:** High-end shopping mall.\n",
            "*   **Restaurants:** Diverse dining options, including celebrity chef restaurants.\n",
            "*   **\n",
            "Garden of the Gods Pool Oasis:** A complex of pools with different themes.\n",
            "\n",
            "**MGM Grand:** One of the largest hotels in the world:\n",
            "\n",
            "*   **\n",
            "Casino:** Huge casino floor.\n",
            "*   **Restaurants:** Numerous restaurants, from casual to fine dining.\n",
            "*   **Shows:** David Copperfield, KA\n",
            " by Cirque du Soleil.\n",
            "*   **MGM Grand Garden Arena:** Hosts concerts and sporting events.\n",
            "*   **Pools:** A complex of pools and a\n",
            " lazy river.\n",
            "\n",
            "**The Cosmopolitan:** Known for its trendy and modern vibe:\n",
            "\n",
            "*   **Casino:** Stylish casino with a variety of games.\n",
            "\n",
            "*   **Restaurants:** Diverse dining options, including hidden gems and unique concepts.\n",
            "*   **Shows:** Variety of shows and entertainment.\n",
            "*   **Pools:**\n",
            " Multiple pools with different atmospheres.\n",
            "*   **Marquee Nightclub & Dayclub:** Popular nightlife destination.\n",
            "\n",
            "Okay, I'm ready to continue with\n",
            " more Vegas casinos whenever you are, or I can pause here.\n",
            "\n",
            "As for my favorite Cirque du Soleil show, I would have to say **\"O\" at\n",
            " the Bellagio**. The combination of the aquatic setting, the stunning acrobatics, and the artistic storytelling makes it a truly unforgettable experience.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":545,\"response_token_count\":876,\"total_token_count\":1421,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":545}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":876}]}}\n",
            "\n",
            ">> Done get_weather_vegas >>> FunctionResponse(\n",
            "  response={\n",
            "    'weather': 'Sunny, 42 degrees'\n",
            "  },\n",
            "  scheduling=<FunctionResponseScheduling.INTERRUPT: 'INTERRUPT'>\n",
            ")\n",
            "\n",
            "\n",
            "Okay\n",
            ", I understand. You want me to stop talking about the Vegas casinos.\n",
            "\n",
            "\n",
            "I also have the weather information for you. The weather in Vegas is Sunny and 42 degrees.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":1491,\"response_token_count\":39,\"total_token_count\":1530,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":1491}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":39}]}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Mock function, takes 10s to process\n",
        "async def get_weather_vegas():\n",
        "  await asyncio.sleep(10)\n",
        "  return types.FunctionResponse(\n",
        "      response={'weather': \"Sunny, 42 degrees\"},\n",
        "      scheduling=\"INTERRUPT\"\n",
        "  )\n",
        "\n",
        "# multiple prompts, they are going to be asked with 5s delay between each of them.\n",
        "questions = [\n",
        "    \"What's the weather in Vegas?\",\n",
        "    \"In the meantime tell me what you know about the Paris casino and all there's to do and see in it. Then continue to tell me about the Vegas casinos until I tell you to stom talking. Don't ask me, just talk non-stop\"\n",
        "    \"Then can you tell me what's your favorite cirque du soleil show?\"\n",
        "]\n",
        "\n",
        "await Live(client).run(\n",
        "    messages=questions,\n",
        "    functions={\n",
        "        'get_weather_vegas': get_weather_vegas,\n",
        "    },\n",
        "    config={\n",
        "        \"response_modalities\": [\"TEXT\"],\n",
        "        \"tools\": [\n",
        "            {\n",
        "                'function_declarations': [\n",
        "                    {'name': 'get_weather_vegas',  \"behavior\": \"NON_BLOCKING\"},\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YObbWmcW24sH"
      },
      "source": [
        "As you can see, this time, the model acknowledged our request by saying something like \"`The weather in Vegas request is running. I'll let you know when it's done`\", then continues to process what you asked it, and then when the function response comes back, it stopped what it was doing, told us about the weather, and then continued to talk about what it was talking about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLbBme4MTBy7"
      },
      "source": [
        "### **Wait until Idle**: Finish what you're doing before handling this result\n",
        "\n",
        "Once again, the `behavior` is set as `NON_BLOCKING`, which means it will use async function calling and you will have to add a `scheduling` value in the `FunctionResponse`.\n",
        "\n",
        "This time the `scheduling` behavior is \"**`When_idle`**\", which means that the model will **wait until it's finished** with what it's saying and only then tell us about what you asked for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhMQLAVqTBy8",
        "outputId": "f8326d43-9943-4a66-8ac5-33f874d91f86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> {'role': 'user', 'parts': [{'text': \"What's the weather in Vegas?\"}]}\n",
            "\n",
            "<<<  {\"tool_call\":{\"function_calls\":[{\"id\":\"function-call-2349166686780796541\",\"args\":{},\"name\":\"get_weather_vegas\"}]}}\n",
            "\n",
            ">> Starting get_weather_vegas\n",
            "\n",
            "\n",
            "The\n",
            " weather in Vegas request is running. I'll let you know when it's done.\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":457,\"response_token_count\":34,\"total_token_count\":491,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":457}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":34}]}}\n",
            "\n",
            ">>> {'role': 'user', 'parts': [{'text': \"In the meantime, without using tools, tell me what you know about the Paris casino and all there's to do and see in it. Tell me about each casino on the strip!\"}]}\n",
            "\n",
            "\n",
            "Okay\n",
            ", I can provide some information about the Paris Las Vegas casino and other casinos on the strip\n",
            ", but without real-time access to a database, my knowledge is based on my training data, which has a cutoff. Therefore, details about very recent changes\n",
            " or openings might be missing.\n",
            "\n",
            "**Paris Las Vegas:**\n",
            "\n",
            "The Paris Las Vegas is known for its iconic Eiffel Tower replica, which offers stunning views of the Strip\n",
            ". Here's what you can typically expect:\n",
            "\n",
            "*   **Theme:** French, specifically Parisian. The architecture, decor, and atmosphere aim to transport you\n",
            " to France.\n",
            "*   **Eiffel Tower Viewing Deck:** A major attraction, offering panoramic views.\n",
            "*   **Dining:** A variety of French-inspired\n",
            " restaurants, from casual cafes and bakeries to upscale dining experiences. Often, you can find excellent pastries, crepes, and classic French dishes.\n",
            "*   **Casino\n",
            ":** A standard casino floor with slots, table games, and a high-limit area.\n",
            "*   **Shows:** Paris Las Vegas typically hosts a variety of shows,\n",
            " which can include headlining musicians, tribute acts, and production shows.\n",
            "*   **Shopping:** Boutiques and shops with a French theme.\n",
            "*   **Night\n",
            ">> Done get_weather_vegas >>> FunctionResponse(\n",
            "  response={\n",
            "    'weather': 'Sunny, 42 degres'\n",
            "  },\n",
            "  scheduling=<FunctionResponseScheduling.WHEN_IDLE: 'WHEN_IDLE'>\n",
            ")\n",
            "\n",
            "life:** Bars and lounges with live music or DJs.\n",
            "*   **Pool:** A pool area designed with a French garden theme.\n",
            "\n",
            "**Other Casinos on\n",
            " the Las Vegas Strip (General Information):**\n",
            "\n",
            "It's tough to give you exhaustive details on *every* casino, as offerings change frequently, but here\n",
            "'s a summary of some of the major ones and what they're generally known for:\n",
            "\n",
            "*   **Bellagio:** High-end luxury, famous\n",
            " for its fountains, art gallery, and conservatory. Excellent dining and high-roller action.\n",
            "*   **Caesars Palace:** Roman theme, huge casino\n",
            ", many restaurants and shops, Colosseum hosts big-name entertainers.\n",
            "*   **The Venetian/The Palazzo:** Italian theme, canals, gondolas, connected\n",
            " resorts with a vast array of dining, shopping, and entertainment options.\n",
            "*   **MGM Grand:** One of the largest hotels in the world, large casino, multiple\n",
            " entertainment venues, and a variety of restaurants.\n",
            "*   **The Cosmopolitan:** Trendy, modern, known for its restaurants, bars, and unique rooms with\n",
            " balconies.\n",
            "*   **Wynn/Encore:** Luxurious, upscale, with beautiful design, high-end shopping, and fine dining.\n",
            "*   **New\n",
            " York-New York:** New York City theme, roller coaster, lively atmosphere, and a range of dining options.\n",
            "*   **Luxor:** Egyptian theme, pyramid-\n",
            "shaped building, and a variety of attractions and shows.\n",
            "*   **Mandalay Bay:** South Seas/tropical theme, large pool area with a beach\n",
            ", and a variety of restaurants and entertainment options.\n",
            "*   **The Strat (formerly Stratosphere):** Observation tower with thrill rides, casino, and dining\n",
            ".\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Changes Happen:** Casino offerings, shows, restaurants, and even themes can change frequently.\n",
            "*   **Check\n",
            " Official Websites:** The best way to get the most up-to-date information is to visit the official websites of the casinos you're interested in.\n",
            "*   **\n",
            "Reviews and Guides:** Check travel websites and reviews for recent visitor experiences and recommendations.\n",
            "\n",
            "I hope this gives you a good general overview!\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":515,\"response_token_count\":707,\"total_token_count\":1222,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":515}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":707}]}}\n",
            "\n",
            "\n",
            "The\n",
            " weather in Vegas is Sunny, 42 degrees.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":1293,\"response_token_count\":13,\"total_token_count\":1306,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":1293}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":13}]}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Mock function, takes 6s to process\n",
        "async def get_weather_vegas():\n",
        "  await asyncio.sleep(6)\n",
        "  return types.FunctionResponse(\n",
        "      response={'weather': \"Sunny, 42 degres\"},\n",
        "      scheduling=\"WHEN_IDLE\"\n",
        "  )\n",
        "\n",
        "# multiple prompts, they are going to be asked with 5s delay between each of them.\n",
        "questions = [\n",
        "    \"What's the weather in Vegas?\",\n",
        "    \"In the meantime, without using tools, tell me what you know about the Paris casino and all there's to do and see in it. Tell me about each casino on the strip!\"\n",
        "]\n",
        "\n",
        "await Live(client).run(\n",
        "    messages=questions,\n",
        "    functions={\n",
        "        'get_weather_vegas': get_weather_vegas,\n",
        "    },\n",
        "    config={\n",
        "        \"response_modalities\": [\"TEXT\"],\n",
        "        \"tools\": [\n",
        "            {\n",
        "                'function_declarations': [\n",
        "                    {'name': 'get_weather_vegas',  \"behavior\": \"NON_BLOCKING\"},\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xXyeaTR_VpM"
      },
      "source": [
        "As you can see, this time, even though it received the function call response while it was answering about the casinos (cf. `>> Done get_weather_vegas >>> [...] response={'weather': 'Sunny, 42 degres'})` line), it waited until it was finished with its current answer before telling about the weather."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMRlAIkeTq1E"
      },
      "source": [
        "### Silent: Just keep what you learned for yourself\n",
        "\n",
        "This time again, the `behavior` is set as `NON_BLOCKING`, which means it will use async function calling and will need a `scheduling` value in the `FunctionResponse`.\n",
        "\n",
        "This time the `scheduling` behavior is \"**`Silent`**\", which means that the model won't tell you when the function call is finished but it might still use that knowledge later on in the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf36tYwITq1F",
        "outputId": "0eb9d2c8-5e6e-45ba-eccc-51d2583b6a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> {'role': 'user', 'parts': [{'text': \"What's the weather in Vegas?\"}]}\n",
            "\n",
            "<<<  {\"tool_call\":{\"function_calls\":[{\"id\":\"function-call-8119127127359927044\",\"args\":{},\"name\":\"get_weather_vegas\"}]}}\n",
            "\n",
            ">> Starting get_weather_vegas\n",
            "\n",
            ">> Done get_weather_vegas >>> FunctionResponse(\n",
            "  response={\n",
            "    'weather': 'Sunny, 42 degres'\n",
            "  },\n",
            "  scheduling=<FunctionResponseScheduling.SILENT: 'SILENT'>\n",
            ")\n",
            "\n",
            "\n",
            "The\n",
            " weather in Vegas call is running. I'll let you know when it's done.\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":457,\"response_token_count\":34,\"total_token_count\":491,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":457}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":34}]}}\n",
            "\n",
            ">>> {'role': 'user', 'parts': [{'text': 'In the meantime tell me about the Paris casino.'}]}\n",
            "\n",
            "\n",
            "The\n",
            " Paris Las Vegas is a hotel and casino located on the Las Vegas Strip in Paradise\n",
            ", Nevada. As its name suggests, it has a French theme, complete with a half-scale replica of the Eiffel Tower and a two-thirds size Arc\n",
            " de Triomphe. The hotel has over 2,900 rooms and suites, and the casino features a variety of table games, slot machines, and a\n",
            " sportsbook. There are also several restaurants, shops, and entertainment options, including a theater that hosts a variety of shows.\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":547,\"response_token_count\":108,\"total_token_count\":655,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":547}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":108}]}}\n",
            "\n",
            ">>> {'role': 'user', 'parts': [{'text': 'Is the temperature over 40 degres?'}]}\n",
            "\n",
            "\n",
            "Yes, the temperature\n",
            " in Vegas is 42 degrees, which is over 40 degrees.\n",
            "\n",
            "\n",
            "\n",
            "<<<  {\"server_content\":{\"generation_complete\":true}}\n",
            "\n",
            "<<<  {\"server_content\":{\"turn_complete\":true},\"usage_metadata\":{\"prompt_token_count\":675,\"response_token_count\":21,\"total_token_count\":696,\"prompt_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":675}],\"response_tokens_details\":[{\"modality\":\"TEXT\",\"token_count\":21}]}}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Mock function, takes 5s to process\n",
        "async def get_weather_vegas():\n",
        "  time.sleep(10)\n",
        "  return types.FunctionResponse(\n",
        "      response={'weather': \"Sunny, 42 degres\"},\n",
        "      scheduling=\"SILENT\"\n",
        "  )\n",
        "\n",
        "# multiple prompts, they are going to be asked with 5s delay between each of them.\n",
        "questions = [\n",
        "    \"What's the weather in Vegas?\",\n",
        "    \"In the meantime tell me about the Paris casino.\",\n",
        "    \"Is the temperature over 40 degres?\"\n",
        "]\n",
        "\n",
        "await Live(client).run(\n",
        "    messages=questions,\n",
        "    functions={\n",
        "        'get_weather_vegas': get_weather_vegas,\n",
        "    },\n",
        "    config={\n",
        "        \"response_modalities\": [\"TEXT\"],\n",
        "        \"tools\": [\n",
        "            {\n",
        "                'function_declarations': [\n",
        "                    {'name': 'get_weather_vegas',  \"behavior\": \"NON_BLOCKING\"},\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25R0uj7F_KB0"
      },
      "source": [
        "This time, as you can see, the model did nothing when the function call ended, but when asked again about the same thing it answered without doing a new function call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCnCiTbhqE8q"
      },
      "source": [
        "## Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4ptRBNY4N8Q"
      },
      "source": [
        "The `code_execution` lets the model write and run python code. Try it on a math problem the model can't solve from memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4dURhC-QoSw",
        "outputId": "bb9af1ea-3ddd-4cfa-e209-3d747d73a2e5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Can you compute the largest prime palindrome under 100000.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Okay",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", I can help you with that. Here's my plan:\n\n1",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ".  **Generate Palindromes:** Create a list of all palindromes under 100000.\n2.  **Check for Primality:**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " Iterate through the palindromes and check if each one is prime.\n3.  **Find the Largest:** Keep track of the largest prime palindrome found so far.\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Here's the code to do that:\n\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\ndef is_palindrome(n):\n  \"\"\"Checks if a number is a palindrome.\"\"\"\n  return str(n) == str(n)[::-1]\n\n\ndef is_prime(n):\n  \"\"\"Checks if a number is prime.\"\"\"\n  if n < 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\n\nlargest_prime_palindrome = 0\nfor i in range(100000):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n\nprint(largest_prime_palindrome)\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \n98689\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "The largest prime palindrome",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " under 100000 is 98689.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt=\"Can you compute the largest prime palindrome under 100000.\"\n",
        "\n",
        "tools = [\n",
        "    {'code_execution': {}}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality=\"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueeerkpX5F-v"
      },
      "source": [
        "## Compositional Function Calling\n",
        "\n",
        "Compositional function calling refers to the ability to combine user defined functions with the `code_execution` tool. The model will write them into larger blocks of code, and then pause execution while it waits for you to send back responses for each call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzKyL_Rq5sG3",
        "outputId": "2b7e7832-594e-4e50-d228-cdc2b97faa4d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Can write some code to loop through and print integers from 1-20, and every time you hit a multiple of 3 turn on the lights, and every time you hit a multiple of 5 turn them off?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\nfor i in range(1, 21):\n    print(i)\n    if i % 3 == 0:\n        print(default_api.turn_on_the_lights())\n    if i % 5 == 0:\n        print(default_api.turn_off_the_lights())\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-8300535216206876695',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-13729063350833967962',\n",
            "  name='turn_off_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-4323069446248025976',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-10640941620628625603',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-18252625547609790825',\n",
            "  name='turn_off_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-12609532654699498694',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-18252625547609790634',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-14913615560091266299',\n",
            "  name='turn_off_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-8300535216206879578',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n",
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-13718827649018293720',\n",
            "  name='turn_off_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \n1\n2\n3\n{'result': 'ok'}\n4\n5\n{'result': 'ok'}\n6\n{'result': 'ok'}\n7\n8\n9\n{'result': 'ok'}\n10\n{'result': 'ok'}\n11\n12\n{'result': 'ok'}\n13\n14\n15\n{'result': 'ok'}\n{'result': 'ok'}\n16\n17\n18\n{'result': 'ok'}\n19\n20\n{'result': 'ok'}\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Okay",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", I've written the code to loop through integers from 1 to 2",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "0. When a number is a multiple of 3, it turns on the lights, and when it's a multiple of 5, it turns off the lights",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ". The output shows the integer and the result of the API calls when the conditions are met.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt=\"Can write some code to loop through and print integers from 1-20, and every time you hit a multiple of 3 turn on the lights, and every time you hit a multiple of 5 turn them off?\"\n",
        "\n",
        "tools = [\n",
        "    {'code_execution': {}},\n",
        "    {'function_declarations': [turn_on_the_lights, turn_off_the_lights]}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality=\"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G78sxDEcqHyO"
      },
      "source": [
        "## Google search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW10vRPN6UNp"
      },
      "source": [
        "The `google_search` tool lets the model conduct google searches. For example, try asking it about events that are too recent to be in the training data.\n",
        "\n",
        "The search will still execute in `AUDIO` mode, but you won't see the detailed results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKvWzROJic60",
        "outputId": "f4870ea5-e254-4017-a12f-009930a20cfc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "When the latest Brazil vs. Argentina soccer match happened and what was the final score?",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\nprint(google_search.search(queries=[\"latest Brazil vs Argentina soccer match date and final score\", \"Brazil vs Argentina soccer match results\"]))\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \nLooking up information on Google Search.\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "The most recent match",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " between Brazil and Argentina took place on **March 25, 20",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "25**, as part of the FIFA World Cup 2026 Qualifiers. Argentina won the match with a final score of **4-1**.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHRMsiOBaSf_k4yzWZYXjNw1W-cDhpdniXKTJFBW_XqUTNFvKmuUbf3P6ryKi-5QnN5IaIZwzwo6bE16xJrSYiOmP5DMhGN85SB0QZLNgg7cZmyJO_SwDENagIkGahIsKI_-pMGQ3rYC-ZRx4xmPih9n42JDTP2kjd-vLT9wMfazbXec0pEGRSAJ3Lnce802FC2P2YoieIkm5SoDsBK3DIXXPYTM-QNemmp9PDaxmIWAJyp_zStRz9vSrfa\">latest Brazil vs Argentina soccer match date and final score</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFq6OTaILm6J1Pta6NonuFb6XjvJe9bPYzaLgf2SlFmP8j5wVYZ7dnkQ0Zhl9sXucFlthVqWyZL9T7ckZYl3778sDxbtDATDFkdg7V3Pn0VwUXwam_6PHmrs3VH-ONB2anP8H2cIjs6L9GNeWCV1ERwHmDRCCju4jqzURKJZzElxbWtcfgLbZn-XSbOPFPYLJf4u2LbYprFJgkDUhcDVfpZAOvZdZ_X\">Brazil vs Argentina soccer match results</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt=\"When the latest Brazil vs. Argentina soccer match happened and what was the final score?\"\n",
        "\n",
        "tools = [\n",
        "   {'google_search': {}}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality=\"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM9y5rwfqKfY"
      },
      "source": [
        "## Multi-tool\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrxAQjYA6vQX"
      },
      "source": [
        "The biggest difference with the new API however is that you're no longer limited to using 1-tool per request. Try combining those tasks from the previous sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmB_4XPOslyA",
        "outputId": "307f5b4e-bdc6-4290-a953-cec527aae404"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "  Hey, I need you to do three things for me.\n\n  1. Then compute the largest prime plaindrome under 100000.\n  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n  3. Turn on the lights\n\n  Thanks!\n  ",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Okay",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", I'll do those three things for you.\n\nFirst, I'll",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " compute the largest prime palindrome under 100000.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\ndef is_palindrome(n):\n  return str(n) == str(n)[::-1]\n\ndef is_prime(n):\n  if n < 2:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\nlargest_prime_palindrome = 0\nfor i in range(99999, 2, -1):\n  if is_palindrome(i) and is_prime(i):\n    largest_prime_palindrome = i\n    break\n\nprint(largest_prime_palindrome)\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \n98689\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Second",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", I'll use Google search to lookup information about the largest earthquake in California the",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " week of Dec 5 2024.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\nconcise_search(\"largest earthquake california week of December 5 2024\", max_num_results=5)\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` \nLooking up information on Google Search.\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "Based",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " on the search results, the largest earthquake in California the week of December 5",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", 2024, appears to be a magnitude 7.0 earthquake that struck offshore Cape Mendocino on December 5, 202",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "4, at 10:44 AM Pacific Time. The earthquake occurred approximately 70-75 km southwest of Ferndale, Northern California, in",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " the Mendocino Triple Junction area where the Pacific, North America, and Juan de Fuca/Gorda plates meet. A tsunami warning was issued but later",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": " canceled. Reports indicate non-structural impacts in Humboldt County and some structural damage in the Eel River Valley area.\n\nFinally, I'll turn on the lights.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "``` python\ndefault_api.turn_on_the_lights()\n\n```",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "-------------------------------",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFc-3Amo0sDyRivxpLePuhy13jCaO8C2SRTWBGIq7kolpsONvMT6uhbYB6lmwdRBNSLRpG6LVdzPwZ4w-KbkOt73DYvGULhgCeOPYxhKuGUPXMOkfvH1tp0sGPWsDcQGj99ZR1XZ66A74Oy4z6gFxOP4OApiBPSnyYeUKgSs5qZfo7kX_Jpc2AL5GaXbgeIWX4p1wEtrzF7azrtQKVynuQAOcYzkg0GeEkxxsJ0wc1aAYYO-NU=\">largest earthquake california week of December 5 2024</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tool call:\n",
            ">>>  [FunctionResponse(\n",
            "  id='function-call-16854965890989560634',\n",
            "  name='turn_on_the_lights',\n",
            "  response={\n",
            "    'result': 'ok'\n",
            "  }\n",
            ")]\n"
          ]
        },
        {
          "data": {
            "text/markdown": "Okay",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": ", I've turned on the lights. Is there anything else I can help you with",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": "?\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\\\n",
        "  Hey, I need you to do three things for me.\n",
        "\n",
        "  1. Then compute the largest prime plaindrome under 100000.\n",
        "  2. Then use google search to lookup unformation about the largest earthquake in california the week of Dec 5 2024?\n",
        "  3. Turn on the lights\n",
        "\n",
        "  Thanks!\n",
        "  \"\"\"\n",
        "\n",
        "tools = [\n",
        "    {'google_search': {}},\n",
        "    {'code_execution': {}},\n",
        "    {'function_declarations': [turn_on_the_lights, turn_off_the_lights]}\n",
        "]\n",
        "\n",
        "await run(prompt, tools=tools, modality=\"TEXT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0OhM95KkMzl"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- For more information about the SDK see the [SDK docs](https://googleapis.github.io/python-genai/)\n",
        "- This tutorial uses the high level SDK, if you're interested in the lower-level details, try the [Websocket version of this tutorial](../quickstarts/websockets/Get_started_LiveAPI_tools.ipynb)\n",
        "- This tutorial only covers _basic_ usage of these tools for deeper (and more fun) example see the [Search tool tutorial](./Search_Grounding.ipynb)\n",
        "\n",
        "Or check the other Gemini 2.5 capabilities from the [Cookbook](../gemini-2/), in particular this other [multi-tool](../examples/LiveAPI_plotting_and_mapping.ipynb) example and the one about Gemini [spatial capabilities](../quickstarts/Spatial_understanding.ipynb)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mfk6YY3G5kqp"
      ],
      "name": "Get_started_LiveAPI_tools.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}