{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Tuning Quickstart with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Tuning.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp_CKyzxUqx6"
      },
      "source": [
        "In this notebook, you'll learn how to get started with model tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x-2x8A_vi9g"
      },
      "source": [
        "## What is model tuning?\n",
        "\n",
        "Prompt design strategies such as few shot prompting may not always produce the results you need. Use model tuning to improve a model's performance on specific tasks or help the model adhere to specific output requirements when instructions aren't sufficient and you have a set of examples that demonstrate the outputs you want.\n",
        "\n",
        "The goal of model tuning is to further improve the performance of the model for your specific task. Model tuning works by providing the model with a training dataset containing many examples of the task. For niche tasks, you can get significant improvements in model performance by tuning the model on a modest number of examples.\n",
        "\n",
        "Your training data should be structured as examples with prompt inputs and expected response outputs. The goal is to teach the model to mimic the wanted behavior or task, by giving it many examples illustrating that behavior or task.\n",
        "\n",
        "You can also tune models using example data directly in Google AI Studio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fbd6a3290a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cbcf72bcb56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e20edce-f9f0-45f1-bd7e-dce6c9cbbca7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/196.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m194.6/196.3 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m194.6/196.3 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8enrppafJPCX"
      },
      "outputs": [],
      "source": [
        "from google import genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ8p7eI4WIIQ"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dc09b1db88b7"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MYZECwlRCq"
      },
      "source": [
        "You can check your existing tuned models with the `clien.models.list()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XyWzoYFxU4r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05cbbae-ea66-450e-b17b-d0d05d1d8914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ],
      "source": [
        "for m in client.models.list():\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVVPChWGX-2K"
      },
      "source": [
        "## Prepare your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82j6NHPC5g8Q"
      },
      "source": [
        "Before you can start fine-tuning, you need a dataset to tune the model with. For the best performance, the examples in the dataset should be of high quality, diverse, and representative of real inputs and outputs.\n",
        "\n",
        "For this example, you will tune a model to generate the next number in the sequence. For example, if the input is `1`, the model should output `2`. If the input is `one hundred`, the output should be `one hundred one`.\n",
        "\n",
        "Dataset for tuning the model can be one of the following types:\n",
        "1. `Iterable` of dicts or tuples.\n",
        "2. `Mapping` of `Iterable[str]`.\n",
        "3. CSV file.\n",
        "4. JSON file.\n",
        "\n",
        "To know more about preparing a dataset for fine-tuning visit [model-tuning documentation](https://ai.google.dev/gemini-api/docs/model-tuning#prepare-dataset).\n",
        "\n",
        "Note: In general, you need between 100 and 500 examples to significantly change the behavior of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0in9dID6c7lS"
      },
      "source": [
        "The following sections illustrate how to provide the dataset as an `Iterable` or a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIHNSWr90qzN"
      },
      "source": [
        "### Training data as an `Iterable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6mcXJi5W_F"
      },
      "source": [
        "Data can be an `Iterable` of:\n",
        "* `{'text_input': text_input, 'output': output}` dicts\n",
        "* `(text_input, output)` tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J73L1obNYtoF"
      },
      "outputs": [],
      "source": [
        "# Provide data as a list of dicts\n",
        "\n",
        "dict_data =[\n",
        "    {\n",
        "          'text_input': '1',\n",
        "          'output': '2',\n",
        "    },{\n",
        "          'text_input': '3',\n",
        "          'output': '4',\n",
        "    },{\n",
        "          'text_input': '-3',\n",
        "          'output': '-2',\n",
        "    },{\n",
        "          'text_input': 'twenty two',\n",
        "          'output': 'twenty three',\n",
        "    },{\n",
        "          'text_input': 'two hundred',\n",
        "          'output': 'two hundred one',\n",
        "    },{\n",
        "          'text_input': 'ninety nine',\n",
        "          'output': 'one hundred',\n",
        "    },{\n",
        "          'text_input': '8',\n",
        "          'output': '9',\n",
        "    },{\n",
        "          'text_input': '-98',\n",
        "          'output': '-97',\n",
        "    },{\n",
        "          'text_input': '1,000',\n",
        "          'output': '1,001',\n",
        "    },{\n",
        "          'text_input': '10,100,000',\n",
        "          'output': '10,100,001',\n",
        "    },{\n",
        "          'text_input': 'thirteen',\n",
        "          'output': 'fourteen',\n",
        "    },{\n",
        "          'text_input': 'eighty',\n",
        "          'output': 'eighty one',\n",
        "    },{\n",
        "          'text_input': 'one',\n",
        "          'output': 'two',\n",
        "    },{\n",
        "          'text_input': 'three',\n",
        "          'output': 'four',\n",
        "    },{\n",
        "          'text_input': 'seven',\n",
        "          'output': 'eight',\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtCw-DsddxZe"
      },
      "source": [
        "### Training data as a CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD3_PjOI45Jw"
      },
      "source": [
        "You can provide your CSV file to the tuning API in one of the following ways:\n",
        "  * A path of type `str` or `pathlib.Path` to a local CSV file.\n",
        "  * A URL to the CSV file.\n",
        "  * The public URL of a Google Sheets file.\n",
        "\n",
        "For this example, you will provide the path to a local CSV file containing the training dataset as `pathlib.Path` to the tuning API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEBsKxH2AAu"
      },
      "source": [
        "Run the following cell to create the CSV file, `data.csv`.\n",
        "The CSV file has the default columns, `text_input` for the input and `output` for the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JW1Lb9He079n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b736955-0d0d-4908-e293-2138d9ca6b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile data.csv\n",
        "text_input,output\n",
        "1,2\n",
        "3,4\n",
        "-3,-2\n",
        "twenty two,twenty three\n",
        "two hundred,two hundred one\n",
        "ninety nine,one hundred\n",
        "8,9\n",
        "-98,-97\n",
        "\"1,000\",\"1,001\"\n",
        "\"1,01,00,000\",\"1,01,00,001\"\n",
        "thirteen,fourteen\n",
        "eighty,eighty one\n",
        "one,two\n",
        "three,four\n",
        "seven,eight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-THQjy61DgE8"
      },
      "source": [
        "If your CSV file doesn't have the default field names, you can mention your input and output field directly in the `create_tuned_model` function.\n",
        "\n",
        "```\n",
        "create_tuned_model(\n",
        "    training_data = <csv file path>,\n",
        "    ...\n",
        "    input_key= <input field name>,\n",
        "    output_key = <output field name>\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReHerc2i1p6z"
      },
      "source": [
        "Get the CSV file path as a `pathlib.Path` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZJQn89jPgG4K"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "# Provide data as a CSV file `pathlib.Path` object.\n",
        "csv_file=pathlib.Path('data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Q3BrtFBewK"
      },
      "source": [
        "### Pass your dataset as training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ijC7LvH4Bf9L"
      },
      "outputs": [],
      "source": [
        "# Here you can specify any of the supported formats, e.g. dict_data or csv_file.\n",
        "train_data = dict_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhkXRzciv3Dp"
      },
      "source": [
        "## Create tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXAMtm1RHWvC"
      },
      "source": [
        "Get the list of models available for tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bM1qNFgezRf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca106f95-c96d-4055-aaea-c74c9a5fe763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['models/gemini-1.5-flash-001-tuning']\n"
          ]
        }
      ],
      "source": [
        "# List models that support tuning\n",
        "tunable_models = [\n",
        "    m.name\n",
        "    for m in client.models.list()\n",
        "    if \"createTunedModel\" in m.supported_actions\n",
        "]\n",
        "\n",
        "print(tunable_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdMpissCVAO"
      },
      "source": [
        "Select the source model for tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "w-EBSe9wTbLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "363bbbd2-39a0-4587-d1da-437f0a50228f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-flash-001-tuning\n"
          ]
        }
      ],
      "source": [
        "# Pick the last “flash” model for tuning\n",
        "base_model = [m for m in tunable_models if 'flash' in m][-1]\n",
        "\n",
        "print(base_model)  # e.g. 'models/gemini-1.5-flash-001-tuning'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8VZYAinLWc"
      },
      "source": [
        "To create a tuned model, you need to pass the dataset you prepared earlier to  `genai.create_tuned_model` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "_q1iKDiZ_cjG"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from google.genai import types\n",
        "\n",
        "# Wrap your raw train_data into a TuningDataset\n",
        "training_dataset = types.TuningDataset(\n",
        "    examples=[\n",
        "        types.TuningExample(\n",
        "            text_input=example['text_input'],\n",
        "            output=example['output']\n",
        "        )\n",
        "        for example in train_data\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pick a unique display name for the tuned model\n",
        "name = f'generate-num-{random.randint(0, 10000)}'\n",
        "\n",
        "# Launch the tuning job\n",
        "tuning_job = client.tunings.tune(\n",
        "    base_model=base_model,              # e.g., 'models/gemini-1.5-flash-001-tuning'\n",
        "    training_dataset=training_dataset,  # your types.TuningDataset\n",
        "    config=types.CreateTuningJobConfig(\n",
        "        epoch_count=100,\n",
        "        batch_size=4,\n",
        "        learning_rate=0.001,\n",
        "        tuned_model_display_name=name,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-As7ayWDK1w8"
      },
      "source": [
        "Your tuned model is immediately added to the list of tuned models, but its status is set to \"creating\" while the model is tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "su64KgY4Uztj",
        "outputId": "6657e740-6c27-48ad-9f51-f897d844130c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:  name='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> create_time=datetime.datetime(2025, 5, 22, 4, 14, 9, 726300, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 5, 22, 4, 14, 10, 835425, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 5, 22, 4, 28, 49, 728931, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 5, 22, 4, 28, 49, 728931, tzinfo=TzInfo(UTC)) error=None description=None base_model='models/gemini-1.5-flash-001-tuning' tuned_model=TunedModel(model='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44', endpoint='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44', checkpoints=None) supervised_tuning_spec=None tuning_data_stats=None encryption_spec=None partner_model_tuning_spec=None distillation_spec=None experiment=None labels=None pipeline_job=None tuned_model_display_name=None\n",
            "Model state:  JobState.JOB_STATE_SUCCEEDED\n"
          ]
        }
      ],
      "source": [
        "model = client.tunings.get(name=tuning_job.name)\n",
        "\n",
        "print(\"Model: \", model)\n",
        "print(\"Model state: \", model.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi8X5vkQv-3_"
      },
      "source": [
        "### Check tuning progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWI-vAh4LJIz"
      },
      "source": [
        "Use `metadata` to check the state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "g08vqtxYLMxT",
        "outputId": "b1fa3605-03fb-41ff-ead3-aa009c0475fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44' state=<JobState.JOB_STATE_SUCCEEDED: 'JOB_STATE_SUCCEEDED'> create_time=datetime.datetime(2025, 5, 22, 4, 14, 9, 726300, tzinfo=TzInfo(UTC)) start_time=datetime.datetime(2025, 5, 22, 4, 14, 10, 835425, tzinfo=TzInfo(UTC)) end_time=datetime.datetime(2025, 5, 22, 4, 28, 49, 728931, tzinfo=TzInfo(UTC)) update_time=datetime.datetime(2025, 5, 22, 4, 28, 49, 728931, tzinfo=TzInfo(UTC)) error=None description=None base_model='models/gemini-1.5-flash-001-tuning' tuned_model=TunedModel(model='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44', endpoint='tunedModels/generatenum8563-kcy70r9a59pdk4nwuwy9fz44', checkpoints=None) supervised_tuning_spec=None tuning_data_stats=None encryption_spec=None partner_model_tuning_spec=None distillation_spec=None experiment=None labels=None pipeline_job=None tuned_model_display_name=None\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lQ6gSMgK-kz"
      },
      "source": [
        "Wait for the training to finish using `operation.result()`, or `operation.wait_bar()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "SOUowIv1HgSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac10ca2-5528-47b0-8bf1-a618e0fccb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waiting for tuning job to complete \n",
            "Tuning job finished with state: JOB_STATE_SUCCEEDED\n"
          ]
        }
      ],
      "source": [
        "import sys, time\n",
        "from itertools import cycle\n",
        "from google.genai.types import JobState\n",
        "\n",
        "job_name = tuning_job.name\n",
        "\n",
        "# Fetch initial status\n",
        "current_status = client.tunings.get(name=job_name)\n",
        "\n",
        "spinner = cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
        "print(\"Waiting for tuning job to complete \", end=\"\", flush=True)\n",
        "\n",
        "# Loop until the job reaches any terminal state (SUCCEEDED, FAILED, CANCELLED)\n",
        "while not current_status.has_ended:\n",
        "    sys.stdout.write(next(spinner))\n",
        "    sys.stdout.flush()\n",
        "    time.sleep(0.2)\n",
        "    sys.stdout.write(\"\\b\")\n",
        "    # Refresh status from the server\n",
        "    current_status = client.tunings.get(name=job_name)\n",
        "\n",
        "print(f\"\\nTuning job finished with state: {current_status.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cg868HzqOx5"
      },
      "source": [
        "You can cancel your tuning job any time using the `cancel()` method. Uncomment the line below and run the code cell to cancel your job before it finishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQuJ70_hqJi9"
      },
      "outputs": [],
      "source": [
        "# tuning_job.cancel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqiL0TWDqAPn"
      },
      "source": [
        "Once the tuning is complete, you can view the loss curve from the tuning results. The [loss curve](https://generativeai.devsite.corp.google.com/guide/model_tuning_guidance#recommended_configurations) shows how much the model's predictions deviate from the ideal outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bIiG57xWLhP7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "\n",
        "# model = operation.result()\n",
        "\n",
        "# snapshots = pd.DataFrame(model.tuning_task.snapshots)\n",
        "\n",
        "# sns.lineplot(data=snapshots, x = 'epoch', y='mean_loss')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoQTXb1vSBC"
      },
      "source": [
        "## Evaluate your model\n",
        "\n",
        "You can use the `genai.generate_text` method and specify the name of your model to test your model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "zO0YcuSyxydZ"
      },
      "outputs": [],
      "source": [
        "model = client.tunings.get(name=tuning_job.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "UwGrrj6hS_x2",
        "outputId": "735f0bc5-ab3d-435a-bb5e-a487d18861c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=model.tuned_model.model,\n",
        "    contents=\"55\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "YSNB2zjTx5SZ",
        "outputId": "89f3aeba-9ca6-440e-da38-37a2ad981a44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123454\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=model.tuned_model.model,\n",
        "    contents=\"123455\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "Y2YVO-m0Ut9H",
        "outputId": "fe989346-d623-40e2-c657-f0670ef08fa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "three\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=model.tuned_model.model,\n",
        "    contents=\"four\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MkTR0uTb6U",
        "outputId": "87f266ed-6a4f-482e-f3a4-8a47d7c0b511"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cinq'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# French 4\n",
        "result = client.models.generate_content(\n",
        "    model=model.tuned_model.model,\n",
        "    contents=\"quatre\"\n",
        ")\n",
        "\n",
        "print(result.text) # French 5 is \"cinq\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OruCW1zETsZw",
        "outputId": "5c66df8f-dbe4-4a5b-f4dd-c74458627f91"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'IV'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = model.generate_content('III')    # Roman numeral 3\n",
        "result.text                               # Roman numeral 4 is IV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thDdSuUDUJOx",
        "outputId": "44cfdea1-65d8-4e53-934c-87ece2b23ef3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'семь'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = model.generate_content('七')  # Japanese 7\n",
        "result.text                            # Japanese 8 is 八!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpIA1IFevQQR"
      },
      "source": [
        "It really seems to have picked up the task despite the limited examples, but \"next\" is a simple concept, see the [tuning guide](https://ai.google.dev/docs/model_tuning_guidance) for more guidance on improving performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmuQCbTYwIOx"
      },
      "source": [
        "## Update the description\n",
        "\n",
        "You can update the description of your tuned model any time using the `genai.update_tuned_model` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gAVuXT_wG3x"
      },
      "outputs": [],
      "source": [
        "# Your tuned model’s resource name\n",
        "tuned_model_name = f\"tunedModels/{name}\"\n",
        "\n",
        "# Update only the 'description' field\n",
        "tuned_model = client.models.update(\n",
        "    tuned_model_name,\n",
        "    config=types.UpdateModelConfig(\n",
        "        description=\"This is my model.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"New description:\", tuned_model.description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-c3YerBxVYs",
        "outputId": "ed8dbba4-4b28-489e-fd06-23213e30c970"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This is my model.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = genai.get_tuned_model(f'tunedModels/{name}')\n",
        "\n",
        "model.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_TpwvBB4bQ7"
      },
      "source": [
        "## Delete the model\n",
        "\n",
        "You can clean up your tuned model list by deleting models you no longer need. Use the `genai.delete_tuned_model` method to delete a model. If you canceled any tuning jobs, you may want to delete those as their performance may be unpredictable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cepfaUCvVGCo"
      },
      "outputs": [],
      "source": [
        "client.models.delete(           # positional arg only\n",
        "    f\"tunedModels/{name}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljEssIshYDEr"
      },
      "source": [
        "The model no longer exists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN_bkut_4ayL",
        "outputId": "5b3130ca-a64e-4e8e-96c8-e39cf1e5fd25"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/tunedModels/generate-num-9867?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 356.26ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'google.api_core.exceptions.NotFound'>: 404 GET https://generativelanguage.googleapis.com/v1beta/tunedModels/generate-num-9867?%24alt=json%3Benum-encoding%3Dint: Tuned model tunedModels/generate-num-9867 does not exist.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  m = genai.get_tuned_model(f'tunedModels/{name}')\n",
        "  print(m)\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}