{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Tuning Quickstart with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Tuning.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp_CKyzxUqx6"
      },
      "source": [
        "In this notebook, you'll learn how to get started with model tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x-2x8A_vi9g"
      },
      "source": [
        "## What is model tuning?\n",
        "\n",
        "Prompt design strategies such as few shot prompting may not always produce the results you need. Use model tuning to improve a model's performance on specific tasks or help the model adhere to specific output requirements when instructions aren't sufficient and you have a set of examples that demonstrate the outputs you want.\n",
        "\n",
        "The goal of model tuning is to further improve the performance of the model for your specific task. Model tuning works by providing the model with a training dataset containing many examples of the task. For niche tasks, you can get significant improvements in model performance by tuning the model on a modest number of examples.\n",
        "\n",
        "Your training data should be structured as examples with prompt inputs and expected response outputs. The goal is to teach the model to mimic the wanted behavior or task, by giving it many examples illustrating that behavior or task.\n",
        "\n",
        "You can also tune models using example data directly in Google AI Studio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41fbd6a3290a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cbcf72bcb56d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efa869e-415b-46c6-ce0d-a3644d758ba1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/199.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m194.6/199.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -U \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ8p7eI4WIIQ"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dc09b1db88b7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-MYZECwlRCq"
      },
      "source": [
        "You can check your existing tuned models with the `clien.models.list()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyWzoYFxU4r6",
        "outputId": "a1d79da9-150e-4b0f-ce16-8dc3b40cd200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.5-flash-preview-05-20\n",
            "models/gemini-2.5-flash-preview-04-17-thinking\n",
            "models/gemini-2.5-pro-preview-05-06\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-preview-image-generation\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/learnlm-2.0-flash-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/veo-2.0-generate-001\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog\n",
            "models/gemini-2.5-flash-preview-native-audio-dialog-rai-v3\n",
            "models/gemini-2.5-flash-exp-native-audio-thinking-dialog\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ],
      "source": [
        "for m in client.models.list():\n",
        "    print(m.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVVPChWGX-2K"
      },
      "source": [
        "## Prepare your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82j6NHPC5g8Q"
      },
      "source": [
        "Before you can start fine-tuning, you need a dataset to tune the model with. For the best performance, the examples in the dataset should be of high quality, diverse, and representative of real inputs and outputs.\n",
        "\n",
        "For this example, you will tune a model to generate the next number in the sequence. For example, if the input is `1`, the model should output `2`. If the input is `one hundred`, the output should be `one hundred one`.\n",
        "\n",
        "Dataset for tuning the model can be one of the following types:\n",
        "1. `Iterable` of dicts or tuples.\n",
        "2. `Mapping` of `Iterable[str]`.\n",
        "3. CSV file.\n",
        "4. JSON file.\n",
        "\n",
        "To know more about preparing a dataset for fine-tuning visit [model-tuning documentation](https://ai.google.dev/gemini-api/docs/model-tuning#prepare-dataset).\n",
        "\n",
        "Note: In general, you need between 100 and 500 examples to significantly change the behavior of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0in9dID6c7lS"
      },
      "source": [
        "The following sections illustrate how to provide the dataset as an `Iterable` or a CSV file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIHNSWr90qzN"
      },
      "source": [
        "### Training data as an `Iterable`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZ6mcXJi5W_F"
      },
      "source": [
        "Data can be an `Iterable` of:\n",
        "* `{'text_input': text_input, 'output': output}` dicts\n",
        "* `(text_input, output)` tuples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "J73L1obNYtoF"
      },
      "outputs": [],
      "source": [
        "# Provide data as a list of dicts\n",
        "\n",
        "dict_data =[\n",
        "    {\n",
        "          'text_input': '1',\n",
        "          'output': '2',\n",
        "    },{\n",
        "          'text_input': '3',\n",
        "          'output': '4',\n",
        "    },{\n",
        "          'text_input': '-3',\n",
        "          'output': '-2',\n",
        "    },{\n",
        "          'text_input': 'twenty two',\n",
        "          'output': 'twenty three',\n",
        "    },{\n",
        "          'text_input': 'two hundred',\n",
        "          'output': 'two hundred one',\n",
        "    },{\n",
        "          'text_input': 'ninety nine',\n",
        "          'output': 'one hundred',\n",
        "    },{\n",
        "          'text_input': '8',\n",
        "          'output': '9',\n",
        "    },{\n",
        "          'text_input': '-98',\n",
        "          'output': '-97',\n",
        "    },{\n",
        "          'text_input': '1,000',\n",
        "          'output': '1,001',\n",
        "    },{\n",
        "          'text_input': '10,100,000',\n",
        "          'output': '10,100,001',\n",
        "    },{\n",
        "          'text_input': 'thirteen',\n",
        "          'output': 'fourteen',\n",
        "    },{\n",
        "          'text_input': 'eighty',\n",
        "          'output': 'eighty one',\n",
        "    },{\n",
        "          'text_input': 'one',\n",
        "          'output': 'two',\n",
        "    },{\n",
        "          'text_input': 'three',\n",
        "          'output': 'four',\n",
        "    },{\n",
        "          'text_input': 'seven',\n",
        "          'output': 'eight',\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtCw-DsddxZe"
      },
      "source": [
        "### Training data as a CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD3_PjOI45Jw"
      },
      "source": [
        "You can provide your CSV file to the tuning API in one of the following ways:\n",
        "  * A path of type `str` or `pathlib.Path` to a local CSV file.\n",
        "  * A URL to the CSV file.\n",
        "  * The public URL of a Google Sheets file.\n",
        "\n",
        "For this example, you will provide the path to a local CSV file containing the training dataset as `pathlib.Path` to the tuning API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyEBsKxH2AAu"
      },
      "source": [
        "Run the following cell to create the CSV file, `data.csv`.\n",
        "The CSV file has the default columns, `text_input` for the input and `output` for the output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JW1Lb9He079n",
        "outputId": "7c478144-b4c6-4973-e887-45fe44103ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing data.csv\n"
          ]
        }
      ],
      "source": [
        "%%writefile data.csv\n",
        "text_input,output\n",
        "1,2\n",
        "3,4\n",
        "-3,-2\n",
        "twenty two,twenty three\n",
        "two hundred,two hundred one\n",
        "ninety nine,one hundred\n",
        "8,9\n",
        "-98,-97\n",
        "\"1,000\",\"1,001\"\n",
        "\"1,01,00,000\",\"1,01,00,001\"\n",
        "thirteen,fourteen\n",
        "eighty,eighty one\n",
        "one,two\n",
        "three,four\n",
        "seven,eight"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-THQjy61DgE8"
      },
      "source": [
        "If your CSV file doesn't have the default field names, you can mention your input and output field directly in the `create_tuned_model` function.\n",
        "\n",
        "```\n",
        "create_tuned_model(\n",
        "    training_data = <csv file path>,\n",
        "    ...\n",
        "    input_key= <input field name>,\n",
        "    output_key = <output field name>\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReHerc2i1p6z"
      },
      "source": [
        "Get the CSV file path as a `pathlib.Path` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZJQn89jPgG4K"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "# Provide data as a CSV file `pathlib.Path` object.\n",
        "csv_file=pathlib.Path('data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_Q3BrtFBewK"
      },
      "source": [
        "### Pass your dataset as training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ijC7LvH4Bf9L"
      },
      "outputs": [],
      "source": [
        "# Here you can specify any of the supported formats, e.g. dict_data or csv_file.\n",
        "train_data = dict_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhkXRzciv3Dp"
      },
      "source": [
        "## Create tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXAMtm1RHWvC"
      },
      "source": [
        "Get the list of models available for tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM1qNFgezRf8",
        "outputId": "2e548a71-d045-4d5f-de42-8a79108ab209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['models/gemini-1.5-flash-001-tuning']\n"
          ]
        }
      ],
      "source": [
        "# List models that support tuning\n",
        "tunable_models = [\n",
        "    m.name\n",
        "    for m in client.models.list()\n",
        "    if \"createTunedModel\" in m.supported_actions\n",
        "]\n",
        "\n",
        "print(tunable_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obdMpissCVAO"
      },
      "source": [
        "Select the source model for tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-EBSe9wTbLB",
        "outputId": "cfc34283-8ea7-4a87-ee8c-4ad8432d863b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-flash-001-tuning\n"
          ]
        }
      ],
      "source": [
        "# Pick the last “flash” model for tuning\n",
        "base_model = [m for m in tunable_models if 'flash' in m][-1]\n",
        "\n",
        "print(base_model)  # e.g. 'models/gemini-1.5-flash-001-tuning'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO8VZYAinLWc"
      },
      "source": [
        "\n",
        "To create a tuned model, you need to convert your dataset into a `TuningDataset` and pass it to the `client.tunings.tune()` method. The tuning process requires specifying the base model, training dataset, and configuration parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_q1iKDiZ_cjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe5c4dc-71b0-41ac-8383-8fab24e30207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-502602352a93>:19: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n",
            "  tuning_job = client.tunings.tune(\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from google.genai import types\n",
        "\n",
        "# Wrap your raw train_data into a TuningDataset\n",
        "training_dataset = types.TuningDataset(\n",
        "    examples=[\n",
        "        types.TuningExample(\n",
        "            text_input=example['text_input'],\n",
        "            output=example['output']\n",
        "        )\n",
        "        for example in train_data\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pick a unique display name for the tuned model\n",
        "name = f'generate-num-{random.randint(0, 10000)}'\n",
        "\n",
        "# Launch the tuning job\n",
        "tuning_job = client.tunings.tune(\n",
        "    base_model=base_model,              # e.g., 'models/gemini-1.5-flash-001-tuning'\n",
        "    training_dataset=training_dataset,  # your types.TuningDataset\n",
        "    config=types.CreateTuningJobConfig(\n",
        "        epoch_count=100,\n",
        "        batch_size=4,\n",
        "        learning_rate=0.001,\n",
        "        tuned_model_display_name=name,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-As7ayWDK1w8"
      },
      "source": [
        "Your tuned model is immediately added to the list of tuned models, but its state is set to \"JOB_STATE_RUNNING\" while the model is tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su64KgY4Uztj",
        "outputId": "39cb1633-3626-489e-960b-a76711eee682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state:  JobState.JOB_STATE_RUNNING\n"
          ]
        }
      ],
      "source": [
        "model = client.tunings.get(name=tuning_job.name)\n",
        "print(\"Model state: \", model.state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUsjqT94nhvd"
      },
      "source": [
        "Your tuned model is immediately added to the list of tuned models, but its status is set to \"creating\" while the model is tuned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi8X5vkQv-3_"
      },
      "source": [
        "### Check tuning progress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWI-vAh4LJIz"
      },
      "source": [
        "Use `metadata` to check the state:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g08vqtxYLMxT",
        "outputId": "5e8de3a3-0b9a-4b74-fdcb-d556b2620d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"name\": \"tunedModels/generatenum392-dd0p9rp2dktdfzwgrakk71f7h\",\n",
            "    \"state\": \"JOB_STATE_RUNNING\",\n",
            "    \"create_time\": \"2025-06-04T07:54:56.387731Z\",\n",
            "    \"start_time\": \"2025-06-04T07:54:56.473059Z\",\n",
            "    \"update_time\": \"2025-06-04T07:54:56.387731Z\",\n",
            "    \"base_model\": \"models/gemini-1.5-flash-001-tuning\",\n",
            "    \"tuned_model\": {\n",
            "        \"model\": \"tunedModels/generatenum392-dd0p9rp2dktdfzwgrakk71f7h\",\n",
            "        \"endpoint\": \"tunedModels/generatenum392-dd0p9rp2dktdfzwgrakk71f7h\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(json.dumps(model.to_json_dict(), indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetch initial status using `current_status = client.tunings.get(name=job_name)`\n",
        "then Loop until the job reaches any terminal state (SUCCEEDED, FAILED, CANCELLED)"
      ],
      "metadata": {
        "id": "kn6eGt89ohnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOUowIv1HgSE",
        "outputId": "b18ae9e1-0e9a-4da1-8891-5491a795d388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning job finished with state: JOB_STATE_SUCCEEDED\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import time\n",
        "from itertools import cycle\n",
        "from IPython.display import display, clear_output\n",
        "from google.genai.types import JobState\n",
        "\n",
        "job_name = tuning_job.name\n",
        "\n",
        "# Fetch initial status\n",
        "current_status = client.tunings.get(name=job_name)\n",
        "\n",
        "spinner = cycle([\"|\", \"/\", \"-\", \"\\\\\"])\n",
        "\n",
        "# Loop until the job is done\n",
        "while not current_status.has_ended:\n",
        "    # Clear the previous output\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Print the current status with the next spinner character\n",
        "    print(f\"Waiting for tuning job to complete {next(spinner)}\")\n",
        "\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    # Refresh status from the server\n",
        "    current_status = client.tunings.get(name=job_name)\n",
        "\n",
        "# Clear the final spinner output\n",
        "clear_output(wait=True)\n",
        "\n",
        "# Print the final result\n",
        "print(f\"Tuning job finished with state: {current_status.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cg868HzqOx5"
      },
      "source": [
        "You can cancel your tuning job any time using the `cancel()` method. Uncomment the line below and run the code cell to cancel your job before it finishes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQuJ70_hqJi9"
      },
      "outputs": [],
      "source": [
        "# tuning_job.cancel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqiL0TWDqAPn"
      },
      "source": [
        "Once the tuning is complete, you can view the loss curve from the tuning results. The [loss curve](https://generativeai.devsite.corp.google.com/guide/model_tuning_guidance#recommended_configurations) shows how much the model's predictions deviate from the ideal outputs.\n",
        "\n",
        "`result()` decpricated in the new google-genai SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIiG57xWLhP7"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "\n",
        "# model = operation.result()\n",
        "\n",
        "# snapshots = pd.DataFrame(model.tuning_task.snapshots)\n",
        "\n",
        "# sns.lineplot(data=snapshots, x = 'epoch', y='mean_loss')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoQTXb1vSBC"
      },
      "source": [
        "## Evaluate your model\n",
        "\n",
        "You can use the `job_name` (model's jobname) to test your model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwGrrj6hS_x2",
        "outputId": "6906afe7-60ef-42e0-a759-21cd1b0d410a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"55\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSNB2zjTx5SZ",
        "outputId": "405e9e1b-9b2e-427c-81c0-069b09ce0eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123456\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"123455\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2YVO-m0Ut9H",
        "outputId": "6eda9b83-d036-48d3-9daf-0e2f56dc7b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "five\n"
          ]
        }
      ],
      "source": [
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"four\"\n",
        ")\n",
        "\n",
        "print(result.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2MkTR0uTb6U",
        "outputId": "b5c4baff-3982-4627-d06c-09debba64716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cinq\n"
          ]
        }
      ],
      "source": [
        "# French 4\n",
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"quatre\"\n",
        ")\n",
        "\n",
        "print(result.text) # French 5 is \"cinq\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OruCW1zETsZw",
        "outputId": "0515eaeb-6dc2-4eba-81a9-983eff8fe1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IV\n"
          ]
        }
      ],
      "source": [
        "# Roman numeral 3\n",
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"III\"\n",
        ")\n",
        "\n",
        "print(result.text) # Roman numeral 4 is IV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thDdSuUDUJOx",
        "outputId": "9b214bb1-77eb-4afd-d6c7-6c6d5607a8a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eight\n"
          ]
        }
      ],
      "source": [
        "# Japanese 7\n",
        "result = client.models.generate_content(\n",
        "    model=job_name,\n",
        "    contents=\"七\"\n",
        ")\n",
        "\n",
        "print(result.text) # Japanese 8 is 八!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpIA1IFevQQR"
      },
      "source": [
        "It really seems to have picked up the task despite the limited examples, but \"next\" is a simple concept, see the [tuning guide](https://ai.google.dev/docs/model_tuning_guidance) for more guidance on improving performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmuQCbTYwIOx"
      },
      "source": [
        "## Update the description\n",
        "\n",
        "You can update the description of your tuned model any time using the `client.models.update()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gAVuXT_wG3x",
        "outputId": "baa71a95-502d-40e2-94a7-505092452ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New description: This is my model.\n"
          ]
        }
      ],
      "source": [
        "# update only the description field\n",
        "updated_model = client.models.update(\n",
        "    model=model.name,\n",
        "    config=types.UpdateModelConfig(\n",
        "        description=\"This is my model.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"New description:\", updated_model.description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d-c3YerBxVYs",
        "outputId": "6a8f170b-df50-4c58-be0c-a9b62e6cf2a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is my model.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "refetched = client.models.get(model=model.name)\n",
        "\n",
        "refetched.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_TpwvBB4bQ7"
      },
      "source": [
        "## Delete the model\n",
        "\n",
        "You can clean up your tuned model list by deleting models you no longer need. Use the `client.models.delete()` method to delete a model. If you canceled any tuning jobs, you may want to delete those as their performance may be unpredictable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cepfaUCvVGCo",
        "outputId": "200f6ea5-5a54-4f31-cba3-91dbe386f1c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeleteModelResponse()"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "client.models.delete(model=model.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljEssIshYDEr"
      },
      "source": [
        "The model no longer exists:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN_bkut_4ayL",
        "outputId": "53d5e92e-7acb-4b52-c2c7-bb831c235aef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClientError: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'Tuned model tunedModels/generatenum392-dd0p9rp2dktdfzwgrakk71f7h does not exist.', 'status': 'NOT_FOUND'}}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # attempt to re-fetch the deleted tuned model\n",
        "    deleted_model = client.models.get(model=model.name)\n",
        "    print(deleted_model)\n",
        "\n",
        "except Exception as e:\n",
        "    # catch any other unexpected errors\n",
        "    print(f\"{type(e).__name__}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tuning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}