{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "license"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "cellView": "form",
        "id": "license-code"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Gemini API: Self-critique prompt optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Self_critique_prompt_optimization.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "Prompt engineering often involves manual trial and error. You write a prompt, evaluate the output, tweak the prompt, and repeat. This notebook demonstrates how to automate this process by having Gemini critique its own outputs and suggest prompt improvements.\n",
        "\n",
        "This technique, sometimes called **meta-prompting** or **self-critique**, uses the model to:\n",
        "\n",
        "1. Generate a response from an initial prompt\n",
        "2. Critique the quality of that response\n",
        "3. Identify specific weaknesses\n",
        "4. Rewrite the prompt to address those weaknesses\n",
        "5. Generate an improved response\n",
        "\n",
        "By the end of this notebook, you will understand how to implement an iterative prompt optimization loop that can help you develop better prompts faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-sdk"
      },
      "source": [
        "### Install SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "install"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api-key-section"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-key"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in /Users/escobarsmacbook/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages (1.2.1)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-selection-text"
      },
      "source": [
        "Select the model you want to use from the available options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "model-selection"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"  # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.5-flash-preview\", \"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-statement"
      },
      "source": [
        "## The problem: weak prompts produce weak results\n",
        "\n",
        "Consider a common scenario: you need the model to explain a technical concept, but your initial prompt is vague. The output might be generic, miss key details, or lack the structure you need.\n",
        "\n",
        "Here's a deliberately weak prompt to demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "define-task"
      },
      "outputs": [],
      "source": [
        "# Define the task context - this stays constant throughout optimization\n",
        "TASK_DESCRIPTION = \"Explain how neural networks learn\"\n",
        "\n",
        "# Initial weak prompt - vague and lacks specificity\n",
        "initial_prompt = \"Explain how neural networks learn.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial-response-text"
      },
      "source": [
        "### Generate the initial response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "initial-response"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "INITIAL PROMPT:\n",
            "============================================================\n",
            "Explain how neural networks learn.\n",
            "\n",
            "============================================================\n",
            "INITIAL RESPONSE:\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Neural networks learn through a process of **iterative refinement**, much like a student learns by trial and error, getting feedback, and adjusting their approach. The core idea is to adjust the internal parameters (weights and biases) of the network until it can accurately map input data to desired output.\n",
              "\n",
              "Here's a breakdown of the key steps involved:\n",
              "\n",
              "1.  **Initialization:**\n",
              "    *   Before learning begins, the network's **weights** (the strength of connections between neurons) and **biases** (an additional value that determines how easily a neuron activates) are set to small, random values. Think of these as the network's initial, uneducated guesses.\n",
              "\n",
              "2.  **Forward Pass (Making a Prediction):**\n",
              "    *   **Input Data:** The network is fed a piece of input data (e.g., an image, a row of numbers, text).\n",
              "    *   **Calculation:** This input data travels through the network, from the input layer, through any hidden layers, to the output layer.\n",
              "        *   At each neuron, a calculation is performed: `(sum of inputs * weights) + bias`.\n",
              "        *   This result is then passed through an **activation function** (e.g., ReLU, Sigmoid), which introduces non-linearity and determines if/how much the neuron \"fires.\"\n",
              "    *   **Output:** The network produces an output (a prediction) based on these calculations and its current weights and biases. This is its \"initial guess.\"\n",
              "\n",
              "3.  **Measuring the Error (Loss Function):**\n",
              "    *   **Comparison:** The network's prediction is compared to the **actual correct answer** (the \"ground truth\" or \"label\") that was provided with the input data during training.\n",
              "    *   **Loss Calculation:** A **loss function** (also called a cost function, e.g., Mean Squared Error for regression, Cross-Entropy for classification) quantifies how \"wrong\" the network's prediction was. A higher loss value means a larger error. The goal is to minimize this loss.\n",
              "\n",
              "4.  **Backpropagation (Credit Assignment):**\n",
              "    *   This is the crucial step where the network \"learns\" from its mistakes.\n",
              "    *   **Gradient Calculation:** The error from the loss function is propagated backward through the network, from the output layer towards the input layer. During this process, the network calculates the **gradient** (the partial derivative of the loss with respect to each weight and bias).\n",
              "    *   **Understanding Gradients:** The gradient tells us two things for each weight and bias:\n",
              "        *   **Direction:** Which way should this parameter be adjusted (increased or decreased) to reduce the loss?\n",
              "        *   **Magnitude:** How much should this parameter be adjusted, relative to others, to have the most impact on reducing the loss? (Parameters that contributed more to the error will have larger gradients).\n",
              "\n",
              "5.  **Weight and Bias Update (Optimization):**\n",
              "    *   **Gradient Descent:** An optimization algorithm, most commonly **Gradient Descent** (or its variants like Adam, RMSprop, etc.), uses the gradients calculated during backpropagation to adjust the weights and biases.\n",
              "    *   **Adjustment Rule:** Each weight and bias is updated using the formula:\n",
              "        `New Weight/Bias = Old Weight/Bias - (Learning Rate * Gradient)`\n",
              "    *   **Learning Rate:** This is a crucial hyperparameter that controls the size of the steps taken during each update.\n",
              "        *   A **high learning rate** can make the network learn quickly but risk overshooting the optimal values.\n",
              "        *   A **low learning rate** can make learning slow but more stable.\n",
              "\n",
              "6.  **Iteration (Epochs):**\n",
              "    *   The entire process (forward pass, error calculation, backpropagation, parameter update) is repeated many, many times for all the training examples in the dataset.\n",
              "    *   One complete pass through the entire training dataset is called an **epoch**.\n",
              "    *   Over thousands or millions of iterations and many epochs, the weights and biases are gradually refined. The network gets \"smarter\" with each adjustment, leading to progressively smaller errors and more accurate predictions.\n",
              "\n",
              "**In essence, neural networks learn by:**\n",
              "\n",
              "*   Making an initial prediction.\n",
              "*   Measuring how wrong that prediction was.\n",
              "*   Figuring out which internal parameters (weights and biases) contributed most to the error.\n",
              "*   Adjusting those parameters slightly in the direction that would reduce the error.\n",
              "*   Repeating this cycle countless times until the error is minimized and the network can generalize well to new, unseen data."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "initial_response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=initial_prompt\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INITIAL PROMPT:\")\n",
        "print(\"=\" * 60)\n",
        "print(initial_prompt)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIAL RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(initial_response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "critique-section"
      },
      "source": [
        "## Step 1: Critique the output\n",
        "\n",
        "Now, ask the model to critically evaluate its own response. The critique prompt should ask for specific, actionable feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "critique-prompt"
      },
      "outputs": [],
      "source": [
        "def critique_response(task, prompt, response_text):\n",
        "    \"\"\"\n",
        "    Ask the model to critique a response and identify weaknesses.\n",
        "    \"\"\"\n",
        "    critique_prompt = f\"\"\"\n",
        "You are a prompt engineering expert. Analyze the following prompt and its \n",
        "response, then provide a detailed critique.\n",
        "\n",
        "TASK: {task}\n",
        "\n",
        "PROMPT USED:\n",
        "{prompt}\n",
        "\n",
        "RESPONSE GENERATED:\n",
        "{response_text}\n",
        "\n",
        "Provide your critique in this format:\n",
        "\n",
        "STRENGTHS:\n",
        "- List what the response did well\n",
        "\n",
        "WEAKNESSES:\n",
        "- List specific problems with the response\n",
        "- Focus on: clarity, completeness, structure, accuracy, relevance\n",
        "\n",
        "PROMPT ISSUES:\n",
        "- Identify what was missing or unclear in the original prompt\n",
        "- Explain how prompt weaknesses led to response weaknesses\n",
        "\n",
        "QUALITY SCORE: [1-10]\n",
        "\"\"\"\n",
        "    \n",
        "    critique = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=critique_prompt\n",
        "    )\n",
        "    return critique.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "run-critique"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CRITIQUE OF INITIAL RESPONSE:\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "STRENGTHS:\n",
              "-   **Exceptional Clarity and Accessibility:** The response uses clear, concise language, effectively breaking down a complex topic into understandable parts. Analogies (student learning, initial guesses) are well-chosen and aid comprehension. Technical terms are introduced and explained without being overwhelming.\n",
              "-   **Outstanding Logical Structure:** The explanation follows a perfect chronological and logical flow, from initialization to iterative refinement. The use of numbered headings, bold text for key terms, and bullet points significantly enhances readability and makes the process easy to follow.\n",
              "-   **Comprehensive Coverage:** It covers all essential steps of neural network learning: Initialization, Forward Pass, Loss Function, Backpropagation, Weight/Bias Update (Optimization), and Iteration (Epochs). No critical step is omitted for a general explanation.\n",
              "-   **High Accuracy:** All technical concepts, including weights, biases, activation functions, loss functions, gradients, gradient descent, and learning rate, are accurately described. The explanation of gradients (direction and magnitude) is particularly well-articulated, simplifying a often-difficult concept.\n",
              "-   **Effective Explanation of Complex Concepts:** Backpropagation and Gradient Descent, which are often stumbling blocks in understanding neural networks, are explained with remarkable clarity and precision, focusing on their purpose and mechanism without getting bogged down in excessive mathematical detail.\n",
              "-   **Excellent Summarization:** The concluding \"In essence\" section provides a succinct and impactful recap, reinforcing the core learning mechanism.\n",
              "-   **Directly Relevant:** The response fully and completely answers the prompt without extraneous information.\n",
              "\n",
              "WEAKNESSES:\n",
              "-   This response is exceptionally strong, and identifying true weaknesses that detract from its quality for the given prompt is challenging.\n",
              "-   **Minor Opportunity for Deeper Dive (Not a flaw):** For a slightly more advanced audience, mentioning the concept of mini-batch gradient descent (more common in practice) rather than implying full-batch, or providing a conceptual visual analogy for the loss landscape, could enhance it further. However, for a general explanation, omitting these details keeps it focused and accessible, aligning well with what a broad prompt would typically require.\n",
              "\n",
              "PROMPT ISSUES:\n",
              "-   **Lack of Specificity:** The prompt \"Explain how neural networks learn\" is very open-ended. It does not specify:\n",
              "    *   **Target Audience:** Is the explanation for a beginner, an intermediate learner, or an expert?\n",
              "    *   **Desired Depth:** Should it be a high-level overview, a detailed technical explanation, or include mathematical formulations?\n",
              "    *   **Length or Format Constraints:** Are there any preferred length limits or formatting requirements (e.g., step-by-step, analogy-based, summary only)?\n",
              "-   **Impact on Response (Minimal in this case):** While a lack of specificity *can* often lead to responses that are either too simplistic or too complex for the user's actual need, in this particular instance, the model made excellent assumptions. It produced a comprehensive, well-structured, and accessible explanation that caters well to a general audience seeking a solid understanding of the topic. The model's \"intelligence\" in interpreting the implicit need for clarity and thoroughness compensated for the prompt's generic nature, resulting in an exceptionally high-quality response despite the prompt's weakness.\n",
              "\n",
              "QUALITY SCORE: 10/10"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "critique_1 = critique_response(TASK_DESCRIPTION, initial_prompt, initial_response.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE OF INITIAL RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewrite-section"
      },
      "source": [
        "## Step 2: Rewrite the prompt\n",
        "\n",
        "Based on the critique, ask the model to generate an improved prompt that addresses the identified weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rewrite-function"
      },
      "outputs": [],
      "source": [
        "def rewrite_prompt(task, original_prompt, critique):\n",
        "    \"\"\"\n",
        "    Generate an improved prompt based on the critique.\n",
        "    \"\"\"\n",
        "    rewrite_instruction = f\"\"\"\n",
        "You are a prompt engineering expert. Based on the critique below, rewrite the \n",
        "prompt to address all identified weaknesses.\n",
        "\n",
        "TASK: {task}\n",
        "\n",
        "ORIGINAL PROMPT:\n",
        "{original_prompt}\n",
        "\n",
        "CRITIQUE:\n",
        "{critique}\n",
        "\n",
        "Write an improved prompt that:\n",
        "1. Addresses all weaknesses mentioned in the critique\n",
        "2. Is clear and specific about expectations\n",
        "3. Includes relevant constraints (format, length, audience, etc.)\n",
        "4. Guides the model toward a higher quality response\n",
        "\n",
        "Return ONLY the improved prompt, nothing else. Do not include explanations \n",
        "or commentary.\n",
        "\"\"\"\n",
        "    \n",
        "    result = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=rewrite_instruction\n",
        "    )\n",
        "    return result.text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "run-rewrite"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "IMPROVED PROMPT (Iteration 1):\n",
            "============================================================\n",
            "Explain how neural networks learn.\n",
            "\n",
            "**Audience:** An intermediate learner with basic programming knowledge but no prior machine learning experience.\n",
            "\n",
            "**Depth:** Provide a detailed conceptual explanation of the learning process. Focus on the core mechanisms, their purpose, and how they contribute to learning, avoiding complex mathematical derivations. Accurately define all technical terms introduced.\n",
            "\n",
            "**Content Requirements:**\n",
            "*   Cover the essential steps: Initialization, Forward Pass, Loss Function, Backpropagation, Weight/Bias Update (Optimization), and Iteration (Epochs).\n",
            "*   Briefly introduce mini-batch gradient descent as a common, practical optimization strategy.\n",
            "*   Include a conceptual analogy for the loss landscape to aid understanding of gradient descent.\n",
            "\n",
            "**Format and Style:**\n",
            "*   Structure the explanation with clear, numbered headings for each step.\n",
            "*   Use clear, concise language.\n",
            "*   Employ well-chosen analogies to simplify complex ideas.\n",
            "*   Use bold text for key technical terms.\n",
            "*   Conclude with a succinct summary reinforcing the core learning mechanism.\n",
            "*   The explanation should be approximately 800-1200 words.\n",
            "\n",
            "Ensure the explanation is comprehensive, accurate, and highly accessible, guiding the reader through the entire learning mechanism step-by-step.\n"
          ]
        }
      ],
      "source": [
        "improved_prompt_1 = rewrite_prompt(TASK_DESCRIPTION, initial_prompt, critique_1)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED PROMPT (Iteration 1):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "improved-response-section"
      },
      "source": [
        "## Step 3: Generate response with improved prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "improved-response"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "IMPROVED RESPONSE (Iteration 1):\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Neural networks are incredibly powerful tools for tasks like image recognition, natural language processing, and medical diagnosis. At their core, they learn by trying to make sense of data, making predictions, seeing how wrong they are, and then adjusting themselves to be less wrong next time. This process, while seemingly complex, can be broken down into several understandable steps.\n",
              "\n",
              "Imagine you're trying to teach a child to identify different animals. Initially, they might make wild guesses. You provide feedback (\"That's a cat, not a dog!\"), and over time, they learn to refine their understanding. Neural networks learn in a remarkably similar, albeit mathematical, fashion.\n",
              "\n",
              "Here's a detailed conceptual explanation of how neural networks learn:\n",
              "\n",
              "---\n",
              "\n",
              "### The Learning Process: A Step-by-Step Guide\n",
              "\n",
              "#### 1. Initialization: Starting with a Guess\n",
              "\n",
              "Before a neural network can learn anything, it needs a starting point. This involves setting up its internal parameters: **weights** and **biases**.\n",
              "*   **Weights:** These are numerical values that represent the strength of connection between neurons (the fundamental processing units) in different layers. A higher weight means that input has a stronger influence on the next neuron. Think of them as the \"importance sliders\" for different pieces of information.\n",
              "*   **Biases:** These are additional numerical values attached to each neuron, acting like an adjustable threshold. They allow a neuron to activate even if all its inputs are zero, or to remain inactive even with some positive inputs, providing more flexibility. Think of them as \"offset adjusters.\"\n",
              "\n",
              "Initially, all weights and biases are set to small, random values. Why random? If they all started with the same values, every neuron in a layer would learn the exact same thing, making the network ineffective. Random initialization ensures each neuron starts with a unique perspective, allowing the network to learn diverse features. It's like giving different students slightly varied initial assumptions to encourage independent thought.\n",
              "\n",
              "#### 2. Forward Pass: Making a Prediction\n",
              "\n",
              "With the network initialized, we can now feed it some data. This is called a **forward pass**.\n",
              "1.  **Input:** The raw data (e.g., pixel values of an image, words in a sentence) is fed into the network's **input layer**.\n",
              "2.  **Processing through Layers:** This input then travels through one or more **hidden layers** and finally reaches the **output layer**. In each neuron within these layers:\n",
              "    *   It receives numerical inputs from the neurons in the previous layer.\n",
              "    *   Each input is multiplied by its corresponding **weight**.\n",
              "    *   All these weighted inputs are summed together, and the neuron's **bias** is added to this sum.\n",
              "    *   This total sum then passes through an **activation function**. This function introduces non-linearity, allowing the network to learn complex patterns (e.g., a \"step\" function decides if a neuron \"fires\" or not, or a smooth curve outputs a probability).\n",
              "    *   The output of the activation function becomes the input for the neurons in the next layer.\n",
              "3.  **Prediction:** The values generated by the **output layer** represent the network's **prediction** for the given input (e.g., a probability that the image is a cat, or a predicted stock price).\n",
              "\n",
              "This entire process is like a series of interconnected calculators, each performing a simple calculation and passing its result to the next in line, ultimately arriving at a final answer.\n",
              "\n",
              "#### 3. Loss Function: Measuring the Error\n",
              "\n",
              "After the network makes a prediction, we need to know how good or bad that prediction is. This is where the **loss function** (or cost function) comes in.\n",
              "*   The loss function is a mathematical formula that quantifies the difference between the network's **prediction** and the true, correct answer (known as the **ground truth**).\n",
              "*   For example, if the network predicts \"dog\" but the image is actually a \"cat,\" the loss function would output a high value, indicating a large error. If it predicts \"cat\" for a \"cat,\" the loss would be very low, ideally zero.\n",
              "*   Different tasks use different loss functions. For predicting a continuous value (like house price), **Mean Squared Error** is common. For classification (like identifying an animal), **Cross-Entropy** is often used.\n",
              "\n",
              "The goal of learning is always to **minimize this loss**. Think of it as a golf score â€“ the lower the score, the better the performance.\n",
              "\n",
              "#### 4. Backpropagation: Tracing the Blame\n",
              "\n",
              "Now that we know *how wrong* the network's prediction was (from the loss function), the crucial next step is to figure out *who* (which weights and biases) is responsible for that error, and by how much. This is the job of **backpropagation**.\n",
              "\n",
              "Backpropagation is an algorithm that efficiently calculates the **gradient** of the loss function with respect to every single weight and bias in the network.\n",
              "*   The **gradient** tells us two things: the *direction* in which to adjust a weight/bias to reduce the loss, and the *magnitude* of that adjustment (how much impact that weight/bias had on the error).\n",
              "*   Conceptually, backpropagation works by propagating the error backward from the output layer through the hidden layers, all the way to the input layer. It's like a blame assignment process:\n",
              "    *   It starts by calculating how much each output neuron contributed to the total loss.\n",
              "    *   Then, it moves backward, determining how much each weight and bias in the *previous* layer contributed to the error propagated from the layer above.\n",
              "    *   This process continues layer by layer, meticulously calculating each parameter's individual responsibility for the overall error.\n",
              "\n",
              "Imagine a complex assembly line where a defective product comes out at the end. Backpropagation is like tracing back through each workstation and component, figuring out which specific part introduced the defect and how significantly it contributed to the final flaw. It provides a precise map of how to change each parameter to improve the outcome.\n",
              "\n",
              "#### 5. Weight and Bias Update (Optimization): Adjusting the Knobs\n",
              "\n",
              "With the gradients calculated via backpropagation, we now know exactly how to adjust each weight and bias to reduce the loss. This adjustment is handled by an **optimization algorithm**, the most fundamental of which is **Gradient Descent**.\n",
              "\n",
              "*   **Gradient Descent:** The core idea is simple: we want to move down the \"hill\" of the loss landscape to find the lowest point (minimal loss). Since the gradient points in the direction of the steepest *increase* in loss, we adjust each weight and bias in the *opposite* direction of its gradient.\n",
              "    *   `New Weight = Old Weight - (Learning Rate * Gradient of Weight)`\n",
              "    *   `New Bias = Old Bias - (Learning Rate * Gradient of Bias)`\n",
              "\n",
              "*   **Learning Rate:** This is a crucial **hyperparameter** (a setting that is not learned by the network but set by the programmer). It determines the size of the steps we take down the loss landscape.\n",
              "    *   A large learning rate might cause us to overshoot the minimum or even diverge.\n",
              "    *   A small learning rate might make the learning process very slow.\n",
              "    *   Finding the right learning rate is often key to effective training.\n",
              "\n",
              "**Conceptual Analogy for the Loss Landscape:**\n",
              "Imagine you are blindfolded on a vast, uneven landscape (the **loss landscape**). Your goal is to find the lowest point in this landscape (where the loss is minimized). You can't see, but you can feel the slope under your feet. Gradient descent is like taking a small step in the direction that feels steepest *downhill*. You repeat this process, always stepping downhill, until you reach a valley or a flat area (a local or global minimum). The learning rate determines how big each step you take is.\n",
              "\n",
              "#### 6. Iteration (Epochs): Repeating the Cycle\n",
              "\n",
              "A single forward pass, loss calculation, backpropagation, and parameter update is just one small step in the learning process. Neural networks rarely learn effectively from just one go.\n",
              "*   The entire dataset is typically passed through the network multiple times. Each full pass over the entire training dataset is called an **epoch**.\n",
              "*   We repeat the entire learning cycle (steps 2 through 5) for many epochs. With each epoch, the weights and biases are slightly refined, leading to incrementally better predictions and lower loss.\n",
              "\n",
              "This iterative process is similar to a student studying for an exam. They don't just read the textbook once and ace it. They review material, test themselves, identify weak areas, restudy those specific topics, and repeat until they've mastered the subject.\n",
              "\n",
              "#### 7. Mini-batch Gradient Descent: Learning in Chunks\n",
              "\n",
              "While Gradient Descent sounds straightforward, processing the entire training dataset for every single weight update (full batch gradient descent) can be computationally expensive and slow for very large datasets.\n",
              "*   To address this, **Mini-batch Gradient Descent** is commonly used. Instead of processing the entire dataset at once, the data is divided into smaller, randomly sampled groups called **mini-batches**.\n",
              "*   The network performs a forward pass, calculates loss, and updates weights/biases for one mini-batch at a time. Once all mini-batches have been processed, one epoch is complete.\n",
              "*   This approach offers a practical balance: it's faster than full batch gradient descent and provides a more stable update direction than **Stochastic Gradient Descent (SGD)**, which uses only one data point at a time. It also efficiently utilizes modern computing hardware.\n",
              "\n",
              "---\n",
              "\n",
              "### Summary\n",
              "\n",
              "In essence, a neural network learns through a continuous cycle of **prediction, error measurement, and adjustment**. It starts with random guesses (initialized weights/biases), makes a prediction (forward pass), compares that prediction to the truth (loss function), precisely calculates how each internal parameter contributed to the error (backpropagation), and then slightly tweaks those parameters to reduce future error (gradient descent with a learning rate). This entire process is repeated over many iterations (epochs) using small chunks of data (mini-batches) until the network becomes proficient at its task, effectively learning to identify complex patterns and make accurate predictions."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "improved_response_1 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_1\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED RESPONSE (Iteration 1):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(improved_response_1.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iteration-2-section"
      },
      "source": [
        "## Iteration 2: Further refinement\n",
        "\n",
        "Run the critique-rewrite cycle again to see if additional improvements are possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "iteration-2-critique"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CRITIQUE (Iteration 2):\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "Here's a detailed critique of the prompt and its response:\n",
              "\n",
              "---\n",
              "\n",
              "### STRENGTHS:\n",
              "*   **Exceptional Adherence to Content Requirements:** The response meticulously covers all requested essential steps: Initialization, Forward Pass, Loss Function, Backpropagation, Weight/Bias Update (Optimization), and Iteration (Epochs). It also successfully introduces mini-batch gradient descent as requested.\n",
              "*   **Outstanding Conceptual Clarity and Accuracy:** The explanation of each step is conceptually sound, accurate, and avoids unnecessary mathematical complexity, perfectly aligning with the \"detailed conceptual explanation\" and \"avoiding complex mathematical derivations\" requirements.\n",
              "*   **Highly Accessible Language for the Audience:** The language used is consistently clear, concise, and perfectly pitched for an \"intermediate learner with basic programming knowledge but no prior machine learning experience.\" Technical jargon is handled well.\n",
              "*   **Excellent Use of Analogies:** The response excels in using well-chosen and effective analogies (e.g., teaching a child, importance sliders, interconnected calculators, golf score, blame assignment, blindfolded on a loss landscape, student studying for an exam) to simplify complex ideas and enhance understanding, making the abstract concrete.\n",
              "*   **Thorough Technical Term Definition:** All key technical terms are accurately defined upon introduction, further aiding the target audience's comprehension. Bold text for these terms is consistently applied.\n",
              "*   **Strong Structure and Flow:** The explanation flows logically from one step to the next, building understanding progressively. The introductory and concluding summaries effectively frame the core learning mechanism.\n",
              "*   **Precise Formatting:** The response correctly uses clear, *numbered* headings for each step (`#### 1. Initialization: Starting with a Guess`), which was a specific prompt requirement, demonstrating careful attention to detail.\n",
              "*   **Comprehensive Coverage:** The response provides a truly comprehensive overview of the learning process, fulfilling the prompt's request for a \"comprehensive, accurate, and highly accessible\" explanation.\n",
              "\n",
              "### WEAKNESSES:\n",
              "*   **Slight Word Count Exceedance:** The prompt specified an approximate length of \"800-1200 words.\" The generated response is approximately 1319 words, exceeding the upper bound by about 10%. While not drastic, it is an area where conciseness could be slightly improved to fit within the requested range without sacrificing essential detail.\n",
              "\n",
              "### PROMPT ISSUES:\n",
              "*   The original prompt is exceptionally well-crafted. It is clear, highly detailed, and comprehensive in its requirements for audience, depth, content, and formatting. There are no identifiable weaknesses or ambiguities in the prompt that led to any issues in the response. The minor word count overage in the response is more attributable to the model's execution of providing a \"detailed conceptual explanation\" rather than a flaw in the prompt's clarity regarding the approximate word count.\n",
              "\n",
              "### QUALITY SCORE: 9.5/10"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "critique_2 = critique_response(TASK_DESCRIPTION, improved_prompt_1, improved_response_1.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iteration-2-rewrite"
      },
      "outputs": [
        {
          "ename": "ClientError",
          "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 24.914550338s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m improved_prompt_2 = \u001b[43mrewrite_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTASK_DESCRIPTION\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimproved_prompt_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritique_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIMPROVED PROMPT (Iteration 2):\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrewrite_prompt\u001b[39m\u001b[34m(task, original_prompt, critique)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Generate an improved prompt based on the critique.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     rewrite_instruction = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mYou are a prompt engineering expert. Based on the critique below, rewrite the \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mprompt to address all identified weaknesses.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m \u001b[33mor commentary.\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     result = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrewrite_instruction\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.text.strip()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/models.py:5227\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5225\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5226\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5227\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5228\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5229\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5231\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5232\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/models.py:4009\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4006\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4007\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4009\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4010\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4011\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4014\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4015\u001b[39m ):\n\u001b[32m   4016\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1376\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1377\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1378\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1381\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1382\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1383\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m   response_body = (\n\u001b[32m   1388\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m   )\n\u001b[32m   1390\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1222\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1192\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m       method=http_request.method,\n\u001b[32m   1194\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/gsoc/google_deepmind/cookbook/venv/lib/python3.13/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
            "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 24.914550338s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '24s'}]}}"
          ]
        }
      ],
      "source": [
        "improved_prompt_2 = rewrite_prompt(TASK_DESCRIPTION, improved_prompt_1, critique_2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED PROMPT (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-2-response"
      },
      "outputs": [],
      "source": [
        "improved_response_2 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_2\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED RESPONSE (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(improved_response_2.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iteration-3-section"
      },
      "source": [
        "## Iteration 3: Final refinement\n",
        "\n",
        "One more iteration to maximize prompt quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-critique"
      },
      "outputs": [],
      "source": [
        "critique_3 = critique_response(TASK_DESCRIPTION, improved_prompt_2, improved_response_2.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-rewrite"
      },
      "outputs": [],
      "source": [
        "improved_prompt_3 = rewrite_prompt(TASK_DESCRIPTION, improved_prompt_2, critique_3)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL OPTIMIZED PROMPT (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-response"
      },
      "outputs": [],
      "source": [
        "final_response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_3\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL RESPONSE (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(final_response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison-section"
      },
      "source": [
        "## Compare: Before and after\n",
        "\n",
        "Let's compare the prompt evolution and have the model evaluate the improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompt-evolution"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PROMPT EVOLUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[ORIGINAL PROMPT]\")\n",
        "print(initial_prompt)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[ITERATION 1]\")\n",
        "print(improved_prompt_1)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[ITERATION 2]\")\n",
        "print(improved_prompt_2)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[FINAL PROMPT]\")\n",
        "print(improved_prompt_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final-comparison"
      },
      "outputs": [],
      "source": [
        "comparison_prompt = f\"\"\"\n",
        "Compare these two responses to the task: \"{TASK_DESCRIPTION}\"\n",
        "\n",
        "RESPONSE A (from weak prompt):\n",
        "{initial_response.text[:2000]}...\n",
        "\n",
        "RESPONSE B (from optimized prompt):\n",
        "{final_response.text[:2000]}...\n",
        "\n",
        "Provide a brief comparison:\n",
        "1. What specific improvements do you see in Response B?\n",
        "2. Rate each response on a scale of 1-10\n",
        "3. What made the optimized prompt more effective?\n",
        "\"\"\"\n",
        "\n",
        "comparison = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=comparison_prompt\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL COMPARISON:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(comparison.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automated-loop-section"
      },
      "source": [
        "## Bonus: Automated optimization loop\n",
        "\n",
        "Here's a reusable function that combines all steps into a single optimization loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "automated-loop"
      },
      "outputs": [],
      "source": [
        "def optimize_prompt(task, initial_prompt, iterations=3, verbose=True):\n",
        "    \"\"\"\n",
        "    Automatically optimize a prompt through iterative self-critique.\n",
        "    \n",
        "    Args:\n",
        "        task: Description of what the prompt should accomplish\n",
        "        initial_prompt: The starting prompt to optimize\n",
        "        iterations: Number of critique-rewrite cycles\n",
        "        verbose: Whether to print intermediate results\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with optimization history and final results\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        \"prompts\": [initial_prompt],\n",
        "        \"responses\": [],\n",
        "        \"critiques\": []\n",
        "    }\n",
        "    \n",
        "    current_prompt = initial_prompt\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"ITERATION {i + 1}\")\n",
        "            print(\"=\" * 60)\n",
        "        \n",
        "        # Generate response\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=current_prompt\n",
        "        )\n",
        "        history[\"responses\"].append(response.text)\n",
        "        \n",
        "        # Critique\n",
        "        critique = critique_response(task, current_prompt, response.text)\n",
        "        history[\"critiques\"].append(critique)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nPrompt: {current_prompt[:100]}...\")\n",
        "            print(f\"\\nCritique summary: {critique[:200]}...\")\n",
        "        \n",
        "        # Rewrite\n",
        "        current_prompt = rewrite_prompt(task, current_prompt, critique)\n",
        "        history[\"prompts\"].append(current_prompt)\n",
        "    \n",
        "    # Generate final response with optimized prompt\n",
        "    final_response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=current_prompt\n",
        "    )\n",
        "    history[\"responses\"].append(final_response.text)\n",
        "    \n",
        "    return {\n",
        "        \"initial_prompt\": initial_prompt,\n",
        "        \"final_prompt\": current_prompt,\n",
        "        \"initial_response\": history[\"responses\"][0],\n",
        "        \"final_response\": final_response.text,\n",
        "        \"history\": history\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try-it-section"
      },
      "source": [
        "### Try it with a different task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "try-it"
      },
      "outputs": [],
      "source": [
        "# Try optimizing a different weak prompt\n",
        "result = optimize_prompt(\n",
        "    task=\"Write a product description for a fitness tracker\",\n",
        "    initial_prompt=\"Write about a fitness tracker.\",\n",
        "    iterations=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OPTIMIZATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nInitial prompt: {result['initial_prompt']}\")\n",
        "print(f\"\\nFinal prompt: {result['final_prompt']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "key-learnings"
      },
      "source": [
        "## Key learnings\n",
        "\n",
        "This self-critique approach reveals common prompt improvements:\n",
        "\n",
        "1. **Specificity**: Vague prompts get vague responses. The model adds specific requirements.\n",
        "\n",
        "2. **Structure**: Optimized prompts often request specific formats (bullet points, sections, examples).\n",
        "\n",
        "3. **Audience**: Defining the target audience helps calibrate complexity and tone.\n",
        "\n",
        "4. **Constraints**: Adding length limits, focus areas, or exclusions improves relevance.\n",
        "\n",
        "5. **Context**: Providing background information leads to more informed responses.\n",
        "\n",
        "You can use this technique to:\n",
        "- Rapidly iterate on prompts for production applications\n",
        "- Learn what makes prompts effective for specific tasks\n",
        "- Generate prompt templates for common use cases\n",
        "- Debug why certain prompts underperform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "### Related prompting techniques\n",
        "\n",
        "Explore other prompting examples in this repository:\n",
        "\n",
        "- [Chain of thought prompting](./Chain_of_thought_prompting.ipynb) - Guide the model through reasoning steps\n",
        "- [Few-shot prompting](./Few_shot_prompting.ipynb) - Provide examples to guide output format\n",
        "- [Role prompting](./Role_prompting.ipynb) - Assign personas for specialized responses\n",
        "- [Self-ask prompting](./Self_ask_prompting.ipynb) - Have the model decompose complex questions\n",
        "\n",
        "### Useful API references\n",
        "\n",
        "- [Prompt design guide](https://ai.google.dev/gemini-api/docs/prompting-intro)\n",
        "- [System instructions](https://ai.google.dev/gemini-api/docs/system-instructions)\n",
        "- [JSON mode for structured outputs](../json_capabilities/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Self_critique_prompt_optimization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
