{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "license"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "license-code"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Gemini API: Self-critique prompt optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/prompting/Self_critique_prompt_optimization.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "Prompt engineering often involves manual trial and error. You write a prompt, evaluate the output, tweak the prompt, and repeat. This notebook demonstrates how to automate this process by having Gemini critique its own outputs and suggest prompt improvements.\n",
        "\n",
        "This technique, sometimes called **meta-prompting** or **self-critique**, uses the model to:\n",
        "\n",
        "1. Generate a response from an initial prompt\n",
        "2. Critique the quality of that response\n",
        "3. Identify specific weaknesses\n",
        "4. Rewrite the prompt to address those weaknesses\n",
        "5. Generate an improved response\n",
        "\n",
        "By the end of this notebook, you will understand how to implement an iterative prompt optimization loop that can help you develop better prompts faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install-sdk"
      },
      "source": [
        "### Install SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api-key-section"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api-key"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model-selection-text"
      },
      "source": [
        "Select the model you want to use from the available options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-selection"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\"  # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-2.5-flash-preview\", \"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "problem-statement"
      },
      "source": [
        "## The problem: weak prompts produce weak results\n",
        "\n",
        "Consider a common scenario: you need the model to explain a technical concept, but your initial prompt is vague. The output might be generic, miss key details, or lack the structure you need.\n",
        "\n",
        "Here's a deliberately weak prompt to demonstrate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "define-task"
      },
      "outputs": [],
      "source": [
        "# Define the task context - this stays constant throughout optimization\n",
        "task_description = \"Explain how neural networks learn\"\n",
        "\n",
        "# Initial weak prompt - vague and lacks specificity\n",
        "initial_prompt = \"Explain how neural networks learn.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initial-response-text"
      },
      "source": [
        "### Generate the initial response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initial-response"
      },
      "outputs": [],
      "source": [
        "initial_response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=initial_prompt\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INITIAL PROMPT:\")\n",
        "print(\"=\" * 60)\n",
        "print(initial_prompt)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIAL RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(initial_response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "critique-section"
      },
      "source": [
        "## Step 1: Critique the output\n",
        "\n",
        "Now, ask the model to critically evaluate its own response. The critique prompt should ask for specific, actionable feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "critique-prompt"
      },
      "outputs": [],
      "source": [
        "def critique_response(task, prompt, response_text):\n",
        "    \"\"\"\n",
        "    Ask the model to critique a response and identify weaknesses.\n",
        "    \"\"\"\n",
        "    critique_prompt = f\"\"\"\n",
        "You are a prompt engineering expert. Analyze the following prompt and its \n",
        "response, then provide a detailed critique.\n",
        "\n",
        "TASK: {task}\n",
        "\n",
        "PROMPT USED:\n",
        "{prompt}\n",
        "\n",
        "RESPONSE GENERATED:\n",
        "{response_text}\n",
        "\n",
        "Provide your critique in this format:\n",
        "\n",
        "STRENGTHS:\n",
        "- List what the response did well\n",
        "\n",
        "WEAKNESSES:\n",
        "- List specific problems with the response\n",
        "- Focus on: clarity, completeness, structure, accuracy, relevance\n",
        "\n",
        "PROMPT ISSUES:\n",
        "- Identify what was missing or unclear in the original prompt\n",
        "- Explain how prompt weaknesses led to response weaknesses\n",
        "\n",
        "QUALITY SCORE: [1-10]\n",
        "\"\"\"\n",
        "    \n",
        "    critique = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=critique_prompt\n",
        "    )\n",
        "    return critique.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-critique"
      },
      "outputs": [],
      "source": [
        "critique_1 = critique_response(task_description, initial_prompt, initial_response.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE OF INITIAL RESPONSE:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rewrite-section"
      },
      "source": [
        "## Step 2: Rewrite the prompt\n",
        "\n",
        "Based on the critique, ask the model to generate an improved prompt that addresses the identified weaknesses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rewrite-function"
      },
      "outputs": [],
      "source": [
        "def rewrite_prompt(task, original_prompt, critique):\n",
        "    \"\"\"\n",
        "    Generate an improved prompt based on the critique.\n",
        "    \"\"\"\n",
        "    rewrite_instruction = f\"\"\"\n",
        "You are a prompt engineering expert. Based on the critique below, rewrite the \n",
        "prompt to address all identified weaknesses.\n",
        "\n",
        "TASK: {task}\n",
        "\n",
        "ORIGINAL PROMPT:\n",
        "{original_prompt}\n",
        "\n",
        "CRITIQUE:\n",
        "{critique}\n",
        "\n",
        "Write an improved prompt that:\n",
        "1. Addresses all weaknesses mentioned in the critique\n",
        "2. Is clear and specific about expectations\n",
        "3. Includes relevant constraints (format, length, audience, etc.)\n",
        "4. Guides the model toward a higher quality response\n",
        "\n",
        "Return ONLY the improved prompt, nothing else. Do not include explanations \n",
        "or commentary.\n",
        "\"\"\"\n",
        "    \n",
        "    result = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=rewrite_instruction\n",
        "    )\n",
        "    return result.text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-rewrite"
      },
      "outputs": [],
      "source": [
        "improved_prompt_1 = rewrite_prompt(task_description, initial_prompt, critique_1)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED PROMPT (Iteration 1):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "improved-response-section"
      },
      "source": [
        "## Step 3: Generate response with improved prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "improved-response"
      },
      "outputs": [],
      "source": [
        "improved_response_1 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_1\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED RESPONSE (Iteration 1):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(improved_response_1.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iteration-2-section"
      },
      "source": [
        "## Iteration 2: Further refinement\n",
        "\n",
        "Run the critique-rewrite cycle again to see if additional improvements are possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-2-critique"
      },
      "outputs": [],
      "source": [
        "critique_2 = critique_response(task_description, improved_prompt_1, improved_response_1.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-2-rewrite"
      },
      "outputs": [],
      "source": [
        "improved_prompt_2 = rewrite_prompt(task_description, improved_prompt_1, critique_2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED PROMPT (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-2-response"
      },
      "outputs": [],
      "source": [
        "improved_response_2 = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_2\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPROVED RESPONSE (Iteration 2):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(improved_response_2.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iteration-3-section"
      },
      "source": [
        "## Iteration 3: Final refinement\n",
        "\n",
        "One more iteration to maximize prompt quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-critique"
      },
      "outputs": [],
      "source": [
        "critique_3 = critique_response(task_description, improved_prompt_2, improved_response_2.text)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CRITIQUE (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(critique_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-rewrite"
      },
      "outputs": [],
      "source": [
        "improved_prompt_3 = rewrite_prompt(task_description, improved_prompt_2, critique_3)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL OPTIMIZED PROMPT (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "print(improved_prompt_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iteration-3-response"
      },
      "outputs": [],
      "source": [
        "final_response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=improved_prompt_3\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL RESPONSE (Iteration 3):\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(final_response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison-section"
      },
      "source": [
        "## Compare: Before and after\n",
        "\n",
        "Let's compare the prompt evolution and have the model evaluate the improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prompt-evolution"
      },
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"PROMPT EVOLUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n[ORIGINAL PROMPT]\")\n",
        "print(initial_prompt)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[ITERATION 1]\")\n",
        "print(improved_prompt_1)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[ITERATION 2]\")\n",
        "print(improved_prompt_2)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n[FINAL PROMPT]\")\n",
        "print(improved_prompt_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final-comparison"
      },
      "outputs": [],
      "source": [
        "comparison_prompt = f\"\"\"\n",
        "Compare these two responses to the task: \"{task_description}\"\n",
        "\n",
        "RESPONSE A (from weak prompt):\n",
        "{initial_response.text[:2000]}...\n",
        "\n",
        "RESPONSE B (from optimized prompt):\n",
        "{final_response.text[:2000]}...\n",
        "\n",
        "Provide a brief comparison:\n",
        "1. What specific improvements do you see in Response B?\n",
        "2. Rate each response on a scale of 1-10\n",
        "3. What made the optimized prompt more effective?\n",
        "\"\"\"\n",
        "\n",
        "comparison = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=comparison_prompt\n",
        ")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"FINAL COMPARISON:\")\n",
        "print(\"=\" * 60)\n",
        "display(Markdown(comparison.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "automated-loop-section"
      },
      "source": [
        "## Bonus: Automated optimization loop\n",
        "\n",
        "Here's a reusable function that combines all steps into a single optimization loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "automated-loop"
      },
      "outputs": [],
      "source": [
        "def optimize_prompt(task, initial_prompt, iterations=3, verbose=True):\n",
        "    \"\"\"\n",
        "    Automatically optimize a prompt through iterative self-critique.\n",
        "    \n",
        "    Args:\n",
        "        task: Description of what the prompt should accomplish\n",
        "        initial_prompt: The starting prompt to optimize\n",
        "        iterations: Number of critique-rewrite cycles\n",
        "        verbose: Whether to print intermediate results\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with optimization history and final results\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        \"prompts\": [initial_prompt],\n",
        "        \"responses\": [],\n",
        "        \"critiques\": []\n",
        "    }\n",
        "    \n",
        "    current_prompt = initial_prompt\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        if verbose:\n",
        "            print(f\"\\n{'='*60}\")\n",
        "            print(f\"ITERATION {i + 1}\")\n",
        "            print(\"=\" * 60)\n",
        "        \n",
        "        # Generate response\n",
        "        response = client.models.generate_content(\n",
        "            model=MODEL_ID,\n",
        "            contents=current_prompt\n",
        "        )\n",
        "        history[\"responses\"].append(response.text)\n",
        "        \n",
        "        # Critique\n",
        "        critique = critique_response(task, current_prompt, response.text)\n",
        "        history[\"critiques\"].append(critique)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"\\nPrompt: {current_prompt[:100]}...\")\n",
        "            print(f\"\\nCritique summary: {critique[:200]}...\")\n",
        "        \n",
        "        # Rewrite\n",
        "        current_prompt = rewrite_prompt(task, current_prompt, critique)\n",
        "        history[\"prompts\"].append(current_prompt)\n",
        "    \n",
        "    # Generate final response with optimized prompt\n",
        "    final_response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=current_prompt\n",
        "    )\n",
        "    history[\"responses\"].append(final_response.text)\n",
        "    \n",
        "    return {\n",
        "        \"initial_prompt\": initial_prompt,\n",
        "        \"final_prompt\": current_prompt,\n",
        "        \"initial_response\": history[\"responses\"][0],\n",
        "        \"final_response\": final_response.text,\n",
        "        \"history\": history\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "try-it-section"
      },
      "source": [
        "### Try it with a different task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "try-it"
      },
      "outputs": [],
      "source": [
        "# Try optimizing a different weak prompt\n",
        "result = optimize_prompt(\n",
        "    task=\"Write a product description for a fitness tracker\",\n",
        "    initial_prompt=\"Write about a fitness tracker.\",\n",
        "    iterations=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OPTIMIZATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nInitial prompt: {result['initial_prompt']}\")\n",
        "print(f\"\\nFinal prompt: {result['final_prompt']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "key-learnings"
      },
      "source": [
        "## Key learnings\n",
        "\n",
        "This self-critique approach reveals common prompt improvements:\n",
        "\n",
        "1. **Specificity**: Vague prompts get vague responses. The model adds specific requirements.\n",
        "\n",
        "2. **Structure**: Optimized prompts often request specific formats (bullet points, sections, examples).\n",
        "\n",
        "3. **Audience**: Defining the target audience helps calibrate complexity and tone.\n",
        "\n",
        "4. **Constraints**: Adding length limits, focus areas, or exclusions improves relevance.\n",
        "\n",
        "5. **Context**: Providing background information leads to more informed responses.\n",
        "\n",
        "You can use this technique to:\n",
        "- Rapidly iterate on prompts for production applications\n",
        "- Learn what makes prompts effective for specific tasks\n",
        "- Generate prompt templates for common use cases\n",
        "- Debug why certain prompts underperform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next-steps"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "### Related prompting techniques\n",
        "\n",
        "Explore other prompting examples in this repository:\n",
        "\n",
        "- [Chain of thought prompting](./Chain_of_thought_prompting.ipynb) - Guide the model through reasoning steps\n",
        "- [Few-shot prompting](./Few_shot_prompting.ipynb) - Provide examples to guide output format\n",
        "- [Role prompting](./Role_prompting.ipynb) - Assign personas for specialized responses\n",
        "- [Self-ask prompting](./Self_ask_prompting.ipynb) - Have the model decompose complex questions\n",
        "\n",
        "### Useful API references\n",
        "\n",
        "- [Prompt design guide](https://ai.google.dev/gemini-api/docs/prompting-intro)\n",
        "- [System instructions](https://ai.google.dev/gemini-api/docs/system-instructions)\n",
        "- [JSON mode for structured outputs](../json_capabilities/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Self_critique_prompt_optimization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
