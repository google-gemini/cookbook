{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "license"
            },
            "source": [
                "##### Copyright 2025 Google LLC."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "license-code"
            },
            "outputs": [],
            "source": [
                "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
                "# you may not use this file except in compliance with the License.\n",
                "# You may obtain a copy of the License at\n",
                "#\n",
                "# https://www.apache.org/licenses/LICENSE-2.0\n",
                "#\n",
                "# Unless required by applicable law or agreed to in writing, software\n",
                "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
                "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
                "# See the License for the specific language governing permissions and\n",
                "# limitations under the License."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "# Gemma 3: Native tool-calling via Model Context Protocol (MCP)\n",
                "\n",
                "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Gemma_3_Native_Tool_Calling_via_MCP.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>\n",
                "\n",
                "Gemma 3 is a highly capable open-weights model from Google DeepMind. One of its standout features is native support for function calling via standardized control tokens. \n",
                "\n",
                "This tutorial demonstrates how to integrate Gemma 3 with the **Model Context Protocol (MCP)**. MCP is an open standard that allows models to reach out to local or remote system environments to perform actions, search for data, or interact with files in a secure and standardized way.\n",
                "\n",
                "In this notebook, you will learn:\n",
                "1. How to define MCP tool logic.\n",
                "2. How to map MCP tool definitions to Gemma 3's native function calling format.\n",
                "3. How to implement an autonomous execution loop for agentic workflows."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-markdown"
            },
            "source": [
                "## Setup\n",
                "\n",
                "First, you install the necessary libraries. You use the `mcp` SDK for the protocol logic and `google-genai` for model interaction (if using hosted Gemma via Vertex AI) or standard Python libraries for local simulation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "installs"
            },
            "outputs": [],
            "source": [
                "%pip install -q -U mcp httpx 'google-genai>=1.0.0'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "mcp-logic-markdown"
            },
            "source": [
                "## 1. Define MCP tool logic\n",
                "\n",
                "The Model Context Protocol allows you to expose local functions to an LLM. Here, you define a simple `FileCreator` tool that allows Gemma to create files in the local environment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "mcp-server-impl"
            },
            "outputs": [],
            "source": [
                "# @title MCP tool definitions and implementation\n",
                "\n",
                "import json\n",
                "from typing import Dict, Any, Optional\n",
                "\n",
                "class LocalMCPGateway:\n",
                "    \"\"\"\n",
                "    A simplified MCP Gateway that manages tool registration and execution.\n",
                "    \"\"\"\n",
                "    def __init__(self):\n",
                "        self.registry = {}\n",
                "\n",
                "    def register_tool(self, name: str, description: str, schema: Dict[str, Any], func: Any):\n",
                "        self.registry[name] = {\n",
                "            \"description\": description,\n",
                "            \"schema\": schema,\n",
                "            \"func\": func\n",
                "        }\n",
                "\n",
                "    def get_tool_definitions(self) -> str:\n",
                "        \"\"\"\n",
                "        Formats tools for Gemma 3's <start_function_declaration> block.\n",
                "        \"\"\"\n",
                "        definitions = []\n",
                "        for name, info in self.registry.items():\n",
                "            def_str = f\"declaration:{name}{json.dumps(info['schema'])}\"\n",
                "            definitions.append(def_str)\n",
                "        return \"\\n\".join(definitions)\n",
                "\n",
                "    async def call_tool(self, name: str, arguments: Dict[str, Any]) -> str:\n",
                "        \"\"\"\n",
                "        Executes a tool and returns the result encapsulated for Gemma.\n",
                "        \"\"\"\n",
                "        if name not in self.registry:\n",
                "            return f\"Error: Tool {name} not found.\"\n",
                "        \n",
                "        try:\n",
                "            result = await self.registry[name][\"func\"](**arguments)\n",
                "            return str(result)\n",
                "        except Exception as e: # Generic catch for tool execution errors reported back to model\n",
                "            return f\"Execution Error: {str(e)}\"\n",
                "\n",
                "async def create_local_file(filename: str, content: str) -> str:\n",
                "    \"\"\"\n",
                "    Creates a file on the local filesystem.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        with open(filename, 'w') as f:\n",
                "            f.write(content)\n",
                "        return f\"Successfully created {filename}.\"\n",
                "    except (IOError, OSError) as e:\n",
                "        return f\"Failed to create file: {str(e)}\"\n",
                "\n",
                "# Initialize and Register\n",
                "gateway = LocalMCPGateway()\n",
                "gateway.register_tool(\n",
                "    name=\"create_file\",\n",
                "    description=\"Creates a new file with the specified content.\",\n",
                "    schema={\n",
                "        \"type\": \"object\",\n",
                "        \"properties\": {\n",
                "            \"filename\": {\"type\": \"string\", \"description\": \"The name of the file to create.\"},\n",
                "            \"content\": {\"type\": \"string\", \"description\": \"The content to write to the file.\"}\n",
                "        },\n",
                "        \"required\": [\"filename\", \"content\"]\n",
                "    },\n",
                "    func=create_local_file\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "gemma-integration-markdown"
            },
            "source": [
                "## 2. Gemma 3 integration logic\n",
                "\n",
                "Gemma 3 uses specialized tokens to handle tool-calling. You need to wrap the model interaction to parse these tokens and feed results back into the context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "gemma-native-logic"
            },
            "outputs": [],
            "source": [
                "# @title Native token handling utilities\n",
                "\n",
                "import re\n",
                "import json\n",
                "from typing import Optional, Dict, Any\n",
                "\n",
                "GEMMA_SYSTEM_PROMPT = \"\"\"\n",
                "You are a model that can do function calling with the following functions:\n",
                "<start_function_declaration>\n",
                "{tool_definitions}\n",
                "<end_function_declaration>\n",
                "\n",
                "When you need to use a tool, use the following format:\n",
                "<thought>\n",
                "[Your reasoning here]\n",
                "</thought>\n",
                "<start_function_call>call:{tool_name}{{\"arg1\": \"value1\"}}<end_function_call>\n",
                "\n",
                "You will then receive a response in this format:\n",
                "<start_function_response>... result ...<end_function_response>\n",
                "\"\"\"\n",
                "\n",
                "def parse_tool_call(text: str) -> tuple[Optional[str], Optional[Dict[str, Any]]]:\n",
                "    \"\"\"\n",
                "    Regex parser for Gemma 3's official <start_function_call> token.\n",
                "    \"\"\"\n",
                "    match = re.search(r\"<start_function_call>call:(\\w+)(\\{.*?\\})<end_function_call>\", text, re.DOTALL)\n",
                "    if match:\n",
                "        return match.group(1), json.loads(match.group(2))\n",
                "    return None, None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "execution-markdown"
            },
            "source": [
                "## 3. The execution loop\n",
                "\n",
                "In a real-world scenario, you would call a Gemma 3 API (like Vertex AI or a local Ollama instance). Here, you simulate the model's responses to demonstrate the multi-turn protocol flow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "agent-loop"
            },
            "outputs": [],
            "source": [
                "import asyncio\n",
                "\n",
                "async def run_gemma_mcp_session(user_query: str):\n",
                "    \"\"\"\n",
                "    Demonstrates the full lifecycle of an MCP-enabled Gemma 3 session.\n",
                "    \"\"\"\n",
                "    print(f\"USER: {user_query}\\n\")\n",
                "    \n",
                "    # 1. Initialize System Instruction\n",
                "    tool_defs = gateway.get_tool_definitions()\n",
                "    system_instr = GEMMA_SYSTEM_PROMPT.format(tool_definitions=tool_defs)\n",
                "    print(f\"SYSTEM INSTRUCTION:\\n{system_instr}\\n\")\n",
                "    \n",
                "    # 2. Simulate Model Output (In reality, this would be an API call)\n",
                "    # Gemma sees the query and decides to call the tool.\n",
                "    simulated_model_output = \"\"\"<thought>\n",
                "I need to create a file as requested by the user. I should use the 'create_file' tool.\n",
                "</thought>\n",
                "<start_function_call>call:create_file{\"filename\": \"hello.txt\", \"content\": \"Hello from the Gemma 3 MCP Gateway!\"}<end_function_call>\"\"\"\n",
                "    \n",
                "    print(f\"MODEL THOUGHTS & CALL:\\n{simulated_model_output}\\n\")\n",
                "    \n",
                "    # 3. Parse and Execute Tool via MCP\n",
                "    tool_name, tool_args = parse_tool_call(simulated_model_output)\n",
                "    \n",
                "    if tool_name:\n",
                "        print(f\"--- EXECUTING MCP TOOL: {tool_name} ---\")\n",
                "        result = await gateway.call_tool(tool_name, tool_args)\n",
                "        print(f\"RESULT: {result}\\n\")\n",
                "        \n",
                "        # 4. Feed Result Back to Model\n",
                "        # The response_token would be sent back to the model to get the final response.\n",
                "        response_token = f\"<start_function_response>{result}<end_function_response>\"\n",
                "        print(f\"FEEDING BACK TO MODEL:\\n{response_token}\\n\")\n",
                "        \n",
                "        # 5. Simulate Final Model Response (In reality, this is another API call with history)\n",
                "        final_output = \"I have successfully created 'hello.txt' with the greeting you requested.\"\n",
                "        print(f\"MODEL FINAL ANSWER: {final_output}\")\n",
                "    else:\n",
                "        print(\"No tool call detected.\")\n",
                "\n",
                "# Run the demonstration using asyncio.run() for robustness\n",
                "try:\n",
                "    asyncio.run(run_gemma_mcp_session(\"Create a file named 'hello.txt' with a greeting.\"))\n",
                "except RuntimeError: # Handle case where loop is already running (e.g. in some notebook environments)\n",
                "    import nest_asyncio\n",
                "    nest_asyncio.apply()\n",
                "    asyncio.run(run_gemma_mcp_session(\"Create a file named 'hello.txt' with a greeting.\"))\n",
                "except Exception: # Fallback if nest_asyncio is not available\n",
                "    await run_gemma_mcp_session(\"Create a file named 'hello.txt' with a greeting.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "conclusion"
            },
            "source": [
                "## Conclusion\n",
                "\n",
                "By using **native control tokens** and the **Model Context Protocol**, Gemma 3 becomes a powerful agent capable of interacting with its environment in a decoupled, standardized way. This architecture allows developers to build specialized toolsets that can be shared across any MCP-compliant application."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mips_identifier": "python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}