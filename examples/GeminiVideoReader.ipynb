{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 Google LLC."
      ],
      "metadata": {
        "id": "7mSjwDa6XKIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YZKjA-Q3WVYn"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gemini API: GeminiVideoReader -- Batch Prediction with Long Context and Context Caching Code Sample 🚀🧠\n",
        "\n",
        "This notebook serves as a step-by-step guide for building an AI-powered video analysis Batch Prediction with Long Context and Context Caching Code Sample."
      ],
      "metadata": {
        "id": "QLn8Bjl7XO0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q google-genai youtube_transcript_api"
      ],
      "metadata": {
        "id": "aGDDxCU8YJcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e49782-2055-4ad4-fc42-6c1eb663980c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/144.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure your API key\n",
        "To run the following cell, your API key must be stored in a Colab Secret named GOOGLE_API_KEY. If you don't already have an API key, or you're not sure how to create a Colab Secret, see Authentication for an example."
      ],
      "metadata": {
        "id": "SgHmdp_javKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "QBvi8w3FYKs1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries and Initialize Google Gemini Client\n",
        "Import required libraries and initialize the Google Gemini API client using Colab’s userdata."
      ],
      "metadata": {
        "id": "-QmgdCf2YR24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import logging\n",
        "import json\n",
        "import os,re\n",
        "import time\n",
        "from hashlib import sha256\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "MODEL_ID = \"gemini-2.0-flash\"\n",
        "SYSTEM_PROMPT = \"Answer the question based on the provided transcript context. Provide a concise answer.\"\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"GeminiVideoReader\")\n"
      ],
      "metadata": {
        "id": "nmz--fuOg6Y3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Define Helper Functions\n",
        "Define functions to extract the YouTube video ID, fetch its transcript, and call the Gemini API asynchronously."
      ],
      "metadata": {
        "id": "NRyRTE8yhAGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_video_id(url):\n",
        "    if \"v=\" in url:\n",
        "        return url.split(\"v=\")[1].split(\"&\")[0]\n",
        "    elif \"youtu.be/\" in url:\n",
        "        return url.split(\"youtu.be/\")[1].split(\"?\")[0]\n",
        "    return None\n",
        "\n",
        "def get_youtube_transcript(video_url):\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        contents=genai.types.Content(\n",
        "            parts=[\n",
        "                genai.types.Part(text=\"Extract the full transcript from this video.\"),\n",
        "                genai.types.Part(\n",
        "                    file_data=genai.types.FileData(file_uri=video_url)\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "def segment_text(text, max_chars=3000):\n",
        "    sentences = text.split('. ')\n",
        "    segments = []\n",
        "    current_segment = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_segment) + len(sentence) + 1 < max_chars:\n",
        "            current_segment += sentence + \". \"\n",
        "        else:\n",
        "            segments.append(current_segment.strip())\n",
        "            current_segment = sentence + \". \"\n",
        "    if current_segment:\n",
        "        segments.append(current_segment.strip())\n",
        "    return segments\n",
        "\n",
        "class GeminiCache:\n",
        "    CACHE_FILE = \"context_cache.json\"\n",
        "    def __init__(self):\n",
        "        if os.path.exists(self.CACHE_FILE):\n",
        "            with open(self.CACHE_FILE, \"r\") as f:\n",
        "                self.cache = json.load(f)\n",
        "        else:\n",
        "            self.cache = {}\n",
        "    def save_cache(self):\n",
        "        with open(self.CACHE_FILE, \"w\") as f:\n",
        "            json.dump(self.cache, f)\n",
        "    def generate_cache_key(self, text):\n",
        "        return sha256(text.encode()).hexdigest()\n",
        "    def get_or_summarize(self, text, summarization_prompt):\n",
        "        key = self.generate_cache_key(text)\n",
        "        if key in self.cache:\n",
        "            return self.cache[key]\n",
        "        response = client.models.generate_content(\n",
        "            model=f\"models/{MODEL_ID}\",\n",
        "            contents=[f\"{summarization_prompt}\\n{text}\"],\n",
        "            config=genai.types.GenerateContentConfig(system_instruction=\"Summarize the text concisely.\")\n",
        "        )\n",
        "        summary = response.text\n",
        "        self.cache[key] = summary\n",
        "        self.save_cache()\n",
        "        return summary\n",
        "\n",
        "gemini_cache = GeminiCache()\n",
        "\n",
        "def process_long_transcript(transcript_context, max_chars=3000):\n",
        "    if len(transcript_context) <= max_chars:\n",
        "        return transcript_context\n",
        "    segments = segment_text(transcript_context, max_chars)\n",
        "    summarized_segments = []\n",
        "    for seg in segments:\n",
        "        summary = gemini_cache.get_or_summarize(seg, \"Summarize this transcript segment:\")\n",
        "        summarized_segments.append(summary)\n",
        "    return \"\\n\".join(summarized_segments)\n",
        "\n",
        "def call_gemini_api_sync(prompt, cached_context, history, max_retries=3):\n",
        "    history_text = \"\\n\".join([f\"Q: {q['question']} A: {q['answer']}\" for q in history])\n",
        "    full_prompt = f\"Use this context:\\n{cached_context}\\n\\n{history_text}\\n\\n{prompt}\"\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=f\"models/{MODEL_ID}\",\n",
        "                contents=[full_prompt],\n",
        "                config=genai.types.GenerateContentConfig(system_instruction=SYSTEM_PROMPT)\n",
        "            )\n",
        "            return {\"question\": prompt, \"answer\": response.text}\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error calling Gemini API: {e}. Retrying in {2 ** retries} seconds...\")\n",
        "            time.sleep(2 ** retries)\n",
        "            retries += 1\n",
        "    return {\"question\": prompt, \"answer\": \"Error: Unable to get response after retries.\"}\n",
        "\n",
        "async def call_gemini_api(prompt, cached_context, history):\n",
        "    return await asyncio.to_thread(call_gemini_api_sync, prompt, cached_context, history)\n",
        "\n",
        "async def batch_predict_async(prompts, cached_context, history, max_concurrent=5):\n",
        "    semaphore = asyncio.Semaphore(max_concurrent)\n",
        "    results = []\n",
        "    async def sem_call(prompt):\n",
        "        async with semaphore:\n",
        "            result = await call_gemini_api(prompt, cached_context, history)\n",
        "            history.append(result)\n",
        "            return result\n",
        "    tasks = [asyncio.create_task(sem_call(p)) for p in prompts]\n",
        "    for task in asyncio.as_completed(tasks):\n",
        "        results.append(await task)\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "6OqELwaShBTC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input YouTube Link, and Fetch Transcript\n",
        "\n",
        "Let's use [Why was the Rosetta Stone so important? - Franziska Naether](https://www.youtube.com/watch?v=Z8dZSySRX_g) as an example"
      ],
      "metadata": {
        "id": "haVE3Xi4lGRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "youtube_url = input(\"Enter the YouTube video URL: \")\n",
        "video_id = extract_video_id(youtube_url)\n",
        "if not video_id:\n",
        "    print(\"Invalid YouTube URL!\")\n",
        "else:\n",
        "    print(\"Extracted video ID:\", video_id)\n",
        "transcript_data = get_youtube_transcript(youtube_url)\n",
        "if transcript_data is None:\n",
        "    print(\"No transcript available for this video!\")\n",
        "print(\"Transcript fetched successfully.\")\n",
        "print(\"\\n--- Transcript Preview (first 500 characters) ---\\n\")\n",
        "print(transcript_data[:500])\n",
        "transcript_context = process_long_transcript(transcript_data)\n",
        "print(\"\\nTranscript context processed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIrQWm6FlJPr",
        "outputId": "00ee3dfa-bff8-48e3-831e-2e6bb0170cbc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the YouTube video URL: https://www.youtube.com/watch?v=Z8dZSySRX_g\n",
            "Extracted video ID: Z8dZSySRX_g\n",
            "Transcript fetched successfully.\n",
            "\n",
            "--- Transcript Preview (first 500 characters) ---\n",
            "\n",
            "Okay, here's the full transcript from the video:\n",
            "\n",
            "[00:00:06] For centuries, scholars puzzled over the hieroglyphs they found carved onto ancient Egyptian ruins, tablets, and papyri.\n",
            "\n",
            "[00:00:14] But a unique discovery would finally help unlock their meaning.\n",
            "\n",
            "[00:00:17] In 1799, as the French military invaded Egypt, an officer encountered a curious stone on the outskirts of Rashid or Rosetta.\n",
            "\n",
            "[00:00:27] It was inscribed with three different portions of text: Egyptian hieroglyphs, which is the ol\n",
            "\n",
            "Transcript context processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input Questions and Run Batch Prediction"
      ],
      "metadata": {
        "id": "XxYNMIuIlSRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your questions (one per line). Press Enter on an empty line when done:\")\n",
        "questions_list = []\n",
        "while True:\n",
        "    line = input()\n",
        "    if not line:\n",
        "        break\n",
        "    questions_list.append(line)\n",
        "if not questions_list:\n",
        "    print(\"No questions were entered!\")\n",
        "else:\n",
        "    print(\"Questions received:\", questions_list)\n",
        "if transcript_data is not None and questions_list:\n",
        "    conversation_history = []\n",
        "    results = asyncio.run(batch_predict_async(questions_list, transcript_context, conversation_history))\n",
        "    print(\"\\n--- AI-Generated Answers ---\\n\")\n",
        "    for i, res in enumerate(results, start=1):\n",
        "        print(f\"Q{i}: {res['question']}\")\n",
        "        print(f\"A{i}: {res['answer']}\\n\")\n",
        "    print(\"\\n--- Conversation History ---\\n\")\n",
        "    for entry in conversation_history:\n",
        "        print(f\"Q: {entry['question']}\\nA: {entry['answer']}\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "if transcript_data is not None and questions_list:\n",
        "    conversation_history = []\n",
        "    results = asyncio.run(batch_predict_async(questions_list, transcript_context, conversation_history))\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal batch prediction time: {end_time - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp6d6KoGlTcd",
        "outputId": "a65b54b5-cb3e-4590-89f2-2538038a273d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your questions (one per line). Press Enter on an empty line when done:\n",
            "What is the ancient message of the Rosetta Stone?\n",
            "Who are the scholars decoded the ancient message?\n",
            "\n",
            "Questions received: ['What is the ancient message of the Rosetta Stone?', 'Who are the scholars decoded the ancient message?']\n",
            "\n",
            "--- AI-Generated Answers ---\n",
            "\n",
            "Q1: Who are the scholars decoded the ancient message?\n",
            "A1: Åkerblad, Young, and Champollion are the scholars who worked on decoding the Rosetta Stone.\n",
            "\n",
            "\n",
            "Q2: What is the ancient message of the Rosetta Stone?\n",
            "A2: The Rosetta Stone contains a decree honoring Pharaoh Ptolemy V's coronation anniversary, outlining benefits for the priesthood and temple maintenance.\n",
            "\n",
            "\n",
            "\n",
            "--- Conversation History ---\n",
            "\n",
            "Q: Who are the scholars decoded the ancient message?\n",
            "A: Åkerblad, Young, and Champollion are the scholars who worked on decoding the Rosetta Stone.\n",
            "\n",
            "\n",
            "Q: What is the ancient message of the Rosetta Stone?\n",
            "A: The Rosetta Stone contains a decree honoring Pharaoh Ptolemy V's coronation anniversary, outlining benefits for the priesthood and temple maintenance.\n",
            "\n",
            "\n",
            "\n",
            "Total batch prediction time: 0.46 seconds\n"
          ]
        }
      ]
    }
  ]
}