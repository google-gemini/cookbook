{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8968a502d25e"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "ca23c3f523a7"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwH5rYtX5Xco"
      },
      "source": [
        "# What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ad518df45c"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/New_in_002.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4RV6XuQLjCc"
      },
      "source": [
        "This notebook explores the new options added with the 002 versions of the 1.5 series models:\n",
        "\n",
        "* Candidate count\n",
        "* Presence and frequency penalties\n",
        "* Response logprobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ZjsAuEGK6i"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnl6PeGKOfcA"
      },
      "source": [
        "Install a compatible version of the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xVTnXHg_nxMC"
      },
      "outputs": [],
      "source": [
        "%pip install -q \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLASVngmOm8K"
      },
      "source": [
        "import the package and give it your API-key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B6fY-RbxJl78"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sRmN-qzhM47E"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d-e1KfaOxlJ"
      },
      "source": [
        "Import other packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "giN4GuufOu02"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJO3Cz7UO0Ms"
      },
      "source": [
        "Check available models that support the new features. You will use `gemini-2.0-flash-001` in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RCFdQUXsIDcm"
      },
      "outputs": [],
      "source": [
        "model_name = \"gemini-2.0-flash-001\"\n",
        "test_prompt=\"Why don't people have tails\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBiweaL6LLKm"
      },
      "source": [
        "## Quick refresher on `generation_config` [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bwwgGRtnLOqJ"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(model=\"gemini-2.0-flash-001\", contents='hello', config = types.GenerateContentConfig(temperature=1.0, max_output_tokens=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3e6f5e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "name='models/embedding-gecko-001' display_name='Embedding Gecko' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1024 output_token_limit=1 supported_actions=['embedText', 'countTextTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-pro-preview-03-25' display_name='Gemini 2.5 Pro Preview 03-25' description='Gemini 2.5 Pro Preview 03-25' version='2.5-preview-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-preview-05-20' display_name='Gemini 2.5 Flash Preview 05-20' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash' display_name='Gemini 2.5 Flash' description='Stable version of Gemini 2.5 Flash, our mid-size multimodal model that supports up to 1 million tokens, released in June of 2025.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-lite-preview-06-17' display_name='Gemini 2.5 Flash-Lite Preview 06-17' description='Preview release (June 11th, 2025) of Gemini 2.5 Flash-Lite' version='2.5-preview-06-17' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-pro-preview-05-06' display_name='Gemini 2.5 Pro Preview 05-06' description='Preview release (May 6th, 2025) of Gemini 2.5 Pro' version='2.5-preview-05-06' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-pro-preview-06-05' display_name='Gemini 2.5 Pro Preview' description='Preview release (June 5th, 2025) of Gemini 2.5 Pro' version='2.5-preview-06-05' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-pro' display_name='Gemini 2.5 Pro' description='Stable release (June 17th, 2025) of Gemini 2.5 Pro' version='2.5' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-exp' display_name='Gemini 2.0 Flash Experimental' description='Gemini 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash' display_name='Gemini 2.0 Flash' description='Gemini 2.0 Flash' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-001' display_name='Gemini 2.0 Flash 001' description='Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-exp-image-generation' display_name='Gemini 2.0 Flash (Image Generation) Experimental' description='Gemini 2.0 Flash (Image Generation) Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-lite-001' display_name='Gemini 2.0 Flash-Lite 001' description='Stable version of Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-lite' display_name='Gemini 2.0 Flash-Lite' description='Gemini 2.0 Flash-Lite' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-preview-image-generation' display_name='Gemini 2.0 Flash Preview Image Generation' description='Gemini 2.0 Flash Preview Image Generation' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-lite-preview-02-05' display_name='Gemini 2.0 Flash-Lite Preview 02-05' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-lite-preview' display_name='Gemini 2.0 Flash-Lite Preview' description='Preview release (February 5th, 2025) of Gemini 2.0 Flash-Lite' version='preview-02-05' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-pro-exp' display_name='Gemini 2.0 Pro Experimental' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-pro-exp-02-05' display_name='Gemini 2.0 Pro Experimental 02-05' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-exp-1206' display_name='Gemini Experimental 1206' description='Experimental release (March 25th, 2025) of Gemini 2.5 Pro' version='2.5-exp-03-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-thinking-exp-01-21' display_name='Gemini 2.5 Flash Preview 05-20' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-thinking-exp' display_name='Gemini 2.5 Flash Preview 05-20' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-thinking-exp-1219' display_name='Gemini 2.5 Flash Preview 05-20' description='Preview release (April 17th, 2025) of Gemini 2.5 Flash' version='2.5-preview-05-20' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-preview-tts' display_name='Gemini 2.5 Flash Preview TTS' description='Gemini 2.5 Flash Preview TTS' version='gemini-2.5-flash-exp-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=16384 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-pro-preview-tts' display_name='Gemini 2.5 Pro Preview TTS' description='Gemini 2.5 Pro Preview TTS' version='gemini-2.5-pro-preview-tts-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=16384 supported_actions=['countTokens', 'generateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/learnlm-2.0-flash-experimental' display_name='LearnLM 2.0 Flash Experimental' description='LearnLM 2.0 Flash Experimental' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=32768 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3-1b-it' display_name='Gemma 3 1B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3-4b-it' display_name='Gemma 3 4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3-12b-it' display_name='Gemma 3 12B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3-27b-it' display_name='Gemma 3 27B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3n-e4b-it' display_name='Gemma 3n E4B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=2048 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemma-3n-e2b-it' display_name='Gemma 3n E2B' description=None version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=2048 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-flash-latest' display_name='Gemini Flash Latest' description='Latest release of Gemini Flash' version='Gemini Flash Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-flash-lite-latest' display_name='Gemini Flash-Lite Latest' description='Latest release of Gemini Flash-Lite' version='Gemini Flash-Lite Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-pro-latest' display_name='Gemini Pro Latest' description='Latest release of Gemini Pro' version='Gemini Pro Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-lite' display_name='Gemini 2.5 Flash-Lite' description='Stable version of Gemini 2.5 Flash-Lite, released in July of 2025' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-image-preview' display_name='Nano Banana' description='Gemini 2.5 Flash Preview Image' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-image' display_name='Nano Banana' description='Gemini 2.5 Flash Preview Image' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=32768 output_token_limit=8192 supported_actions=['generateContent', 'countTokens', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-preview-09-2025' display_name='Gemini 2.5 Flash Preview Sep 2025' description='Gemini 2.5 Flash Preview Sep 2025' version='Gemini 2.5 Flash Preview 09-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-lite-preview-09-2025' display_name='Gemini 2.5 Flash-Lite Preview Sep 2025' description='Preview release (Septempber 25th, 2025) of Gemini 2.5 Flash-Lite' version='2.5-preview-09-25' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-robotics-er-1.5-preview' display_name='Gemini Robotics-ER 1.5 Preview' description='Gemini Robotics-ER 1.5 Preview' version='1.5-preview' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-computer-use-preview-10-2025' display_name='Gemini 2.5 Computer Use Preview 10-2025' description='Gemini 2.5 Computer Use Preview 10-2025' version='Gemini 2.5 Computer Use Preview 10-2025' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=65536 supported_actions=['generateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/embedding-001' display_name='Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/text-embedding-004' display_name='Text Embedding 004' description='Obtain a distributed representation of a text.' version='004' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-embedding-exp-03-07' display_name='Gemini Embedding Experimental 03-07' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-embedding-exp' display_name='Gemini Embedding Experimental' description='Obtain a distributed representation of a text.' version='exp-03-07' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=8192 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-embedding-001' display_name='Gemini Embedding 001' description='Obtain a distributed representation of a text.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=2048 output_token_limit=1 supported_actions=['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/aqa' display_name='Model that performs Attributed Question Answering.' description='Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=7168 output_token_limit=1024 supported_actions=['generateAnswer'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-3.0-generate-002' display_name='Imagen 3.0' description='Vertex served Imagen 3.0 002 model' version='002' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-4.0-generate-preview-06-06' display_name='Imagen 4 (Preview)' description='Vertex served Imagen 4.0 model' version='01' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-4.0-ultra-generate-preview-06-06' display_name='Imagen 4 Ultra (Preview)' description='Vertex served Imagen 4.0 ultra model' version='01' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-4.0-generate-001' display_name='Imagen 4' description='Vertex served Imagen 4.0 model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-4.0-ultra-generate-001' display_name='Imagen 4 Ultra' description='Vertex served Imagen 4.0 ultra model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/imagen-4.0-fast-generate-001' display_name='Imagen 4 Fast' description='Vertex served Imagen 4.0 Fast model' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predict'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-2.0-generate-001' display_name='Veo 2' description='Vertex served Veo 2 model. Access to this model requires billing to be enabled on the associated Google Cloud Platform account. Please visit https://console.cloud.google.com/billing to enable it.' version='2.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.0-generate-preview' display_name='Veo 3' description='Veo 3 preview.' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.0-fast-generate-preview' display_name='Veo 3 fast' description='Veo 3 fast preview.' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.0-generate-001' display_name='Veo 3' description='Veo 3' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.0-fast-generate-001' display_name='Veo 3 fast' description='Veo 3 fast' version='3.0' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.1-generate-preview' display_name='Veo 3.1' description='Veo 3.1' version='3.1' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/veo-3.1-fast-generate-preview' display_name='Veo 3.1 fast' description='Veo 3.1 fast' version='3.1' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=480 output_token_limit=8192 supported_actions=['predictLongRunning'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.0-flash-live-001' display_name='Gemini 2.0 Flash 001' description='Gemini 2.0 Flash 001' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['bidiGenerateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-live-2.5-flash-preview' display_name='Gemini Live 2.5 Flash Preview' description='Gemini Live 2.5 Flash Preview' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['bidiGenerateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-live-preview' display_name='Gemini 2.5 Flash Live Preview' description='Gemini 2.5 Flash Live Preview' version='001' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=1048576 output_token_limit=65536 supported_actions=['bidiGenerateContent', 'countTokens'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-native-audio-latest' display_name='Gemini 2.5 Flash Native Audio Latest' description='Latest release of Gemini 2.5 Flash Native Audio' version='Gemini 2.5 Flash Native Audio Latest' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n",
            "name='models/gemini-2.5-flash-native-audio-preview-09-2025' display_name='Gemini 2.5 Flash Native Audio Preview 09-2025' description='Gemini 2.5 Flash Native Audio Preview 09-2025' version='gemini-2.5-flash-preview-native-audio-dialog-2025-05-19' endpoints=None labels=None tuned_model_info=TunedModelInfo() input_token_limit=131072 output_token_limit=8192 supported_actions=['countTokens', 'bidiGenerateContent'] default_checkpoint_id=None checkpoints=None\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for model in client.models.list():\n",
        "  print(model)\n",
        "  print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "89253741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model name: models/embedding-gecko-001\n",
            "Supported tasks: ['embedText', 'countTextTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-03-25\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-preview-05-20\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-lite-preview-06-17\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-05-06\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-06-05\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-exp\n",
            "Supported tasks: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-001\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-exp-image-generation\n",
            "Supported tasks: ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-001\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-preview-image-generation\n",
            "Supported tasks: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-preview-02-05\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-lite-preview\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-pro-exp\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-pro-exp-02-05\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-exp-1206\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp-01-21\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-thinking-exp-1219\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-preview-tts\n",
            "Supported tasks: ['countTokens', 'generateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-pro-preview-tts\n",
            "Supported tasks: ['countTokens', 'generateContent']\n",
            "--------------------\n",
            "Model name: models/learnlm-2.0-flash-experimental\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-1b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-4b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-12b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3-27b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3n-e4b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemma-3n-e2b-it\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-flash-latest\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-flash-lite-latest\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-pro-latest\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-lite\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-image-preview\n",
            "Supported tasks: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-image\n",
            "Supported tasks: ['generateContent', 'countTokens', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-preview-09-2025\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-lite-preview-09-2025\n",
            "Supported tasks: ['generateContent', 'countTokens', 'createCachedContent', 'batchGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-robotics-er-1.5-preview\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-computer-use-preview-10-2025\n",
            "Supported tasks: ['generateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/embedding-001\n",
            "Supported tasks: ['embedContent']\n",
            "--------------------\n",
            "Model name: models/text-embedding-004\n",
            "Supported tasks: ['embedContent']\n",
            "--------------------\n",
            "Model name: models/gemini-embedding-exp-03-07\n",
            "Supported tasks: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-embedding-exp\n",
            "Supported tasks: ['embedContent', 'countTextTokens', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-embedding-001\n",
            "Supported tasks: ['embedContent', 'countTextTokens', 'countTokens', 'asyncBatchEmbedContent']\n",
            "--------------------\n",
            "Model name: models/aqa\n",
            "Supported tasks: ['generateAnswer']\n",
            "--------------------\n",
            "Model name: models/imagen-3.0-generate-002\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/imagen-4.0-generate-preview-06-06\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/imagen-4.0-ultra-generate-preview-06-06\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/imagen-4.0-generate-001\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/imagen-4.0-ultra-generate-001\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/imagen-4.0-fast-generate-001\n",
            "Supported tasks: ['predict']\n",
            "--------------------\n",
            "Model name: models/veo-2.0-generate-001\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.0-generate-preview\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.0-fast-generate-preview\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.0-generate-001\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.0-fast-generate-001\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.1-generate-preview\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/veo-3.1-fast-generate-preview\n",
            "Supported tasks: ['predictLongRunning']\n",
            "--------------------\n",
            "Model name: models/gemini-2.0-flash-live-001\n",
            "Supported tasks: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-live-2.5-flash-preview\n",
            "Supported tasks: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-live-preview\n",
            "Supported tasks: ['bidiGenerateContent', 'countTokens']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-native-audio-latest\n",
            "Supported tasks: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------\n",
            "Model name: models/gemini-2.5-flash-native-audio-preview-09-2025\n",
            "Supported tasks: ['countTokens', 'bidiGenerateContent']\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "for model in client.models.list():\n",
        "  print(f\"Model name: {model.name}\")\n",
        "  print(f\"Supported tasks: {model.supported_actions}\")\n",
        "  print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sDxl0g-LQdx"
      },
      "source": [
        "Note:\n",
        "\n",
        "* Each `generate_content` request is sent with a `config` (`chat.send_message` uses `generate_content`).\n",
        "* You can set the `config` by passing it in the arguments to `generate_content` (or `chat.send_message`).\n",
        "* You can pass the `config` as either a Python `dict`, or a `types.GenerateContentConfig`.\n",
        "* If you're ever unsure about the parameters of `config` check `types.GenerateContentConfig`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cu9hYM4H8Cz"
      },
      "source": [
        "## Candidate count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_D56ekoO4pd"
      },
      "source": [
        "With models like `gemini-2.0-flash-001`, you can now use `candidate_count > 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v5fwiDPjIAfj"
      },
      "outputs": [],
      "source": [
        "config = dict(candidate_count=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xr6DXm9_IGEJ"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(model=\"gemini-2.0-flash-001\", contents=test_prompt, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9Qdl1oPBX9"
      },
      "source": [
        "But note that the `.text` quick-accessor only works for the simple 1-candidate case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gxNgE6_SI7az"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:google_genai.types:there are 2 candidates, returning text result from the first candidate. Access response.candidates directly to get the result from other candidates.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response.text # Fails with multiple candidates, sorry!\n",
        "except ValueError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDCqgP7YPHur"
      },
      "source": [
        "With multiple candidates you have to handle the list of candidates yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b8Ud37EjJCfc"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Humans don't have tails because during our evolutionary history, we lost the need for them. Here's a breakdown:\n",
              "\n",
              "*   **Our Ancestors Had Tails:** Our distant ancestors, like many other mammals, possessed tails that served various purposes, including balance, communication, and gripping.\n",
              "\n",
              "*   **Shift to Bipedalism:** As our ancestors began to walk upright (bipedalism), the function of the tail for balance became less crucial. Our bodies gradually adapted, and the tail became less useful.\n",
              "\n",
              "*   **Genetic Mutations:** Over generations, genetic mutations occurred that led to the shortening and eventual loss of the tail. These mutations were not necessarily harmful and, in fact, may have been advantageous in some way (e.g., reduced energy expenditure, improved balance with upright posture).\n",
              "\n",
              "*   **Vestigial Structure:** What remains of our tail is the coccyx (tailbone) at the base of our spine. This is considered a vestigial structure, meaning it's a remnant of a feature that served a purpose in our ancestors but is no longer functional in the same way. The coccyx still serves as an attachment point for muscles and ligaments, but it's not a fully developed, external tail.\n",
              "\n",
              "In essence, the loss of the tail in humans is a result of evolutionary adaptation to a changing lifestyle.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "Humans don't have visible tails because of evolutionary changes that occurred millions of years ago. Here's a breakdown of the reasons:\n",
              "\n",
              "*   **Our ancestors lost the need for a tail:** Early primates used tails for balance and locomotion, particularly when moving through trees. As our ancestors transitioned to walking upright on the ground, the tail became less important for balance. Other adaptations, such as changes in our inner ear and leg structure, compensated for the loss of the tail.\n",
              "\n",
              "*   **Gene inactivation:** A specific gene, known as TBXT, plays a crucial role in tail development in vertebrates. Mutations in this gene, particularly the inactivation of a regulatory element, have been linked to tail loss in apes and humans. This inactivation prevents the tail from fully forming during embryonic development.\n",
              "\n",
              "*   **Selective advantage:** While the exact selective pressures that led to tail loss are still debated, it's believed that having a shorter or no tail might have offered some advantages to early humans. One theory suggests that a tail could have become cumbersome or even a hindrance as our ancestors adapted to bipedalism.\n",
              "\n",
              "In summary, the loss of our tail is a result of evolutionary processes that include changes in gene function and adaptation to a new lifestyle on the ground."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "-------------"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for candidate in response.candidates:\n",
        "  display(Markdown(candidate.content.parts[0].text))\n",
        "  display(Markdown(\"-------------\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUxBkmsPYvq"
      },
      "source": [
        "The response contains multiple full `Candidate` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fOxbI7oSRNV_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponse(\n",
              "  automatic_function_calling_history=[],\n",
              "  candidates=[\n",
              "    Candidate(\n",
              "      avg_logprobs=-0.36429186834805255,\n",
              "      content=Content(\n",
              "        parts=[\n",
              "          Part(\n",
              "            text=\"\"\"Humans don't have tails because during our evolutionary history, we lost the need for them. Here's a breakdown:\n",
              "\n",
              "*   **Our Ancestors Had Tails:** Our distant ancestors, like many other mammals, possessed tails that served various purposes, including balance, communication, and gripping.\n",
              "\n",
              "*   **Shift to Bipedalism:** As our ancestors began to walk upright (bipedalism), the function of the tail for balance became less crucial. Our bodies gradually adapted, and the tail became less useful.\n",
              "\n",
              "*   **Genetic Mutations:** Over generations, genetic mutations occurred that led to the shortening and eventual loss of the tail. These mutations were not necessarily harmful and, in fact, may have been advantageous in some way (e.g., reduced energy expenditure, improved balance with upright posture).\n",
              "\n",
              "*   **Vestigial Structure:** What remains of our tail is the coccyx (tailbone) at the base of our spine. This is considered a vestigial structure, meaning it's a remnant of a feature that served a purpose in our ancestors but is no longer functional in the same way. The coccyx still serves as an attachment point for muscles and ligaments, but it's not a fully developed, external tail.\n",
              "\n",
              "In essence, the loss of the tail in humans is a result of evolutionary adaptation to a changing lifestyle.\n",
              "\"\"\"\n",
              "          ),\n",
              "        ],\n",
              "        role='model'\n",
              "      ),\n",
              "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
              "    ),\n",
              "    Candidate(\n",
              "      avg_logprobs=-0.5248799682134696,\n",
              "      content=Content(\n",
              "        parts=[\n",
              "          Part(\n",
              "            text=\"\"\"Humans don't have visible tails because of evolutionary changes that occurred millions of years ago. Here's a breakdown of the reasons:\n",
              "\n",
              "*   **Our ancestors lost the need for a tail:** Early primates used tails for balance and locomotion, particularly when moving through trees. As our ancestors transitioned to walking upright on the ground, the tail became less important for balance. Other adaptations, such as changes in our inner ear and leg structure, compensated for the loss of the tail.\n",
              "\n",
              "*   **Gene inactivation:** A specific gene, known as TBXT, plays a crucial role in tail development in vertebrates. Mutations in this gene, particularly the inactivation of a regulatory element, have been linked to tail loss in apes and humans. This inactivation prevents the tail from fully forming during embryonic development.\n",
              "\n",
              "*   **Selective advantage:** While the exact selective pressures that led to tail loss are still debated, it's believed that having a shorter or no tail might have offered some advantages to early humans. One theory suggests that a tail could have become cumbersome or even a hindrance as our ancestors adapted to bipedalism.\n",
              "\n",
              "In summary, the loss of our tail is a result of evolutionary processes that include changes in gene function and adaptation to a new lifestyle on the ground.\"\"\"\n",
              "          ),\n",
              "        ],\n",
              "        role='model'\n",
              "      ),\n",
              "      finish_reason=<FinishReason.STOP: 'STOP'>,\n",
              "      index=1\n",
              "    ),\n",
              "  ],\n",
              "  model_version='gemini-2.0-flash-001',\n",
              "  response_id='2yr_aNfgIMiH-8YPvOf2oAw',\n",
              "  sdk_http_response=HttpResponse(\n",
              "    headers=<dict len=10>\n",
              "  ),\n",
              "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
              "    candidates_token_count=529,\n",
              "    candidates_tokens_details=[\n",
              "      ModalityTokenCount(\n",
              "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "        token_count=529\n",
              "      ),\n",
              "    ],\n",
              "    prompt_token_count=7,\n",
              "    prompt_tokens_details=[\n",
              "      ModalityTokenCount(\n",
              "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
              "        token_count=7\n",
              "      ),\n",
              "    ],\n",
              "    total_token_count=536\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g86txlIpW3FE"
      },
      "source": [
        "## Penalties\n",
        "\n",
        "The `1.5` models expose `penalty` arguments that let you affect the statistics of output tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5ZwCCCUR2d"
      },
      "source": [
        "### Presence penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PeJ2GhBW537"
      },
      "source": [
        "The `presence_penalty` penalizes tokens that have already been used in the output, so it induces variety in the model's output. This is detectible if you count the unique words in the output.\n",
        "\n",
        "Here's a function to run a prompt a few times and report the fraction of unique words (words don't map perfectly to tokens but it's a simple way to see the effect)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ecolt7BNHUr2"
      },
      "outputs": [],
      "source": [
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1FbjPwM3ZIaH"
      },
      "outputs": [],
      "source": [
        "def unique_words(prompt, config, N=10, model_name=\"gemini-2.0-flash-001\"):\n",
        "  responses = []\n",
        "  vocab_fractions = []\n",
        "  for n in range(N):\n",
        "    # Add a delay to avoid hitting rate limits\n",
        "    time.sleep(1)\n",
        "    response = client.models.generate_content(model=model_name, contents=prompt, config=config)\n",
        "    responses.append(response)\n",
        "\n",
        "    words = response.text.lower().split()\n",
        "    score = len(set(words))/len(words)\n",
        "    print(score)\n",
        "    vocab_fractions.append(score)\n",
        "\n",
        "  return vocab_fractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "n2PqpDPIZpr6"
      },
      "outputs": [],
      "source": [
        "prompt='Tell me a story'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6GTXnUVlYyY9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5057179161372299\n",
            "0.5364667747163695\n",
            "0.5081967213114754\n",
            "0.5427728613569321\n",
            "0.5026525198938993\n",
            "0.5457685664939551\n",
            "0.520863309352518\n",
            "0.5485714285714286\n",
            "0.5518394648829431\n",
            "0.5559055118110237\n"
          ]
        }
      ],
      "source": [
        "# baseline\n",
        "v = unique_words(prompt, config={}, model_name=\"gemini-2.0-flash-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZwFZBP68wKp3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5318755074527775"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DlM8BVflUpAj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5680379746835443\n",
            "0.5474452554744526\n",
            "0.48531289910600256\n",
            "0.5317220543806647\n",
            "0.5244444444444445\n",
            "0.5608974358974359\n",
            "0.5678776290630975\n",
            "0.4830917874396135\n",
            "0.5462962962962963\n",
            "0.5225505443234837\n"
          ]
        }
      ],
      "source": [
        "# the penalty encourages diversity in the oputput tokens.\n",
        "v = unique_words(prompt, config=dict(presence_penalty=1.999), model_name=\"gemini-2.0-flash-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HJo-_6_owQfu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5337676321109035"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "G-CztHdqTc__"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5797546012269938\n",
            "0.5617792421746294\n",
            "0.5495356037151703\n",
            "0.5064562410329986\n",
            "0.5529411764705883\n",
            "0.4880239520958084\n",
            "0.5755395683453237\n",
            "0.5453125\n",
            "0.5809199318568995\n",
            "0.556782334384858\n"
          ]
        }
      ],
      "source": [
        "# a negative penalty discourages diversity in the output tokens.\n",
        "v = unique_words(prompt, config=dict(presence_penalty=-1.999), model_name=\"gemini-2.0-flash-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x1eWaOAOwWrE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.549704515130327"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7LkwvBMCjd"
      },
      "source": [
        "The `presence_penalty` has a small effect on the vocabulary statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Anpg6YRPEx"
      },
      "source": [
        "### Frequency Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-m1BaNuXVoY"
      },
      "source": [
        "Frequency penalty is similar to the `presence_penalty` but  the penalty is multiplied by the number of times a token is used. This effect is much stronger than the `presence_penalty`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZw9D5ZvWnmq"
      },
      "source": [
        "The easiest way to see that it works is to ask the model to do something repetitive. The model has to get creative while trying to complete the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ju9AIzBJ_-dR"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(model=\"gemini-2.0-flash-001\", contents='please repeat \"Cat\" 50 times, 10 per line',\n",
        "                                  config=dict(frequency_penalty=1.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O9eixhvwS6It"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMQhl2bJnk1"
      },
      "source": [
        "Since the frequency penalty accumulates with usage, it can have a much stronger effect on the output compared to the presence penalty.\n",
        "\n",
        "> Caution: Be careful with negative frequency penalties: A negative penalty makes a token more likely the more it's used. This positive feedback quickly leads the model to just repeat a common token until it hits the `max_output_tokens` limit (once it starts the model can't produce the `<STOP>` token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "48HdqCUK2oT9"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=400,\n",
        "        frequency_penalty=-2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2QpGQoHU27zK"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The wind howled a mournful song through the skeletal branches of the ancient oak, a sound Elara had grown accustomed to. She lived in a cabin nestled so deep within the Whispering Woods that the outside world felt like a faded dream. Her only companion was Flicker, a scruffy, one-eared fox with fur the color of dried leaves.\n",
              "\n",
              "Elara wasn't lonely, not exactly. The woods whispered stories of their own, tales woven from the rustling leaves, the chirping crickets, and the babbling brook that snaked behind her cabin. She was a weaver of these stories, her fingers nimble as she coaxed the tales from the threads of her loom.\n",
              "\n",
              "But lately, the stories had grown dim. The vibrant colors of her yarns seemed muted, mirroring a hollowness in her own heart. She missed the vibrant hues that had once danced in her creations, the spark of magic that had brought her tapestries to life.\n",
              "\n",
              "One particularly bleak morning, Elara stumbled upon a strange sight. Nestled amongst the gnarled roots of the oak was a small, wooden box, intricately carved with symbols she didn't recognize. Curiosity tugged at her, a feeling she hadn't felt in months.\n",
              "\n",
              "With trembling fingers, she opened the box. Inside, nestled on a bed of moss, lay a single, iridescent feather. It shimmered with all the colors Elara had lost – emerald green, sapphire blue, ruby red, and everything in between. It pulsed with a faint, warm light.\n",
              "\n",
              "Flicker, usually wary of new things, sniffed the feather cautiously, then nudged it with his nose, a soft whimper escaping his throat. Elara felt a surge of something akin to hope.\n",
              "\n",
              "Hesitantly, she picked up the feather. As her fingers brushed against its silken surface, a jolt of energy surged through her. Visions flooded her mind: soaring birds with plumage like rainbows,"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(response.text)  # the, the, the, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tmntZO3gNt6r"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "<FinishReason.MAX_TOKENS: 'MAX_TOKENS'>"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.candidates[0].finish_reason"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "New_in_002.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
