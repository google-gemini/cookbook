{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb5yiH5h8x3h"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "906e07f6e562"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMGdicu8PVD9"
      },
      "source": [
        "# Video understanding with Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR4Ti6Q0QKIl"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Video_understanding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w14yjWnPVD-"
      },
      "source": [
        "Gemini has from the begining been a multimodal model, capable of analyzing all sorts of medias using its [long context window](https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/).\n",
        "\n",
        "[Gemini models](https://ai.google.dev/gemini-api/docs/models/) bring video analysis to a whole new level as illustrated in [this video](https://www.youtube.com/watch?v=Mot-JEU26GQ):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CumMaR-sts53"
      },
      "outputs": [],
      "source": [
        "#@title Building with Gemini 2.0: Video understanding\n",
        "%%html\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Mot-JEU26GQ?si=pcb7-_MZTSi_1Zkw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jexx9acnuDsA"
      },
      "source": [
        "This notebook will show you how to easily use Gemini to perform the same kind of video analysis. Each of them has different prompts that you can select using the dropdown, also feel free to experiment with your own.\n",
        "\n",
        "You can also check the [live demo](https://aistudio.google.com/starter-apps/video) and try it on your own videos on [AI Studio](https://aistudio.google.com/starter-apps/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0HWzIEAQYqz"
      },
      "source": [
        "## Setup\n",
        "\n",
        "This section install the SDK, set it up using your [API key](../quickstarts/Authentication.ipynb), imports the relevant libs, downloads the sample videos and upload them to Gemini.\n",
        "\n",
        "Expand the section if you are curious, but you can also just run it (it should take a couple of minutes since there are large files) and go straight to the examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzBKAaL4QYq0"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The new **[Google Gen AI SDK](https://ai.google.dev/gemini-api/docs/sdks)** provides programmatic access to Gemini 2.0 (and previous models) using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://ai.google.dev/gemini-api/docs/sdks) or in the [Getting started](../quickstarts/Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IbKkL5ksQYq1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/196.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m194.6/196.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q \"google-genai>=1.16.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDUGen_kQYq2"
      },
      "source": [
        "### Setup your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0H_lRdlrQYq3"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Lez1vBQYq3"
      },
      "source": [
        "### Initialize SDK client\n",
        "\n",
        "With the new SDK you now only need to initialize a client with you API key (or OAuth if using [Vertex AI](https://cloud.google.com/vertex-ai)). The model is now set in each call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X3CAp9YrQYq4"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITgsQyaXQYq4"
      },
      "source": [
        "### Select the Gemini model\n",
        "\n",
        "Video understanding works best with Gemini 2.5 models. You can also select former models to compare their behavior but it is recommended to use at least the 2.0 ones.\n",
        "\n",
        "For more information about all Gemini models, check the [documentation](https://ai.google.dev/gemini-api/docs/models/gemini) for extended information on each of them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IO7IoqbrQYq5"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\" # @param [\"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-05-06\",\"gemini-2.0-flash\",\"gemini-2.0-flash-lite\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv8ULT0lvJ47"
      },
      "source": [
        "### Get sample videos\n",
        "\n",
        "You will start with uploaded videos, as it's a more common use-case, but you will also see later that you can also use Youtube videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fMcwUw48vL1N"
      },
      "outputs": [],
      "source": [
        "# Load sample images\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4 -O Pottery.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4 -O Trailcam.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4 -O Post_its.mp4 -q\n",
        "!wget https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4 -O User_study.mp4 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4YMNQulz_yY"
      },
      "source": [
        "### Upload the videos\n",
        "\n",
        "Upload all the videos using the File API. You can find modre details about how to use it in the [Get Started](../quickstarts/Get_started.ipynb#scrollTo=KdUjkIQP-G_i) notebook.\n",
        "\n",
        "This can take a couple of minutes as the videos will need to be processed and tokenized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LUUMJ4kE0OZS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/kf6plbysnobp\n",
            "Waiting for video to be processed.\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/fy5ajdxy7ayy\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/iate7gdvvcw1\n",
            "Waiting for video to be processed.\n",
            "Video processing complete: https://generativelanguage.googleapis.com/v1beta/files/phl4zyzf11gr\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def upload_video(video_file_name):\n",
        "  video_file = client.files.upload(file=video_file_name)\n",
        "\n",
        "  while video_file.state == \"PROCESSING\":\n",
        "      print('Waiting for video to be processed.')\n",
        "      time.sleep(10)\n",
        "      video_file = client.files.get(name=video_file.name)\n",
        "\n",
        "  if video_file.state == \"FAILED\":\n",
        "    raise ValueError(video_file.state)\n",
        "  print(f'Video processing complete: ' + video_file.uri)\n",
        "\n",
        "  return video_file\n",
        "\n",
        "pottery_video = upload_video('Pottery.mp4')\n",
        "trailcam_video = upload_video('Trailcam.mp4')\n",
        "post_its_video = upload_video('Post_its.mp4')\n",
        "user_study_video = upload_video('User_study.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF5tDbb-Q0oc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B0Z9QzC3Q2wX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAa7sCD7tuMW"
      },
      "source": [
        "# Search within videos\n",
        "\n",
        "First, try using the model to search within your videos and describe all the animal sightings in the trailcam video.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Jukin_Trailcam_Videounderstanding.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PZw41-lsKKMf"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "- **00:00**- The camera lens is covered by brown fur. A \"chirp\" sound is heard.\n- **00:01**- The camera view clears, showing two gray foxes interacting in a rocky, leaf-covered area. One fox walks in from the right, tail first, and sniffs the ground. Another fox enters and joins it, both foraging. \"Rustling sounds\" \"Clicks\"\n- **00:11**- One fox jumps onto a large rock, looking around, while the other continues to forage below. \"Rustling sounds\"\n- **00:17**- A mountain lion appears in infrared (black and white footage). It walks slowly, sniffing the ground intently, then pauses to look up, before continuing to walk out of frame. \"Clicks\" \"Rustling sounds\"\n- **00:34**- Two gray foxes are seen at night in infrared. One appears to be digging or scratching at the ground, while the other approaches and seems to lunge playfully at the first one. \"Clicks\" \"Rustling sounds\"\n- **00:50**- Two gray foxes are visible at night in infrared. One climbs onto a large rock, then jumps off and continues to move around. \"Clicks\" \"Rustling sounds\"\n- **01:04**- A mountain lion is seen walking away from the camera at night in infrared. \"Clicks\"\n- **01:17**- Two mountain lions appear at night in infrared. The first one walks across the foreground and out of sight. The second one then appears behind it, walking across some rocks and disappearing from view. \"Clicks\"\n- **01:29**- A bobcat is seen at night in infrared. It walks into the frame, pauses to sniff the ground, then walks out of frame to the right. \"Clicks\"\n- **01:51**- A brown-colored black bear walks from right to left across the frame in daylight. \"Clicks\" \"Rustling sounds\"\n- **01:56**- A mountain lion walks from right to left across the frame in infrared (black and white footage). \"Clicks\"\n- **02:05**- A mother black bear (brown phase) and her cub appear in daylight. The cub walks into the frame first, followed by the mother. They both forage on the ground before walking away from the camera. \"Clicks\" \"Rustling sounds\"\n- **02:22**- A gray fox is seen at night in infrared, on a ridge overlooking a city illuminated by lights. It sniffs the ground. \"Clicks\"\n- **02:34**- A black bear (brown phase) walks from right to left across the frame at night in infrared, with city lights visible in the background. \"Clicks\"\n- **02:42**- A mountain lion walks from left to right across the frame at night in infrared, with city lights visible in the background. \"Clicks\"\n- **02:51**- A mountain lion is seen at night in infrared. It approaches the camera from behind, then turns around and walks away from the camera. \"Clicks\"\n- **03:04**- A black bear (dark phase) stands in the frame in daylight. It sniffs the air, looks around, then walks off to the left. \"Clicks\" \"Rustling sounds\"\n- **03:22**- A black bear (brown phase) is seen sniffing the ground in daylight. \"Clicks\"\n- **03:32**- A mother black bear (brown phase) and her cub walk across the frame in daylight. The cub is seen from behind. \"Clicks\"\n- **03:40**- Two black bears (brown phase) are seen foraging on the ground in daylight. \"Clicks\" \"Rustling sounds\"\n- **04:03**- Two black bears (brown phase) walk towards the camera in daylight, then pass by it. \"Clicks\"\n- **04:22**- A bobcat walks from left to right across the frame at night in infrared. \"Clicks\"\n- **04:30**- A gray fox walks towards the camera at night in infrared. \"Clicks\"\n- **04:49**- A gray fox walks away from the camera at night in infrared. \"Clicks\"\n- **04:57**- A mountain lion approaches the camera, sniffs the ground, and then walks away from the camera at night in infrared. \"Clicks\" \"Rustling sounds\"",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\"  # @param [\"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\", \"Organize all scenes from this video in a table, along with timecode, a short description, a list of objects visible in the scene (with representative emojis) and an estimation of the level of excitement on a scale of 1 to 10\"] {\"allow-input\":true}\n",
        "\n",
        "video = trailcam_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOQzKYGJKAnD"
      },
      "source": [
        "The prompt used is quite a generic one, but you can get even better results if you cutomize it to your needs (like asking specifically for foxes).\n",
        "\n",
        "The [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows how you can postprocess this output to jump directly to the the specific part of the video by clicking on the timecodes. If you are interested, you can check the [code of that demo on Github](https://github.com/google-gemini/starter-applets/tree/main/video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wog32E7CKnT6"
      },
      "source": [
        "# Extract and organize text\n",
        "\n",
        "Gemini models can also read what's in the video and extract it in an organized way. You can even use Gemini reasoning capabilities to generate new ideas for you.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/post_its.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "baNCeA3GKrfu"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here are the project name ideas transcribed from the sticky notes, organized in a table, followed by a few more ideas:\n\n## Brainstorm: Project Name Ideas\n\n| Project Name Ideas   |\n| :------------------- |\n| Aether               |\n| Andromeda's Reach    |\n| Astral Forge         |\n| Athena               |\n| Athena's Eye         |\n| Bayes Theorem        |\n| Canis Major          |\n| Celestial Drift      |\n| Centaurus            |\n| Cerberus             |\n| Chaos Field          |\n| Chaos Theory         |\n| Chimera Dream        |\n| Comets Tail          |\n| Convergence          |\n| Delphinus            |\n| Draco                |\n| Echo                 |\n| Equilibrium          |\n| Euler's Path         |\n| Fractal              |\n| Galactic Core        |\n| Golden Ratio         |\n| Hera                 |\n| Infinity Loop        |\n| Leo Minor            |\n| Lunar Eclipse        |\n| Lyra                 |\n| Lynx                 |\n| Medusa               |\n| Odin                 |\n| Orion's Belt         |\n| Orion's Sword        |\n| Pandora's Box        |\n| Perseus Shield       |\n| Phoenix              |\n| Prometheus Rising    |\n| Riemann's Hypothesis |\n| Sagitta              |\n| Serpens              |\n| Stellar Nexus        |\n| Stokes Theorem       |\n| Supernova Echo       |\n| Symmetry             |\n| Taylor Series        |\n| Titan                |\n| Vector               |\n| Zephyr               |\n\n---\n\n## A Few More Project Name Ideas:\n\n1.  **Quantum Leap**\n2.  **Cosmic Weave**\n3.  **Ares Vanguard**\n4.  **Vortex Protocol**\n5.  **Event Horizon**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\" # @param [\"Transcribe the sticky notes, organize them and put it in a table. Can you come up with a few more ideas?\", \"Which of those names who fit an AI product that can resolve complex questions using its thinking abilities?\"] {\"allow-input\":true}\n",
        "\n",
        "video = post_its_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjKIsLDMTNk1"
      },
      "source": [
        "# Structure information\n",
        "\n",
        "Gemini 2.0 is not only able to read text but also to reason and structure about real world objects. Like in this video about a display of ceramics with handwritten prices and notes.\n",
        "\n",
        "<video controls width=\"500\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/Pottery.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bqzqedMFT5Wp"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here's a table summarizing the items and their details from the image:\n\n| Item Type       | Description                                                                                                                                                                                                                             | Quantity | Dimensions                       | Price         | Notes/Glaze Information                                                      |\n| :-------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------- | :------------------------------- | :------------ | :--------------------------------------------------------------------------- |\n| **Tumblers**    | Ceramic tumblers with a speckled light brown/beige base and a distinct horizontal line where a second, lighter bluish/white glaze has been applied over the top half. The bottom portion appears unglazed.                                     | 7        | ~4\"h x 3\"d                       | \\$20 each     | Glaze: #5 Artichoke double dip                                               |\n| **Small Bowls** | Round ceramic bowls with a reddish-brown, speckled clay body and a dark, glossy glaze that shows hints of metallic or greenish tones, particularly around the rim.                                                                     | 2        | 3.5\"h x 6.5\"d                    | \\$35 each     |                                                                              |\n| **Medium Bowls**| Larger and deeper round ceramic bowls, similar in clay body and glaze to the small bowls but with more prominent greenish/bluish variations in the dark, glossy glaze, especially on the inner rim.                                     | 3        | 4\"h x 7\"d                        | \\$40 each     |                                                                              |\n| **Glaze Test Tile** | Small, irregular ceramic pieces; one is reddish-brown, the other displays a mottled green/blue/gray glaze over reddish-brown. This likely represents the \"#5 Artichoke double dip\" glaze used on the tumblers. (Not for sale as an item) | 2        | N/A                              | N/A           | Glaze sample: #5 Artichoke double dip                                        |\n| **Glaze Test Tile** | Small, rectangular ceramic piece with a textured surface, showing a dark brown/black base with lighter brown specks and a distinct area of bluish/white glaze on one side. Marked \"6rb.\" (Not for sale as an item)                       | 1        | N/A                              | N/A           | Glaze sample: #6 Gemini double dip SLOW COOL. Marked \"6rb\" (possibly a batch/kiln number). |",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Give me a table of my items and notes\" # @param [\"Give me a table of my items and notes\", \"Help me come up with a selling pitch for my potteries\"] {\"allow-input\":true}\n",
        "\n",
        "video = pottery_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ],\n",
        "    config = types.GenerateContentConfig(\n",
        "        system_instruction=\"Don't forget to escape the dollar signs\",\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsh6i-Z6VHNK"
      },
      "source": [
        "As you can see, Gemini is able to grasp to with item corresponds each note, including the last one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIfsFC0pVUTD"
      },
      "source": [
        "# Analyze screen recordings for key moments\n",
        "\n",
        "You can also use the model to analyze screen recordings. Let's say you're doing user studies on how people use your product, so you end up with lots of screen recordings, like this one, that you have to manually comb through.\n",
        "With just one prompt, the model can describe all the actions in your video.\n",
        "\n",
        "<video controls width=\"400\"><source src=\"https://storage.googleapis.com/generativeai-downloads/videos/user_study.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wrMHZ0MxW75y"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "This video demonstrates the functionalities of a \"My Garden App\" for purchasing plants. It begins by showcasing a list of various plants with their descriptions and prices (0:00-0:09). Users can interact with each plant by clicking a \"Like\" button, which turns red when activated, or an \"Add to Cart\" button, which confirms the item addition (0:09-0:25).\n\nThe video illustrates adding a Fern, Cactus, and Hibiscus to the cart. Navigating to the \"Cart\" tab displays the selected items along with their individual prices and a total cost of $58.97 (0:30-0:33). The \"Profile\" tab then summarizes user activity, showing the number of liked plants and cart items (0:33-0:35). The user then returns to the home screen and continues browsing and adding more items (0:37-0:45).",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\" # @param [\"Generate a paragraph that summarizes this video. Keep it to 3 to 5 sentences with corresponding timecodes.\", \"Choose 5 key shots from this video and put them in a table with the timecode, text description of 10 words or less, and a list of objects visible in the scene (with representative emojis).\", \"Generate bullet points for the video. Place each bullet point into an object with the timecode of the bullet point in the video.\"] {\"allow-input\":true}\n",
        "\n",
        "video = user_study_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        prompt,\n",
        "    ]\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEYYemjyKcZ7"
      },
      "source": [
        "# Analyze youtube videos\n",
        "\n",
        "On top of using your own videos you can also ask Gemini to get a video from Youtube and analyze it. He's an example using the keynote from Google IO 2023. Guess what the main theme was?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DP0Dd0hJKvYm"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here are all the instances where Sundar says \"AI\" in the video, along with timestamps and broader context:\n\n1.  **0:28 - 0:31:** \"As you may have heard, **AI** is having a very busy year.\"\n    *   **Context:** Sundar is opening the Google I/O keynote and immediately highlights the significant activity and progress in the field of Artificial Intelligence.\n\n2.  **0:38 - 0:41:** \"Seven years into our journey as an **AI**-first company.\"\n    *   **Context:** He references Google's foundational shift seven years prior to prioritize AI, emphasizing that AI is now at an \"exciting inflection point.\"\n\n3.  **0:45 - 0:48:** \"We have an opportunity to make **AI** even more helpful for people, for businesses, for communities, for everyone.\"\n    *   **Context:** Sundar articulates the core mission of Google's AI development: to enhance helpfulness on a broad scale.\n\n4.  **0:54 - 0:57:** \"We've been applying **AI** to make our products radically more helpful for a while.\"\n    *   **Context:** He establishes that Google has a history of integrating AI into its products to improve their utility.\n\n5.  **0:59 - 1:02:** \"With generative **AI**, we are taking the next step.\"\n    *   **Context:** Sundar introduces generative AI as the next frontier for product innovation at Google.\n\n6.  **1:16 - 1:19:** \"Let me start with few examples of how generative **AI** is helping to evolve our products, starting with Gmail.\"\n    *   **Context:** He transitions into specific demonstrations of generative AI's application in various Google products.\n\n7.  **1:40 - 1:42:** \"Smart Compose led to more advanced writing features powered by **AI**.\"\n    *   **Context:** Discussing how existing features like Smart Compose in Gmail have evolved through AI, leading to new capabilities like \"Help me write.\"\n\n8.  **3:02 - 3:05:** \"Since the early days of Street View, **AI** has stitched together billions of panoramic images so people can explore the world from their device.\"\n    *   **Context:** Explaining AI's long-standing role in creating and enhancing Google Maps' Street View experience.\n\n9.  **3:13 - 3:17:** \"At last year's I/O, we introduced immersive view, which uses **AI** to create a high-fidelity representation of a place, so you can experience it before you visit.\"\n    *   **Context:** Highlighting a specific AI-powered feature in Google Maps that provides detailed virtual exploration.\n\n10. **5:15 - 5:17:** \"It was one of our first **AI**-native products.\"\n    *   **Context:** Referring to Google Photos as an early and successful example of a product built from the ground up with AI.\n\n11. **5:40 - 5:42:** \"**AI** advancements give us more powerful ways to do this.\"\n    *   **Context:** Speaking about how advancements in AI are enabling more sophisticated photo editing capabilities.\n\n12. **5:47 - 5:51:** \"Magic Eraser, launched first on Pixel, uses **AI**-powered computational photography to remove unwanted distractions.\"\n    *   **Context:** Providing a concrete example of an existing AI feature in Google Photos that helps users edit images.\n\n13. **5:57 - 6:00:** \"And later this year, using a combination of semantic understanding and generative **AI**, you can do much more with a new experience called Magic Editor.\"\n    *   **Context:** Introducing the next-generation photo editing tool, Magic Editor, built on advanced AI.\n\n14. **7:39 - 7:42:** \"These are just a few examples of how **AI** can help you in moments that matter.\"\n    *   **Context:** Summarizing the product demonstrations and reiterating the helpfulness of AI.\n\n15. **7:47 - 7:49:** \"And there is so much more we can do to deliver the full potential of **AI** across the products you know and love.\"\n    *   **Context:** Expressing continued optimism and ambition for integrating AI more deeply across Google's entire product suite.\n\n16. **8:23 - 8:26:** \"And looking ahead, making **AI** helpful for everyone is the most profound way we will advance our mission.\"\n    *   **Context:** Reiterating the central theme of the keynote: making AI accessible and beneficial to all.\n\n17. **8:52 - 8:57:** \"And finally, by building and deploying **AI** responsibly, so that everyone can benefit equally.\"\n    *   **Context:** Emphasizing the critical importance of ethical and responsible development and deployment of AI.\n\n18. **9:03 - 9:09:** \"Our ability to make **AI** helpful for everyone relies on continuously advancing our foundation models.\"\n    *   **Context:** Connecting the vision of helpful AI to the underlying technological progress in foundation models.\n\n19. **11:25 - 11:33:** \"We recently released Sec-PaLM, a version of PaLM 2 fine-tuned for security use cases. It uses **AI** to better detect malicious scripts and can help security experts understand and resolve threats.\"\n    *   **Context:** Illustrating how specialized AI models like Sec-PaLM are being applied to enhance cybersecurity.\n\n20. **12:44 - 12:49:** \"PaLM 2 is the latest step in our decade-long journey to bring **AI** in responsible ways to billions of people.\"\n    *   **Context:** Positioning PaLM 2 as a continuation of Google's long-term commitment to making AI widely available and beneficial.\n\n21. **12:58 - 13:03:** \"Looking back at the defining **AI** breakthroughs over the last decade, these teams have contributed to a significant number of them.\"\n    *   **Context:** Reflecting on Google's historical contributions to significant AI advancements.\n\n22. **14:09 - 14:11:** \"As we invest in more advanced models, we are also deeply investing in **AI** responsibility.\"\n    *   **Context:** Stressing that technological progress in AI is coupled with a strong focus on ethical considerations.\n\n23. **14:18 - 14:19:** \"Every one of our **AI**-generated images has that metadata.\"\n    *   **Context:** Explaining the mechanisms (watermarking and metadata) Google is implementing to ensure transparency and accountability for AI-generated content.\n\n24. **15:09 - 15:12:** \"James will talk about our responsible approach to **AI** later.\"\n    *   **Context:** Transitioning to another speaker who will delve deeper into Google's ethical framework for AI.\n\n25. **15:28 - 15:31:** \"That's the opportunity we have with Bard, our experiment for conversational **AI**.\"\n    *   **Context:** Concluding the segment on products with an introduction to Bard as a public-facing conversational AI tool.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Find all the instances where Sundar says \\\"AI\\\". Provide timestamps and broader context for each instance.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=ixRanV-rdAQ')\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN0296794Lui"
      },
      "source": [
        "# Customizing video preprocessing\n",
        "\n",
        "The Gemini API allows you to define some preprocessing steps to enhance your abilities to understand and extract information from videos.\n",
        "\n",
        "You can use clipping intervals (or define time offsets to focus on specific video parts) and custom FPS (to define how many frames will be considered to analyze the video.\n",
        "\n",
        "For more details about those features, you can take a look at the [Customizing video preprocessing](https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-preprocessing) at the Gemini API documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEUu4qCY-V7r"
      },
      "source": [
        "## Analyze specific parts of videos using clipping intervals\n",
        "\n",
        "Sometimes you want to look for specific parts of your videos. You can define time offsets on your request, pointing to the model which specific video interval you are more interested about.\n",
        "\n",
        "**Note:** The `video_metadata` that you will inform must be representing the time offsets in seconds.\n",
        "\n",
        "In this example, you are using this video, from [Google I/O 2025 keynote](https://www.youtube.com/watch?v=XEzRZ35urlk) and asking the model to consider specifically the time offset between 20min50s and 26min10s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qeLcdeu95Bd9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Demis Hassabis of Google DeepMind outlined the company's long-term goal of building Artificial General Intelligence (AGI) responsibly to benefit humanity, showcasing recent advancements like AlphaFold 3 for molecular modeling. He then announced Gemini 1.5 Flash, a new lightweight, fast, and cost-efficient multimodal model designed for high-speed and efficient applications, available now with a 1 million token context window. Finally, Hassabis introduced Project Astra, Google DeepMind's vision for a universal AI agent that can proactively understand and respond to the complex real world in a conversational, teachable, and personal manner, aiming for seamless everyday interaction.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),\n",
        "                video_metadata=types.VideoMetadata(\n",
        "                    start_offset='1250s',\n",
        "                    end_offset='1570s'\n",
        "                )\n",
        "            ),\n",
        "            types.Part(text='Please summarize the video in 3 sentences.')\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL2UeuT6X7k"
      },
      "source": [
        "You can also use clipping intervals for videos uploaded to the File API as also inline videos on your prompts (remembering that inline data cannot exceed 20MB in size)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mluu__Cw6ktt"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here's a summary of the video:\n\n*   Multiple mountain lions (cougars) are captured by a trail camera at night, walking through the forest.\n*   In some clips, two mountain lions appear together.\n*   A bobcat is seen sniffing the ground and briefly lying down at night.\n*   A black bear walks across the frame during the daytime.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"Summarize this video in few short bullets\"  # @param [\"For each scene in this video, generate captions that describe the scene along with any spoken text placed in quotation marks. Place each caption into an object with the timecode of the caption in the video.\", \"Organize all scenes from this video in a table, along with timecode, a short description, a list of objects visible in the scene (with representative emojis) and an estimation of the level of excitement on a scale of 1 to 10\"] {\"allow-input\":true}\n",
        "\n",
        "video = trailcam_video # @param [\"trailcam_video\", \"pottery_video\", \"post_its_video\", \"user_study_video\"] {\"type\":\"raw\",\"allow-input\":true}\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                file_data=types.FileData(\n",
        "                    file_uri=video.uri,\n",
        "                    mimeType=video.mime_type),\n",
        "                video_metadata=types.VideoMetadata(\n",
        "                    start_offset='60s',\n",
        "                    end_offset='120s'\n",
        "                )\n",
        "            ),\n",
        "            types.Part(text=prompt)\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZGz46VP8pE5"
      },
      "source": [
        "## Customize the number of video frames per second (FPS) analyzed\n",
        "\n",
        "By default, the Gemini API extract 1 (one) FPS to analyze your videos. But this amount may be too much (for videos with less activities, like a lecture) or to preserve more detail in fast-changing visuals, a higher FPS should be selected.\n",
        "\n",
        "In this scenario, you are using one specific interval of one Nascar pit-stop as also you will capture a higher number of FPS (in this case, 24 FPS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "_6OhwEH0-zHq"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Based on the video, the pit crew changed **all four tires** on the car:\n\n*   The **left front** and **left rear** tires are changed first (from 00:17 to 00:22).\n*   Then, the **right front** and **right rear** tires are changed (from 00:23 to 00:29).\n\nThey also refuel the car during this pit stop.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=types.Content(\n",
        "        parts=[\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=McN0-DpyHzE'),\n",
        "                video_metadata=types.VideoMetadata(\n",
        "                    start_offset='15s',\n",
        "                    end_offset='35s',\n",
        "                    fps=24\n",
        "                )\n",
        "            ),\n",
        "            types.Part(text='How many tires where changed? Front tires or rear tires?')\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cizoUEdIYLd0"
      },
      "source": [
        "Once again, you can check the  [live demo on AI Studio](https://aistudio.google.com/starter-apps/video) shows an example on how to postprocess this output. Check the [code of that demo](https://github.com/google-gemini/starter-applets/tree/main/video) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lND4jB6MrsSk"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "Try with you own videos using the [AI Studio's live demo](https://aistudio.google.com/starter-apps/video) or play with the examples from this notebook (in case you haven't seen, there are other prompts you can try in the dropdowns).\n",
        "\n",
        "For more examples of the Gemini capabilities, check the other guide from the [Cookbook](https://github.com/google-gemini/cookbook/). You'll learn how to use the [Live API](../quickstarts/Get_started_LiveAPI.ipynb), juggle with [multiple tools](../quickstarts/Get_started_LiveAPI_tools.ipynb) or use Gemini 2.0 [spatial understanding](../quickstarts/Spatial_understanding.ipynb) abilities.\n",
        "\n",
        "The [examples](https://github.com/google-gemini/cookbook/tree/main/examples/) folder from the cookbook is also full of nice code samples illustrating creative ways to use Gemini multimodal capabilities and long-context."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Video_understanding.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
