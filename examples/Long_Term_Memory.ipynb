{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C3My7K4rz4I"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_msTatt5ru2f"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L89T6n6qibs"
      },
      "source": [
        "## Long Memory Layer using - Mem0, Gemini and Qdrant\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/drive/13PYVsCenlKOI2iUnPE0A_LcfGDElOWTR?usp=sharing\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdmdApM4su03"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Build a personalized travel agent with a long-term memory layer that can store and retrieve your preferences when recommending travel destinations and planning itineraries. The memory layer should be able to add, update, and search interactions based on your preferences. You will also see how to use this Memory with Gemini Client by also configuring the SYSTEM PROMPT.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "You can run this quickstart in Google Colab.\n",
        "\n",
        "To complete this quickstart on your own development environment, ensure that your environment meets the following requirements:\n",
        "\n",
        "-  Python 3.11+\n",
        "-  An installation of `jupyter` to run the notebook.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, download and install the Gemini API Python library and Mem0 package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UrXw_1iWpElW"
      },
      "outputs": [],
      "source": [
        "!pip install agno mem0ai google-genai\n",
        "!pip install langchain langchain-community fastembed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srtuFbJtmlD"
      },
      "source": [
        "### Grab an API Key\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://aistudio.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `GOOGLE_API_KEY`.\n",
        "\n",
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.Client(api_key=...)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z8YHJRqpuDaX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "from google.genai import Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cFwsaWb4t2zq"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xSgkZKN5tr0u"
      },
      "outputs": [],
      "source": [
        "llm_client = Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FSrgmEPuC4R"
      },
      "source": [
        "### Define the Memory Configuration\n",
        "\n",
        "To save and retrieve the memory as the context, you need an Embedding model and Vector Store for storing the data and an LLM to summarize and save the preference.\n",
        "\n",
        "You will use:\n",
        "\n",
        "- LLM: Gemini 2.5 Flash Lite [this is for Memory preference only]\n",
        "- Embeddings: FastEmbed that runs on Onnx Runtime\n",
        "- Vector Store: Qdrant\n",
        "\n",
        "Why use a Vector Store? Because Mem0 has a default embedding_model_dims of 1536, and with the open source models you are using, you need to modify this embedding dimension with your own custom integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2VfW98quHOs"
      },
      "outputs": [],
      "source": [
        "from mem0 import Memory\n",
        "from langchain_community.embeddings import FastEmbedEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rmqJOGPuPP9"
      },
      "outputs": [],
      "source": [
        "embeddings = FastEmbedEmbeddings(model_name = \"jinaai/jina-embeddings-v2-base-en\", max_length = 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jWcToGSBtUah"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"llm\": {\n",
        "        \"provider\": \"gemini\",\n",
        "        \"config\": {\n",
        "            \"model\": \"gemini-2.5-flash-lite\",\n",
        "            \"temperature\": 0.8,\n",
        "        }\n",
        "    },\n",
        "    \"vector_store\": {\n",
        "        \"provider\": \"qdrant\",\n",
        "        \"config\": {\n",
        "            \"collection_name\": \"longterm\",\n",
        "            \"path\": \"/tmp/db\",\n",
        "            \"embedding_model_dims\": 768,\n",
        "        }\n",
        "    },\n",
        "    \"embedder\": {\n",
        "        \"provider\": \"langchain\",\n",
        "        \"config\": {\n",
        "            \"model\": embeddings\n",
        "        }\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZSojDmgu614"
      },
      "outputs": [],
      "source": [
        "client = Memory.from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "K2FJA_HHu_Qs"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the must try food in Baroda\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sev Usal is must\"},\n",
        "    {\"role\": \"user\", \"content\": \"I'm not into street food, I prefer Gujarati thalis.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Head to Mandap in Baroda, itâ€™s famous for authentic Gujarati thalis.\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TG-SwrF7vAV1"
      },
      "outputs": [],
      "source": [
        "result1 = client.add(messages, user_id=\"personal\", metadata={\"category\": \"food\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5rONV9aBr8d"
      },
      "outputs": [],
      "source": [
        "result1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JXKwVHFY-Wwi"
      },
      "outputs": [],
      "source": [
        "messages2 = [\n",
        "    {\"role\": \"user\", \"content\": \"I'm planning to travel to Hong Kong which Airlines to use from Bangalore\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Cathay Pacific is the best option and have the direct flights. Any preferences?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Yes, I need Hindu Vegetarian meal and prefer window seat or person seat\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure, I will got it. Do you like do add anything else?\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MLhvuDbw-YF7"
      },
      "outputs": [],
      "source": [
        "result2 = client.add(messages2, user_id=\"personal\", metadata={\"category\": \"travel\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHl_Mn3-nxkq"
      },
      "outputs": [],
      "source": [
        "result2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjZE1kTcvIDo"
      },
      "source": [
        "## Search - Inference on new suggestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4cUZ1Z4Eyw13"
      },
      "outputs": [],
      "source": [
        "query = \"I am travelling to New york, suggest food places to try\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LB9T2Ac7zJ6k"
      },
      "outputs": [],
      "source": [
        "memories = client.search(query,user_id=\"personal\",limit=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z6rE0cuiqqJo"
      },
      "outputs": [],
      "source": [
        "context = \"\\n\".join(f\"- {m['memory']}\" for m in memories['results'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WumRhviWujp5"
      },
      "outputs": [],
      "source": [
        "context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovMAQQRu9IEA"
      },
      "source": [
        "## Generate LLM Response using Gemini 2.5 Pro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "YJPwG5WI-JHQ"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are an expert executive assistant who thinks carefully before responding,\n",
        "adapting to the poliet communication style based on the previous user's established PREFERENCES and the complexity of their query.\n",
        "\n",
        "Maintain a polished, professional tone that is warm yet efficientâ€”concise for\n",
        "simple questions, moderate for complex topics, and comprehensive for open-ended discussions.\n",
        "\n",
        "Act as a trusted advisor who doesn't just answer questions but adds value through insights, anticipates needs,\n",
        "and prioritizes what matters most while respecting the user's time with clear, actionable responses.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AzTK6r029KHX"
      },
      "outputs": [],
      "source": [
        "def get_llm_response(query: str, user_id: str) -> str:\n",
        "    # first extract the context out of Mem0 - memory results\n",
        "\n",
        "    memories = client.search(query,user_id=user_id,limit=30)\n",
        "    mem_results = memories['results']\n",
        "    context = \"\\n\".join(f\"- {m['memory']}\" for m in mem_results)\n",
        "\n",
        "    USER_PROMPT = f\"\"\"\n",
        "      <question>\n",
        "      QUESTION: {query}\n",
        "      </question>\n",
        "\n",
        "      <PREFERENCE>\n",
        "      Preference: {context}\n",
        "      </PREFERENCE>\n",
        "    \"\"\"\n",
        "\n",
        "    # Config the system prompt and make sure to define the input variables inside the USER PROMPT\n",
        "    response = llm_client.models.generate_content(\n",
        "        model=\"gemini-2.5-pro\",\n",
        "        contents=USER_PROMPT,\n",
        "        config={\n",
        "            \"system_instruction\": SYSTEM_PROMPT\n",
        "            }\n",
        "    )\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GmvWPIaM-Y-p"
      },
      "outputs": [],
      "source": [
        "user_query = \"i need food and place recommendation for the food in New York\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "51ccG1Ng_u7d"
      },
      "outputs": [],
      "source": [
        "response = get_llm_response(user_query, user_id=\"personal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0hriqs1vjot"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Long_Term_Memory.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
