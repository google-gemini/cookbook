{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lgX9omPXF-"
      },
      "source": [
        "## Gemini API: Getting started with information grounding for Gemini models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkR4fWudrHCs"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Grounding.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDKKNfXWrHgs"
      },
      "source": [
        "In this notebook you will learn how to use information grounding with [Gemini models](https://ai.google.dev/gemini-api/docs/models/).\n",
        "\n",
        "Information grounding is the process of connecting these models to specific, verifiable information sources to enhance the accuracy, relevance, and factual correctness of their responses. While LLMs are trained on vast amounts of data, this knowledge can be general, outdated, or lack specific context for particular tasks or domains. Grounding helps to bridge this gap by providing the LLM with access to curated, up-to-date information.\n",
        "\n",
        "Here you will experiment with:\n",
        "- Grounding information using Google Search grounding\n",
        "- Adding YouTube links to gather context information to your prompt\n",
        "- Using URL context to include website URL as context to your prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKu1tRBrQ7xj"
      },
      "source": [
        "## Set up the SDK\n",
        "\n",
        "This guide uses the [`google-genai`](https://pypi.org/project/google-genai) Python SDK to connect to the Gemini models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIWKUlPqP5NK"
      },
      "source": [
        "### Install SDK\n",
        "\n",
        "The **[Google Gen AI SDK](https://github.com/googleapis/python-genai)** provides programmatic access to Gemini models using both the [Google AI for Developers](https://ai.google.dev/gemini-api/docs/models/) and [Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview) APIs. With a few exceptions, code that runs on one platform will run on both. This means that you can prototype an application using the Developer API and then migrate the application to Vertex AI without rewriting your code.\n",
        "\n",
        "More details about this new SDK on the [documentation](https://googleapis.github.io/python-genai/) or in the [Getting started](./Get_started.ipynb) notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Fr84vJuPSHb"
      },
      "outputs": [],
      "source": [
        "%pip install -q -U \"google-genai>=1.16.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a503bnWNQoCL"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RjvgYmdLQd5s"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhKXgMSNQrrV"
      },
      "source": [
        "### Select model and initialize SDK client\n",
        "\n",
        "Select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C75s1LR9QmOz"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "MODEL_ID = \"gemini-2.5-flash-preview-05-20\" # @param [\"gemini-2.5-flash-preview-05-20\", \"gemini-2.5-pro-preview-06-05\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mDMScex1It5"
      },
      "source": [
        "## Use Google Search grounding\n",
        "Google Search grounding is particularly useful for queries that require current information or external knowledge. Using Google Search, Gemini can access nearly real-time information and better responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FHIcazUO0-xU"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Response:\n The latest Indian Premier League (IPL) match, which was the final of the 2024 season, took place on May 26, 2024.\n\nIn this match, the Kolkata Knight Riders (KKR) defeated the Sunrisers Hyderabad (SRH) by 8 wickets to win their third IPL title. The Sunrisers Hyderabad were bundled out for 113 runs, which was the lowest total in an IPL final.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Search Query: ['latest Indian Premier League match and winner', 'When did IPL 2024 end?']\n",
            "Search Pages: wikipedia.org, jagranjosh.com, livemint.com, thehindu.com\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              ".container {\n",
              "  align-items: center;\n",
              "  border-radius: 8px;\n",
              "  display: flex;\n",
              "  font-family: Google Sans, Roboto, sans-serif;\n",
              "  font-size: 14px;\n",
              "  line-height: 20px;\n",
              "  padding: 8px 12px;\n",
              "}\n",
              ".chip {\n",
              "  display: inline-block;\n",
              "  border: solid 1px;\n",
              "  border-radius: 16px;\n",
              "  min-width: 14px;\n",
              "  padding: 5px 16px;\n",
              "  text-align: center;\n",
              "  user-select: none;\n",
              "  margin: 0 8px;\n",
              "  -webkit-tap-highlight-color: transparent;\n",
              "}\n",
              ".carousel {\n",
              "  overflow: auto;\n",
              "  scrollbar-width: none;\n",
              "  white-space: nowrap;\n",
              "  margin-right: -12px;\n",
              "}\n",
              ".headline {\n",
              "  display: flex;\n",
              "  margin-right: 4px;\n",
              "}\n",
              ".gradient-container {\n",
              "  position: relative;\n",
              "}\n",
              ".gradient {\n",
              "  position: absolute;\n",
              "  transform: translate(3px, -9px);\n",
              "  height: 36px;\n",
              "  width: 9px;\n",
              "}\n",
              "@media (prefers-color-scheme: light) {\n",
              "  .container {\n",
              "    background-color: #fafafa;\n",
              "    box-shadow: 0 0 0 1px #0000000f;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #1f1f1f;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #ffffff;\n",
              "    border-color: #d2d2d2;\n",
              "    color: #5e5e5e;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #f2f2f2;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #d8d8d8;\n",
              "    border-color: #b6b6b6;\n",
              "  }\n",
              "  .logo-dark {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
              "  }\n",
              "}\n",
              "@media (prefers-color-scheme: dark) {\n",
              "  .container {\n",
              "    background-color: #1f1f1f;\n",
              "    box-shadow: 0 0 0 1px #ffffff26;\n",
              "  }\n",
              "  .headline-label {\n",
              "    color: #fff;\n",
              "  }\n",
              "  .chip {\n",
              "    background-color: #2c2c2c;\n",
              "    border-color: #3c4043;\n",
              "    color: #fff;\n",
              "    text-decoration: none;\n",
              "  }\n",
              "  .chip:hover {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:focus {\n",
              "    background-color: #353536;\n",
              "  }\n",
              "  .chip:active {\n",
              "    background-color: #464849;\n",
              "    border-color: #53575b;\n",
              "  }\n",
              "  .logo-light {\n",
              "    display: none;\n",
              "  }\n",
              "  .gradient {\n",
              "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
              "  }\n",
              "}\n",
              "</style>\n",
              "<div class=\"container\">\n",
              "  <div class=\"headline\">\n",
              "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
              "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
              "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
              "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
              "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
              "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
              "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
              "    </svg>\n",
              "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
              "  </div>\n",
              "  <div class=\"carousel\">\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXHWySZV7FmaSfTtng1Ftfk3PEzSI6NYmiEzGwZktDsAHXaQQOrMIzPPjgikATzP2nHVi-P_eFXaLZChsHfhnQMk5XRKy032_BLaDB9UepBlUS6rGYhmN4T9jNtvSmbaTd4F54qaADFRpHuzSuNfLoN-xqUaxJ1CqB9njIlN_Hb84TFBt1JVRVgTUVR07JghL3SrekqZvxppNF92yxs_YHIYcm37edGrTP65Qf82SUSQ\">latest Indian Premier League match and winner</a>\n",
              "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AbF9wXGFSCl_cfg2PZlrXgPcuFFbJWV9SqT406atAj9uSvp9x3XOlN77YS60B34gn7L0fwNhBLk9jpojz6NOObCop76IJGaSYVIlXIESd6KZenNJgLFIXb8wu7X5VMW4N300wSwLlKj3KruVnVVdQiQkIfSV36g5TtormG97lFh3Iu4VmaXBpVIZuWzJLCKgtWyt4DW3LojMN8ewk1fq\">When did IPL 2024 end?</a>\n",
              "  </div>\n",
              "</div>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML, Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='What was the latest Indian Premier League match and who won?',\n",
        "    config={\"tools\": [{\"google_search\": {}}]},\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(f\"Response:\\n {response.text}\"))\n",
        "# print the search details\n",
        "print(f\"Search Query: {response.candidates[0].grounding_metadata.web_search_queries}\")\n",
        "# urls used for grounding\n",
        "print(f\"Search Pages: {', '.join([site.web.title for site in response.candidates[0].grounding_metadata.grounding_chunks])}\")\n",
        "\n",
        "display(HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wROLHEYLLBHX"
      },
      "source": [
        "You can see that running the same prompt without search grounding gives you outdated information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EdUkQ40cKaGX"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "The latest Indian Premier League match played was the **final of the 2024 season**, which took place on **May 26, 2024**.\n\n**Match:** Sunrisers Hyderabad (SRH) vs. Kolkata Knight Riders (KKR)\n**Winner:** **Kolkata Knight Riders (KKR)** won by 8 wickets.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='What was the latest Indian Premier League match and who won?',\n",
        ")\n",
        "\n",
        "# print the response\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XfNrFR7j6F6"
      },
      "source": [
        "## Grounding with YouTube links\n",
        "\n",
        "you can directly include a public YouTube URL in your prompt. The Gemini models will then process the video content to perform tasks like summarization and answering questions about the content.\n",
        "\n",
        "This capability leverages Gemini's multimodal understanding, allowing it to analyze and interpret video data alongside any text prompts provided.\n",
        "\n",
        "You do need to explicitly declare the video URL you want the model to process as part of the contents of the request. Here a simple interaction where you ask the model to summarize a YouTube video:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "akVTribOkgT2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "This video introduces \"Gemma Chess,\" a new application of Google DeepMind's Gemma language model to the game of chess. The speaker, Ju-yeong Ji, explains that unlike traditional chess engines (like AlphaZero) which are \"super smart calculators\" focused on finding the best moves, Gemma aims to add a \"new dimension\" by leveraging its ability to understand and generate human-like text.\n\nGemma's key applications in chess include:\n\n1.  **Explaining Chess:** It can analyze complex games (like Kasparov vs. Deep Blue) and explain *why* specific moves are significant, detailing strategies, tactical opportunities, and potential dangers in plain language, rather than just technical move sequences or numbers.\n2.  **Storytelling:** Gemma can transform chess game data into engaging narratives, describing the flow of matches, the players involved, and the overall dramatic arc, making the games more accessible and relatable.\n3.  **Supporting Chess Learning:** It acts as a \"super helpful study buddy,\" explaining chess concepts (e.g., Sicilian Defense, passed pawn) in natural language, adapting to the user's skill level (beginner, intermediate, advanced), and even offering explanations in different languages. This provides a personalized, 24/7 \"chess coach.\"\n\nIn summary, Gemma combines the computational power of chess AI with advanced linguistic understanding, offering a more intuitive and human-centric way to learn, analyze, and experience chess.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"Summarize this video.\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri=yt_link)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR7sQVlxy8Yr"
      },
      "source": [
        "But you can also use the link as the source of truth for your request. In this example, you will first ask how Gemma models can help on chess games:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZgNZ2ZCtzLU8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Gemma models, as Large Language Models (LLMs), don't play chess directly like dedicated chess engines (e.g., Stockfish, AlphaZero). They lack the internal game tree search, move validation, and evaluation functions that chess engines possess.\n\nHowever, Gemma's strengths lie in its ability to understand, generate, and process human language, which can be immensely helpful in various aspects of chess learning, analysis, and communication. Think of Gemma as a powerful analytical and educational *companion* rather than an opponent or a direct player.\n\nHere's how Gemma models can help in chess games:\n\n1.  **Learning and Understanding Chess Concepts:**\n    *   **Explaining Rules and Mechanics:** For beginners, Gemma can clearly explain how pieces move, special rules (castling, en passant), and basic objectives.\n    *   **Defining Terminology:** Ask Gemma to explain terms like \"zugzwang,\" \"fork,\" \"skewer,\" \"pawn structure,\" \"tempo,\" \"initiative,\" \"desperado,\" etc.\n    *   **Illustrating Concepts with Examples:** Gemma can describe positions or sequences of moves that demonstrate specific tactics or strategic ideas.\n    *   **Opening and Endgame Theory:** While not an exhaustive database, Gemma can summarize the main ideas, common lines, and strategic goals behind various openings (e.g., \"Explain the ideas behind the Sicilian Defense\" or \"What are the basic principles of king and pawn vs. king endgames?\").\n\n2.  **Post-Game Analysis and Improvement:**\n    *   **Interpreting Engine Analysis:** If you have an engine's output (like a FEN string with an evaluation, or a recommended move sequence), Gemma can help translate that into human-understandable explanations. For example, \"Why does Stockfish recommend this move in this FEN?\" or \"Explain the tactical idea behind this engine line.\"\n    *   **Summarizing Game Logs (PGNs):** You can feed a PGN (Portable Game Notation) to Gemma and ask it to summarize the key moments, critical mistakes, turning points, or strategic themes of the game.\n    *   **Identifying Common Mistakes (with context):** If you describe your typical errors (e.g., \"I often blunder pawns in the middlegame\"), Gemma can offer general advice or common reasons why such mistakes occur, or suggest drills.\n    *   **Explaining Tactical Puzzles:** If you describe a chess puzzle position (FEN or visual description), Gemma can explain the solution, the underlying tactical motif, and why other moves don't work.\n\n3.  **Strategic Planning and Brainstorming:**\n    *   **Developing Game Plans:** Given a specific opening or middlegame position, Gemma can brainstorm potential strategic plans for both sides, considering factors like pawn structures, piece activity, and king safety.\n    *   **Opponent Analysis (with input):** If you provide information about an opponent's past games or playing style, Gemma could help summarize their tendencies or suggest counter-strategies (e.g., \"My opponent often plays the Caro-Kann; what are some aggressive lines to consider against it?\").\n    *   **Generating Ideas:** For a complex position, Gemma can suggest different strategic approaches, even if it can't calculate the precise best move.\n\n4.  **Content Creation and Communication:**\n    *   **Generating Commentary:** Gemma can help write descriptive commentary for a chess game, explaining moves, player intentions, and the flow of the match.\n    *   **Creating Study Material:** It can help generate questions, explanations, or summaries for chess lessons or study guides.\n    *   **Translating Chess Content:** If you have chess articles, videos, or commentary in a foreign language, Gemma can assist with translation.\n\n**Important Limitations to Keep in Mind:**\n\n*   **Gemma is NOT a Chess Engine:** It cannot play chess, calculate moves, validate moves, or perform the deep search required for optimal play. Its \"understanding\" of chess comes from text data, not a game engine's algorithms.\n*   **No Real-time Assistance (Ethical/Practical):** Using Gemma during a live game for move suggestions would be cheating. Its utility is primarily for learning, analysis, and preparation *outside* of live play.\n*   **Relies on Input:** Gemma needs clear and accurate input (FEN, PGN, detailed descriptions) to provide useful chess-related responses. It cannot \"see\" a chessboard.\n*   **Hallucinations/Inaccuracies:** Like any LLM, Gemma can sometimes generate plausible but incorrect information. Always cross-reference critical chess advice with dedicated chess engines or trusted human experts.\n*   **Knowledge Cutoff:** Gemma's training data has a cutoff, meaning it might not be aware of the absolute latest developments in opening theory or recent games.\n\nIn summary, Gemma models are excellent linguistic tools that can demystify complex chess concepts, aid in post-game analysis by interpreting engine output, and assist in strategic planning and content creation. They serve as an intelligent assistant for learning and understanding the game, rather than a player or a direct tactical calculator.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"How Gemma models can help on chess games?\"),\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whgRdJx4ztg0"
      },
      "source": [
        "And then you can ask the same question, now having the YouTube video as context to be used by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UTH4DqBAzx3H"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Gemma models, as large language models (LLMs), can significantly enhance the chess experience by leveraging their natural language understanding and generation capabilities, rather than directly calculating the best moves like traditional chess engines.\n\nHere's how Gemma can help in chess games, based on the video:\n\n1.  **Enhanced Analysis and Explanation (The Explainer):**\n    *   **Translating Complexity:** Traditional chess engines often provide numerical evaluations and complex move sequences (like \"Nf3 d5 2.g3 Bg4\"). Gemma can take this technical output and translate it into plain, human-understandable text.\n    *   **Explaining \"Why\":** Instead of just showing a move, Gemma can explain the *strategic ideas* and *tactical reasons* behind a move. For example, it can explain why a pawn sacrifice is interesting due to disrupting the opponent's plans or its psychological impact.\n    *   **Summarizing Key Moments:** For long or complicated games, Gemma can pick out the most important tactical and strategic moments, helping players quickly grasp the critical turning points.\n\n2.  **Interactive Learning and Coaching (Supporting Chess Learning):**\n    *   **Personalized Explanations:** Gemma can act as a \"study buddy\" or \"personal chess coach.\" You can ask it to explain chess concepts (like the \"Sicilian Defense\" or a \"passed pawn\") and it can tailor the explanation to your skill level (beginner, intermediate, advanced).\n    *   **Multilingual Support:** As demonstrated in the video, Gemma can understand and explain concepts in various languages (e.g., Korean), making chess learning more accessible globally.\n    *   **Targeted Feedback:** It can provide feedback on your understanding of chess ideas and even suggest areas where you might want to improve.\n\n3.  **Storytelling and Narrative Generation (Storytellers):**\n    *   **Bringing Games to Life:** Gemma can take the raw data of a chess game (like PGN notation, including player names and tournament info) and weave it into a compelling narrative or short story.\n    *   **Adding Human Context:** It can describe the atmosphere of a match, infer hypothetical thoughts or emotions of the players, and highlight dramatic turns, making the game more engaging and relatable than just a sequence of moves. This is like getting a \"cool backstory\" for a puzzle, making it more interesting.\n\n4.  **Combining Strengths (Hybrid Approach):**\n    *   **Complementing Engines:** By integrating with traditional chess engines (which excel at calculation), Gemma can take the optimal moves identified by the engine and then *explain* the reasoning behind them in natural language. This blends the raw computational power of chess AI with the human-like understanding and communication of an LLM, offering a more intuitive and insightful analysis experience.\n\nIn essence, Gemma models don't play chess themselves, but they act as an intelligent interpreter and communicator, making chess analysis, learning, and enjoyment more accessible and profound for human players.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yt_link = \"https://www.youtube.com/watch?v=XV1kOFo1C8M\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents= types.Content(\n",
        "        parts=[\n",
        "            types.Part(text=\"How Gemma models can help on chess games?\"),\n",
        "            types.Part(\n",
        "                file_data=types.FileData(file_uri=yt_link)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHhdfKqLz_D6"
      },
      "source": [
        "Now your answer is more insightful for the topic you want, using the knowledge shared on the video and not necessarily available on the model knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKBPhxA-0RiT"
      },
      "source": [
        "## Grounding information using URL context\n",
        "\n",
        "The URL Context tool empowers Gemini models to directly access and process content from specific web page URLs you provide within your API requests. This is incredibly interesting because it allows your applications to dynamically interact with live web information without needing you to manually pre-process and feed that content to the model.\n",
        "\n",
        "URL Context is effective because it allows the models to base its responses and analysis directly on the content of the designated web pages. Instead of relying solely on its general training data or broad web searches (which are also valuable grounding tools), URL Context anchors the model's understanding to the specific information present at those URLs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eOXM1Fh2D9Ai"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Here's a comparison of key differences between Gemini 1.5, Gemini 2.0, and Gemini 2.5 models based on the provided documentation:\n\n| Feature                 | Gemini 1.5 Pro                                                                                                                                                                                                                                               | Gemini 1.5 Flash                                                                      | Gemini 2.0 Flash                                                                                              | Gemini 2.5 Pro (Preview)                                                                                                                                                                                                                                                                                                                             | Gemini 2.5 Flash (Preview)                                                                                                                                                                                                                                                                                                                             |\n| :---------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Optimization/Purpose** | Mid-size multimodal model optimized for a wide range of reasoning tasks, capable of processing large amounts of data (e.g., 2 hours of video, 19 hours of audio, 60,000 lines of code, or 2,000 pages of text).                                         | Fast and versatile performance across a diverse variety of tasks.                 | Newest multimodal model with next-generation features, improved capabilities, low latency, enhanced performance, built to power agentic experiences. | Our most powerful thinking model with maximum response accuracy and state-of-the-art performance, best for complex coding, reasoning, and multimodal understanding, and analyzing large databases.                                                                                                                                                        | Our best model in terms of price-performance, offering well-rounded capabilities, best for low latency, high volume tasks that require thinking.                                                                                                                                                                                                 |\n| **Input Modalities**    | Audio, images, video, and text.                                                                                                                                                                                                                          | Audio, images, video, and text.                                                   | Audio, images, video, and text.                                                   | Audio, images, video, and text.                                                                                                                                                                                                                                                                                                                  | Audio, images, video, and text.                                                                                                                                                                                                                                                                                                                  |\n| **Output Modalities**   | Text.                                                                                                                                                                                                                                                    | Text.                                                                             | Text.                                                                             | Text.                                                                                                                                                                                                                                                                                                                                            | Text.                                                                                                                                                                                                                                                                                                                                            |\n| **Input Token Limit**   | 2,097,152.                                                                                                                                                                                                                                               | 1,048,576.                                                                        | 1,048,576.                                                                        | 1,048,576.                                                                                                                                                                                                                                                                                                                                       | 1,048,576.                                                                                                                                                                                                                                                                                                                                       |\n| **Output Token Limit**  | 8,192.                                                                                                                                                                                                                                                   | 8,192.                                                                            | 8,192.                                                                            | 65,536.                                                                                                                                                                                                                                                                                                                                          | 65,536.                                                                                                                                                                                                                                                                                                                                          |\n| **Key Capabilities**    | Handles complex reasoning tasks, large datasets, and long context.                                                                                                                                                                                       | Fast and versatile performance.                                                   | Next-gen features, speed, thinking, real-time streaming, built for agentic experiences. | Enhanced thinking and reasoning, multimodal understanding, advanced coding.                                                                                                                                                                                                                                                                          | Adaptive thinking, cost efficiency, with well-rounded capabilities.                                                                                                                                                                                                                                                                                    |\n| **Latest Update**       | September 2024.                                                                                                                                                                                                                                          | September 2024.                                                                   | February 2025.                                                                    | May 2025.                                                                                                                                                                                                                                                                                                                                        | May 2025.                                                                                                                                                                                                                                                                                                                                        |\n\n**Summary of Key Differences:**\n\n*   **Generational Advancements**: Gemini 2.0 and 2.5 represent newer generations with \"next-generation features\" and \"improved capabilities\" compared to 1.5.\n*   **Performance and Purpose**:\n    *   **Pro models (1.5 Pro, 2.5 Pro)** are designed for complex reasoning, multimodal understanding, and handling large amounts of data. Gemini 2.5 Pro is presented as the most powerful thinking model with maximum accuracy.\n    *   **Flash models (1.5 Flash, 2.0 Flash, 2.5 Flash)** prioritize speed, cost efficiency, and versatility. Gemini 2.5 Flash offers the best price-performance ratio for low-latency, high-volume tasks.\n*   **Output Token Limit**: A significant difference is the output token limit for Gemini 2.5 models (65,536 tokens) compared to Gemini 1.5 and 2.0 models (8,192 tokens).\n*   **Long Context Window**: Gemini 1.5 Pro stands out with a larger input token limit (2,097,152) than other models listed (1,048,576), indicating its superior capability for processing extensive inputs.\n*   **Experimental/Preview Status**: Gemini 2.5 models are currently in \"Preview\" status, meaning they may have more restrictive rate limits compared to stable models.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "based on https://ai.google.dev/gemini-api/docs/models, what are the key\n",
        "differences between Gemini 1.5, Gemini 2.0 and Gemini 2.5 models?\n",
        "Create a markdown table comparing the differences.\n",
        "\"\"\"\n",
        "\n",
        "tools = []\n",
        "tools.append(types.Tool(url_context=types.UrlContext))\n",
        "\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "      contents=[prompt],\n",
        "      model=MODEL_ID,\n",
        "      config=config\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cf0uIAhEhPH"
      },
      "source": [
        "As a reference, you can see how the answer would be without the URL context, using the official models documentation as reference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ktlFAiFTEnHS"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "It's important to clarify that as of my last update, **Gemini 1.5 Pro** is the current major iteration of Google's flagship multimodal model, and it represents a significant leap from the initial **Gemini 1.0** series (which included Ultra, Pro, and Nano).\n\nThere hasn't been a publicly announced, distinct model called \"Gemini 2.0\" or \"Gemini 2.5\" in the same way that 1.0 and 1.5 Pro were launched. It's possible you might be thinking of:\n*   The **Gemini 1.0 series** as the initial release.\n*   **Gemini 1.5 Pro** as the \"next generation\" (which includes the revolutionary long context window).\n*   **Future, unannounced versions** that would logically follow 1.5, which might eventually be named 2.0 or 2.x. Google often refers to underlying architectural shifts or future capabilities internally, but these don't always translate to immediate, distinct public model names.\n\nTherefore, the key comparison is primarily between the **Gemini 1.0 family** and **Gemini 1.5 Pro**. Any mention of \"2.0\" or \"2.5\" would refer to speculative future models or a misinterpretation of current naming conventions.\n\nHere's a table comparing Gemini 1.0 and Gemini 1.5 Pro, and addressing the \"2.0\" and \"2.5\" points:\n\n---\n\n## Comparison of Gemini Models\n\n| Feature                 | Gemini 1.0 (e.g., Ultra, Pro, Nano)             | Gemini 1.5 Pro                                            | Gemini 2.0 / 2.5 (Future/Speculative)             |\n| :---------------------- | :---------------------------------------------- | :-------------------------------------------------------- | :------------------------------------------------- |\n| **Launch/Announcement** | December 2023                                   | February 2024 (private preview), April 2024 (public preview) | Not publicly announced as distinct models yet.     |\n| **Core Architecture**   | Traditional Transformer-based                  | **Mixture-of-Experts (MoE)** architecture; highly efficient | Likely further advancements in MoE or novel architectures |\n| **Context Window Size** | Up to **32K tokens**                            | Standard: **1 Million tokens**; Experimental: **2 Million tokens** | Expected to be even larger or more efficient in processing. |\n| **Multimodality**       | Native understanding of text, images, audio, video. | Enhanced native understanding across modalities, especially for long-form video/audio. | Deeper, more integrated multimodal reasoning; potentially new modalities. |\n| **Performance**         | State-of-the-art at launch; strong general reasoning. | Surpasses Gemini 1.0 Ultra on many benchmarks, especially in long-context tasks (e.g., summarization, code analysis). | Expected to significantly outperform 1.5 Pro across all metrics. |\n| **Key Innovation**      | First broadly available truly multimodal model from Google. | **Revolutionary long context window**; MoE efficiency for performance and cost. | Unknown, but likely breakthrough in reasoning, agency, or real-world interaction. |\n| **Use Cases**           | General chatbot, content generation, coding, image analysis. | Advanced long-document analysis, video summarization, large codebase understanding, complex problem-solving. | Future applications requiring even greater autonomy, complex reasoning over vast data, and real-time interaction. |\n| **Current Status**      | Generally Available (API, Gemini Advanced/Bard) | Public Preview / General Availability (API, select products) | Not a defined public model; refers to future generations of Gemini models. |\n\n---\n\n**In summary:**\n\n*   **Gemini 1.0** was the groundbreaking initial release of Google's multimodal model family.\n*   **Gemini 1.5 Pro** is the current flagship, distinguished by its **massive context window** (1M+ tokens) and efficient **Mixture-of-Experts (MoE)** architecture, making it exceptionally powerful for complex, long-form tasks.\n*   \"**Gemini 2.0**\" and \"**Gemini 2.5**\" are not currently distinct, publicly launched models. If and when new major versions are released after 1.5 Pro, they would likely feature even more advanced capabilities, but their naming and specific features are yet to be announced by Google.",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "what are the key differences between Gemini 1.5, Gemini 2.0 and Gemini 2.5\n",
        "models? Create a markdown table comparing the differences.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "      contents=[prompt],\n",
        "      model=MODEL_ID,\n",
        ")\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axxHawtkGQO9"
      },
      "source": [
        "As you can see, using the model knowledge only, it does not know about the new Gemini 2.5 models family."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOt32shZaEXj"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "<a name=\"next_steps\"></a>\n",
        "\n",
        "* For more details about using Google Search grounding, check out the [Search Grounding cookbook](./Search_Grounding.ipynb).\n",
        "* If you are looking for another scenarios using videos, take a look at the [Video understanding cookbook](./Video_understanding.ipynb).\n",
        "\n",
        "Also check the other Gemini capabilities that you can find in the [Gemini quickstarts](https://github.com/google-gemini/cookbook/tree/main/quickstarts/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "Grounding.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
