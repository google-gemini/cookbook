{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2025 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "b7ce279c7b07"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "153886bac3ca"
   },
   "source": [
    "# Gemini API: Chatbot Quickstart\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Gemini_Chatbot_Tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/>\n",
    "---\n",
    "\n",
    "This notebook provides an example of how to use the **Gemini API to build a chatbot**.  \n",
    "You will create a conversational AI that generates responses based on user input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f7f47043869"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -U \"google-genai>=1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0656906d23e"
   },
   "source": [
    "## Set up your API key\n",
    "\n",
    "To run the following cell, store your API key in a Colab Secret named `GOOGLE_API_KEY`.  \n",
    "If you don't already have an API key, or you're unsure how to create a Colab Secret, see the  \n",
    "[Authentication](https://github.com/google-gemini/cookbook/blob/a0b506a8f65141cec4eb9143db760c735f652a59/quickstarts/Authentication.ipynb) quickstart for an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "235ce8c11240"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.colab import userdata\n",
    "\n",
    "# Configure your API key using Colab Secrets\n",
    "GOOGLE_API_KEY = userdata.get(\"GOOGLE_API_KEY\")\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "212ba7018c0e"
   },
   "source": [
    "### **Interacting with the Gemini Chatbot**  \n",
    "This guide walks you through building a chatbot using the Gemini API, from basics to advanced features. Each section builds on the previous one, helping you understand how to create a more interactive and intelligent chatbot.  \n",
    "\n",
    "### **Building a Gemini Chatbot**  \n",
    "1️. **Send a Basic Message** – Get a response from Gemini with a simple prompt.  \n",
    "2️. **Multi-Turn Conversations** – Break complex queries into steps for clearer answers.  \n",
    "3️. **Streaming Responses** – Get real-time responses for a more interactive experience.  \n",
    "4️. **Conversation History** – Maintain context by referring to past messages.  \n",
    "5️. **Configuring Model Parameters** – Control response behavior with settings like temperature and max tokens.  \n",
    "6️. **System Instructions** – Customize chatbot behavior by defining specific roles and response styles.\n",
    "\n",
    "You can modify the model version using the `MODEL_ID` variable. This step-by-step guide aims to help developers build and optimize chatbots with Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the Gemini model\n",
    "MODEL_ID = \"gemini-2.0-flash\"  # @param [\"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.0-pro-exp-02-05\"] {\"allow-input\": true, \"isTemplate\": true}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sending a Basic Message**  \n",
    "The following section demonstrates how to send a simple message to the Gemini model and receive a response. [Learn more](<https://ai.google.dev/gemini-api/docs/text-generation#text-input>).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\"How does AI work?\"]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-Turn Conversations**  \n",
    "The following section demonstrates Gemini ability to collect multiple questions and answers in a chat session. This chat format enables users to step incrementally toward answers and to get help with multipart problems\n",
    "[Learn more](<https://ai.google.dev/gemini-api/docs/text-generation?hl=en#multi-turn-conversations>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a chat session\n",
    "chat = client.chats.create(model = MODEL_ID)\n",
    "\n",
    "# Send the first message\n",
    "response = chat.send_message(\"Each orange has 8 slices.\")\n",
    "print(response.text)\n",
    "\n",
    "# Send a follow-up message\n",
    "response = chat.send_message(\"I have 8 oranges. If 4 slices in total are rotten, how many complete oranges can I eat?\")\n",
    "print(response.text)\n",
    "\n",
    "# Send another follow-up message\n",
    "response = chat.send_message(\"I ate 3 oranges. What is the probability that at least one of the slices I ate was rotten?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the conversation history using `chat.get_history()` method and print it in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display conversation history\n",
    "for message in chat.get_history():\n",
    "    print(f'role - {message.role}',end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Streaming Responses**\n",
    "\n",
    "By default, the model generates and returns the complete response only after finishing the entire text generation process. However, streaming allows real-time output using `send_message_stream()`, enabling responses to be processed as they are generated. This enhances responsiveness, making interactions feel faster and more dynamic, especially for longer replies. [Learn more](<https://ai.google.dev/gemini-api/docs/text-generation#streaming-output>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a message with streaming response\n",
    "response = chat.send_message_stream(\"Each orange has 8 slices.\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "\n",
    "# Send a follow-up message with streaming\n",
    "response = chat.send_message_stream(\"I have 8 oranges. If 4 slices in total are rotten, how many complete oranges can I eat?\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "\n",
    "# Send another follow-up message with streaming\n",
    "response = chat.send_message_stream(\"I ate 3 oranges. What is the probability that at least one of the slices I ate was rotten?\")\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\"\")\n",
    "\n",
    "# Display conversation history\n",
    "for message in chat.get_history():\n",
    "    print(f\"Role: {message.role}\", end=\": \")\n",
    "    print(message.parts[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maintaining Conversation History**  \n",
    "\n",
    "In multi-turn conversations, keeping track of previous messages helps to provide relevant and context-aware responses. This section shows how to store past exchanges so users can ask follow-up questions without losing context.\n",
    "[Learn more](<https://ai.google.dev/api/generate-content#method:-models.generatecontent>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous conversation history\n",
    "conversation_history = [\n",
    "    types.Content(role=\"user\", parts=[types.Part(text=\"Hello!\")]),\n",
    "    types.Content(role=\"model\", parts=[types.Part(text=\"Hi there! How can I assist you today?\")]),\n",
    "    types.Content(role=\"user\", parts=[types.Part(text=\"I have two dogs in my house. How many paws are in my house?\")]),\n",
    "    types.Content(role=\"model\", parts=[types.Part(text=\"Each dog has 4 paws, so if you have 2 dogs, that makes 8 paws in your house.\")]),\n",
    "]\n",
    "\n",
    "# Function to interact with Gemini Chatbot\n",
    "def chat_with_gemini(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a response from the Gemini chatbot for a given prompt.\n",
    "    - Maintains multi-turn conversation history\n",
    "    - Streams AI-generated responses in real-time\n",
    "    - Uses proper message formatting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Append user input to conversation history\n",
    "    conversation_history.append(types.Content(role=\"user\", parts=[types.Part(text=prompt)]))\n",
    "\n",
    "    # Create a chat session with history\n",
    "    chat = client.chats.create(model=MODEL_ID, history=conversation_history)\n",
    "\n",
    "    response = chat.send_message_stream(message=prompt)  # Stream model response  \n",
    "    # response = chat.send_message(message=prompt)  # Get full response at once  \n",
    "\n",
    "    full_response = \"\"\n",
    "    print(\"Gemini:\", end=\" \", flush=True)\n",
    "    for chunk in response:\n",
    "        print(chunk.text, end=\"\", flush=True)  # Streaming output\n",
    "        full_response += chunk.text\n",
    "\n",
    "    # Append response to history\n",
    "    conversation_history.append(types.Content(role=\"model\", parts=[types.Part(text=full_response)]))\n",
    "\n",
    "    return full_response\n",
    "\n",
    "# Ask a follow-up question using the history\n",
    "user_input = \"What if I get another dog?\"\n",
    "response = chat_with_gemini(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Configuring Model Parameters**  \n",
    "\n",
    "When sending a request to the Gemini API, various parameters can be adjusted to control how the model generates responses. If not specified, the model uses default values. Below are key parameters that can be configured:  \n",
    "\n",
    "- **`stopSequences`** – Defines sequences that stop response generation. The model stops output when encountering these sequences.  \n",
    "- **`temperature`** – Controls randomness. Higher values make responses more creative, while lower values make them more deterministic (range: 0.0 to 2.0).  \n",
    "- **`maxOutputTokens`** – Limits the number of tokens in the response.  \n",
    "- **`topP`** – Adjusts token selection by probability. A lower value makes responses more focused, while a higher value allows more variety.  \n",
    "- **`topK`** – Limits token selection to the most probable options. A `topK` of 1 chooses the highest-probability token, while higher values allow more diversity.  \n",
    "\n",
    "The example below demonstrates how to configure these parameters. [Learn more](<https://ai.google.dev/gemini-api/docs/text-generation?hl=en#configuration-parameters>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model parameters for response generation\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\"Explain how AI works\"],\n",
    "    config=types.GenerateContentConfig(\n",
    "        max_output_tokens=500,  # Limit response length\n",
    "        temperature=0.1         # Lower value for more precise responses\n",
    "    )\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using System Instructions**  \n",
    "\n",
    "System instructions define the model’s behavior for a specific use case, ensuring consistent responses throughout a conversation. This is especially useful for chatbots, as it helps maintain a specific persona, tone, or role without requiring repeated context in every user message.  \n",
    "\n",
    "The following code configures a chatbot to act as a friendly travel assistant: [Learn more](<https://ai.google.dev/gemini-api/docs/text-generation?hl=en#system-instructions>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the chatbot with system instructions\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a friendly travel assistant. Suggest travel destinations and give recommendations.\"\n",
    "    ),\n",
    "    contents=\"Can you suggest a good beach destination?\"\n",
    ")\n",
    "\n",
    "# Print response\n",
    "print(response.text)\n",
    "\n",
    "# Multiple follow-up questions\n",
    "follow_ups = [\n",
    "    \"What activities can I do there?\",\n",
    "    \"When is the best time to visit?\",\n",
    "    \"Are there any budget-friendly accommodations?\"\n",
    "]\n",
    "\n",
    "# Send them one-by-one\n",
    "for question in follow_ups:\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=question\n",
    "    )\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5530e2f354c"
   },
   "source": [
    "## **Next Steps**  \n",
    "\n",
    "🔹 **[Handling Long Context](https://ai.google.dev/gemini-api/docs/long-context)**  \n",
    "\n",
    "🔹 **[Build an AI Chat Web App](https://ai.google.dev/gemini-api/tutorials/web-app?lang=python)**  \n",
    "\n",
    "🔹 **[Prompt Design Strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=en)**  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Gemini_Chatbot_Tutorial.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
