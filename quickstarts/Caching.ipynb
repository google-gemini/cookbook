{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navG4j6eqc_o"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "0lpR41ozqBFp"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9g4WhqtqiV-"
      },
      "source": [
        "# Gemini API: Context Caching Quickstart\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Caching.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCC-LFNtqogI"
      },
      "source": [
        "This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the Python SDK. For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857d8bf104ed"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5yRLvyPhrXSf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU 'google-genai'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4xdJSvLerazn"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJYxEpOrc3d"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EgO56yWoriI0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-ZorTqrsoz"
      },
      "source": [
        "## Upload a file\n",
        "\n",
        "A common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.\n",
        "\n",
        "This example will be based on the transcript from the Apollo 11 mission.\n",
        "\n",
        "Start by downloading that transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sa-r2s_ltBXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INTRODUCTION\n",
            "\n",
            "This is the transcription of the Technical Air-to-Ground Voice Transmission (GOSS NET 1) from the Apollo 11 mission.\n",
            "\n",
            "Communicators in the text may be identified according to the following list.\n",
            "\n",
            "Spacecraft:\n",
            "CDR\tCommander\tNeil A. Armstrong\n",
            "CMP\tCommand module pilot   \tMichael Collins\n",
            "LMP\tLunar module pilot\tEdwin E. ALdrin, Jr.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "!head a11.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014697858670"
      },
      "source": [
        "Now upload the transcript using the [File API](../quickstarts/File_API.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b3bU9AvcvZ_x"
      },
      "outputs": [],
      "source": [
        "document = client.files.upload(file=\"a11.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4LP7unAvmce"
      },
      "source": [
        "## Cache the prompt\n",
        "\n",
        "Next create a [`CachedContent`](https://ai.google.dev/api/python/google/generativeai/protos/CachedContent) object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the [`system_instruction`](../quickstarts/System_instructions.ipynb) has been set, and the document was provided in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55V-QkaWv4tb"
      },
      "outputs": [
        {
          "ename": "ClientError",
          "evalue": "404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for createCachedContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Note that caching requires a frozen model, e.g. one with a `-001` suffix.\u001b[39;00m\n\u001b[32m      2\u001b[39m MODEL_ID=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash-preview-04-17\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# @param [\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-preview-03-25\", \"gemini-2.0-flash-001\", \"gemini-1.5-flash-002\",\"gemini-1.5-pro-002\",\"gemini-1.5-flash-8b-latest\"] {\"allow-input\":true, isTemplate: true}\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m apollo_cache = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontents\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msystem_instruction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYou are an expert at analyzing transcripts.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m apollo_cache\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/prototypes/io-activities/io-env/lib/python3.12/site-packages/google/genai/caches.py:1238\u001b[39m, in \u001b[36mCaches.create\u001b[39m\u001b[34m(self, model, config)\u001b[39m\n\u001b[32m   1235\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   1236\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   1243\u001b[39m   response_dict = _CachedContent_from_vertex(\n\u001b[32m   1244\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   1245\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/prototypes/io-activities/io-env/lib/python3.12/site-packages/google/genai/_api_client.py:742\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    734\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    737\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    738\u001b[39m ):\n\u001b[32m    739\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    740\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    741\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    743\u001b[39m   json_response = response.json\n\u001b[32m    744\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/prototypes/io-activities/io-env/lib/python3.12/site-packages/google/genai/_api_client.py:671\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    664\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    665\u001b[39m       method=http_request.method,\n\u001b[32m    666\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    670\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    673\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    674\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/prototypes/io-activities/io-env/lib/python3.12/site-packages/google/genai/errors.py:101\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m     99\u001b[39m status_code = response.status_code\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    103\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
            "\u001b[31mClientError\u001b[39m: 404 NOT_FOUND. {'error': {'code': 404, 'message': 'models/gemini-2.5-flash-preview-04-17 is not found for API version v1beta, or is not supported for createCachedContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}"
          ]
        }
      ],
      "source": [
        "# Note that caching requires a frozen model, e.g. one with a `-001` suffix.\n",
        "MODEL_ID=\"gemini-2.5-flash-preview-04-17\"  # @param [\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-preview-03-25\", \"gemini-2.0-flash-001\", \"gemini-1.5-flash-002\",\"gemini-1.5-pro-002\",\"gemini-1.5-flash-8b-latest\"] {\"allow-input\":true, isTemplate: true}\n",
        "\n",
        "apollo_cache = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        'contents': [document],\n",
        "        'system_instruction': 'You are an expert at analyzing transcripts.',\n",
        "    },\n",
        ")\n",
        "\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "50kG3mJn50WM"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As you can see in the output, you just cached **322698** tokens."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(f\"As you can see in the output, you just cached **{apollo_cache.usage_metadata.total_token_count}** tokens.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5yzay5xyPC"
      },
      "source": [
        "## Manage the cache expiry\n",
        "\n",
        "Once you have a `CachedContent` object, you can update the expiry time to keep it alive while you need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "cUJT2ESUyTGb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CachedContent(name='cachedContents/tlpl2lj2x3og', display_name='', model='models/gemini-2.0-flash-001', create_time=datetime.datetime(2025, 4, 16, 11, 29, 36, 498830, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 4, 16, 11, 29, 36, 683945, tzinfo=TzInfo(UTC)), expire_time=datetime.datetime(2025, 4, 16, 13, 29, 36, 664594, tzinfo=TzInfo(UTC)), usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=322698, video_duration_seconds=None))"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.caches.update(\n",
        "    name=apollo_cache.name,\n",
        "    config=types.UpdateCachedContentConfig(ttl=\"7200s\")  # 2 hours in seconds\n",
        ")\n",
        "apollo_cache = client.caches.get(name=apollo_cache.name) # Get the updated cache\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_PWabuayrf-"
      },
      "source": [
        "## Use the cache for generation\n",
        "\n",
        "As the `CachedContent` object refers to a specific model and parameters, you must create a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) using [`from_cached_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#from_cached_content). Then, generate content as you would with a directly instantiated model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EG8VNpuIzGwT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are several lighthearted moments in this transcript, often involving joking around or lighthearted banter between the crew and mission control. Here's one example:\n",
            "\n",
            "(GOSS NET 1) Tape 4/6 Page 30\n",
            "\n",
            "00 05 20 31 CMP\n",
            "If we're late in answering you, it's because we're munching sandwiches.\n",
            "\n",
            "00 05 20 36 CC\n",
            "Roger. I wish I could do the same here.\n",
            "\n",
            "00 05 20 40 CMP\n",
            "No. Don't leave the console!\n",
            "\n",
            "00 05 20 42 CC\n",
            "Don't worry. I won't.\n",
            "\n",
            "00 05 20 47 CMP\n",
            "FLIGHT doesn't like it.\n",
            "\n",
            "00 05 20 54 CMP\n",
            "How is FLIGHT today?\n",
            "\n",
            "00 05 20 58 CC\n",
            "Oh, he's doing quite well.\n",
            "\n",
            "This exchange is light and relatable. It shows the astronauts are enjoying a simple pleasure (sandwiches) and playfully acknowledges the constraints of Mission Control personnel. It also implies the strictness of Flight Director and ends with a good natured inquiry about his well-being. It's a brief, humanizing moment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='Find a lighthearted moment from this transcript',\n",
        "    config=types.GenerateContentConfig(\n",
        "        cached_content=apollo_cache.name,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzixLQhC3AO2"
      },
      "source": [
        "You can inspect token usage through `usage_metadata`. Note that the cached prompt tokens are included in `prompt_token_count`, but excluded from the `total_token_count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "MLFd8DFZ29lC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322698)], cached_content_token_count=322698, candidates_token_count=283, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=283)], prompt_token_count=322706, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322706)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=322989, traffic_type=None)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "aXeyCrV56pfk"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "  As you can see in the `usage_metadata`, the token usage is split between:\n",
              "  *  322698 tokens for the cache,\n",
              "  *  322706 tokens for the input (including the cache, so 8 for the actual prompt),\n",
              "  *  283 tokens for the output,\n",
              "  *  322989 tokens in total.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(f\"\"\"\n",
        "  As you can see in the `usage_metadata`, the token usage is split between:\n",
        "  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,\n",
        "  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),\n",
        "  *  {response.usage_metadata.candidates_token_count} tokens for the output,\n",
        "  *  {response.usage_metadata.total_token_count} tokens in total.\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-24t14t302N"
      },
      "source": [
        "You can ask new questions of the model, and the cache is reused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pZngmGj13k9O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"THAT'S ONE SMALL STEP FOR (A) MAN, ONE GIANT LEAP FOR MANKIND.\" - CDR (Neil Armstrong) at 04 06 45 48 on Tape 66/11, marking the moment of first human contact with the lunar surface.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "chat = client.chats.create(\n",
        "  model = MODEL_ID,\n",
        "  config = {\"cached_content\": apollo_cache.name}\n",
        ")\n",
        "\n",
        "response = chat.send_message(message=\"Give me a quote from the most important part of the transcript.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GhGTutW65u7h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Following Neil Armstrong's famous first words on the Moon, the transcript continues with Buzz Aldrin's description of the landing, noting it was a very smooth touchdown. This is followed by confirmation from Houston (CC) that the Eagle had landed at Tranquility Base and a general sense of excitement and relief from Mission Control. The transcript then proceeds with standard post-landing procedures and observations.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\n",
        "    message=\"What was recounted after that?\",\n",
        "    config = {\"cached_content\": apollo_cache.name}\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SB5Ywx2D6cOn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322698)], cached_content_token_count=322698, candidates_token_count=80, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=80)], prompt_token_count=322780, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322780)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=322860, traffic_type=None)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "sMEzwz6eW-gp"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "  As you can see in the `usage_metadata`, the token usage is split between:\n",
              "  *  322698 tokens for the cache,\n",
              "  *  322780 tokens for the input (including the cache, so 82 for the actual prompt),\n",
              "  *  80 tokens for the output,\n",
              "  *  322860 tokens in total.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(f\"\"\"\n",
        "  As you can see in the `usage_metadata`, the token usage is split between:\n",
        "  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,\n",
        "  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),\n",
        "  *  {response.usage_metadata.candidates_token_count} tokens for the output,\n",
        "  *  {response.usage_metadata.total_token_count} tokens in total.\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7acf6cd63d2d"
      },
      "source": [
        "Since the cached tokens are cheaper than the normal ones, it means this prompt was much cheaper that if you had not used caching. Check the [pricing here](https://ai.google.dev/pricing) for the up-to-date discount on cached tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeAxehx79ng"
      },
      "source": [
        "## Delete the cache\n",
        "\n",
        "The cache has a small recurring storage cost (cf. [pricing](https://ai.google.dev/pricing)) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using `\"ttl\"`) of 2h.\n",
        "\n",
        "Still, if you don't need you cache anymore, it is good practice to delete it proactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "HdP83dj08Nb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cachedContents/tlpl2lj2x3og\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeleteCachedContentResponse()"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(apollo_cache.name)\n",
        "client.caches.delete(name=apollo_cache.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7396e8bfdcf8"
      },
      "source": [
        "## Next Steps\n",
        "### Useful API references:\n",
        "\n",
        "If you want to know more about the caching API, you can check the full [API specifications](https://ai.google.dev/api/rest/v1beta/cachedContents) and the [caching documentation](https://ai.google.dev/gemini-api/docs/caching).\n",
        "\n",
        "### Continue your discovery of the Gemini API\n",
        "\n",
        "Check the File API notebook to know more about that API. The [vision capabilities](../quickstarts/Video.ipynb) of the Gemini API are a good reason to use the File API and the caching.\n",
        "The Gemini API also has configurable [safety settings](../quickstarts/Safety.ipynb) that you might have to customize when dealing with big files.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Caching.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
