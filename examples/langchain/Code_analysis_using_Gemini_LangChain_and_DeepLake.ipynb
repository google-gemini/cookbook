{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashitpatel001/cookbook/blob/fix%2Flangchain-deeplake-update/examples/langchain/Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLHiTPXNTf2a"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oTuT5CsaTigz"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNM-D0pLXZeR"
      },
      "source": [
        "# Gemini API: Code analysis using LangChain and DeepLake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRZo8H09Bs6u"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/langchain/Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf4f2c17530"
      },
      "source": [
        "<!-- Princing warning Badge -->\n",
        "<table>\n",
        "  <tr>\n",
        "    <!-- Emoji -->\n",
        "    <td bgcolor=\"#f5949e\">\n",
        "      <font size=30>⚠️</font>\n",
        "    </td>\n",
        "    <!-- Text Content Cell -->\n",
        "    <td bgcolor=\"#f5949e\">\n",
        "      <h3><font color=black>This notebook requires paid tier rate limits to run properly.<br>  \n",
        "(cf. <a href=\"https://ai.google.dev/pricing#veo2\"><font color='#217bfe'>pricing</font></a> for more details).</font></h3>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOGNjAZMwMIk"
      },
      "source": [
        "This notebook shows how to use Gemini API with [Langchain](https://python.langchain.com/v0.2/docs/introduction/) and [DeepLake](https://www.deeplake.ai/) for code analysis. The notebook will teach you:\n",
        "- loading and splitting files\n",
        "- creating a Deeplake database with embedding information\n",
        "- setting up Modern LCEL chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlzRUaWguYiE"
      },
      "source": [
        "### Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BiMHjZuRkQM"
      },
      "outputs": [],
      "source": [
        "#Required Installations\n",
        "%pip install -q -U langchain-google-genai langchain-deeplake langchain langchain-text-splitters langchain-community deeplake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAsv4ybKOiUK"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Loaders & Splitters\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
        "\n",
        "# Google Gemini\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# Core Components (Modern LCEL)\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 4. DeepLake\n",
        "from langchain_community.vectorstores import deeplake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQOGMejVu-6D"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GEMINI_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../../quickstarts/Authentication.ipynb) for an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysayz8skEfBW"
      },
      "outputs": [],
      "source": [
        "# Try except block for safe and secure key usage.\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "try:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"API Key loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(\"Error loading API Key. Please check your Secrets tab.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUwX1PxWg31O"
      },
      "source": [
        "## Prepare the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ye873pizjeR"
      },
      "source": [
        "First, download a [langchain-google](https://github.com/langchain-ai/langchain-google) repository. It is the repository you will analyze in this example.\n",
        "\n",
        "It contains code integrating Gemini API, VertexAI, and other Google products with langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa5Om2YJZMs1"
      },
      "outputs": [],
      "source": [
        "# Knowledge Base\n",
        "!git clone https://github.com/langchain-ai/langchain-google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4M2xVb9zlbp"
      },
      "source": [
        "This example will focus only on the integration of Gemini API with langchain and ignore the rest of the codebase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKXFtWDSz87m"
      },
      "outputs": [],
      "source": [
        "#Find patterns to match with the cloned repo\n",
        "repo_match = \"langchain-google/libs/genai/langchain_google_genai**/*.py\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8ecqCn8z8l5"
      },
      "source": [
        "Each file with a matching path will be loaded and split by `RecursiveCharacterTextSplitter`.\n",
        "In this example, it is specified, that the files are written in Python. It helps split the files without having documents that lack context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYU4TJmXZrHF"
      },
      "outputs": [],
      "source": [
        "#Load Documents\n",
        "docs = []\n",
        "for file in glob(repo_match, recursive=True):\n",
        "  loader = TextLoader(file, encoding='utf-8')\n",
        "  splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=100)\n",
        "  docs.extend(loader.load_and_split(splitter))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nBTLdVD34Mg"
      },
      "source": [
        "`Language` Enum provides common separators used in most popular programming languages, it lowers the chances of classes or functions being split in the middle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtp7Vg0835LR"
      },
      "outputs": [],
      "source": [
        "# common seperators used for Python files\n",
        "RecursiveCharacterTextSplitter.get_separators_for_language(Language.PYTHON)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80xuFyotsaZs"
      },
      "source": [
        "## Create the database\n",
        "The data will be loaded into the memory since the database doesn't need to be permanent in this case and is small enough to fit.\n",
        "\n",
        "The type of storage used is specified by prefix in the path, in this case by `mem://`.\n",
        "\n",
        "Check out other types of storage [here](https://docs.activeloop.ai/setup/storage-and-creds/storage-options)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKBZTFWUuYLA"
      },
      "outputs": [],
      "source": [
        "# define path to database\n",
        "dataset_path = 'mem://deeplake/langchain_google'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ak3qzDgRuY4S"
      },
      "outputs": [],
      "source": [
        "# define the embedding model\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deeplake version [4.0.0]\n",
        "%pip  -q install \"deeplake<4.0.0\""
      ],
      "metadata": {
        "id": "qoQP1f0sglqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Cr4TkMualL"
      },
      "source": [
        "Everything needed is ready, and now you can create the database. It should not take longer than a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XCQBbW0g7M0"
      },
      "outputs": [],
      "source": [
        "docs = docs[:8]\n",
        "#Store the docs inside the Database(deeplake)\n",
        "db = deeplake.DeepLake.from_documents(\n",
        "    dataset_path=dataset_path,\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    read_only=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R594TesMl6Pl"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4Ih7Hveuzkj"
      },
      "source": [
        "Set-up the document retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtEUchpAmBFQ"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['k'] = 4 # number of documents to return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "Z17I0fqHkMzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvF5W6OvvAXB"
      },
      "outputs": [],
      "source": [
        "#The\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# define the chat model\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-3-flash-preview\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqM2_zAvwpgJ"
      },
      "source": [
        "Now, you can create a chain for Question Answering. In this case, `RetrievalQA` chain will be used.\n",
        "\n",
        "If you want to use the chat option instead, use `ConversationalRetrievalChain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G__78Wv1nsNd"
      },
      "outputs": [],
      "source": [
        "# LCEL CHAIN\n",
        "# 1.) Retrieve docs -> Format them to string\n",
        "# 2.) Pass question through\n",
        "# 3.) Combine in Prompt -> LLM -> Output Parser\n",
        "final_chain = {\"context\" : retriever | format_docs , \"question\"  : RunnablePassthrough()} | prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SRcc8LGx6ki"
      },
      "source": [
        "The chain is ready to answer your questions.\n",
        "\n",
        "NOTE: `Markdown` is used for improved formatting of the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4ttIFvmn392"
      },
      "outputs": [],
      "source": [
        "query = \"what classes are available in Google-Gen-AI Library\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VEO6-TwE0MN"
      },
      "outputs": [],
      "source": [
        "main_chain = final_chain.invoke(query)\n",
        "display(Markdown(main_chain))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBtm3YM-7vxD"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Gemini API works great with Langchain. The integration is seamless and provides an easy interface for:\n",
        "- loading and splitting files\n",
        "- creating DeepLake database with embeddings\n",
        "- answering questions based on context from files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwxEpyvx1jbU"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "This notebook showed only one possible use case for langchain with Gemini API. You can find many more [here](../../examples/langchain)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Code_analysis_using_Gemini_LangChain_and_DeepLake.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}