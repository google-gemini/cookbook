{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navG4j6eqc_o"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "0lpR41ozqBFp"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9g4WhqtqiV-"
      },
      "source": [
        "# Gemini API: Context Caching Quickstart\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Caching.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCC-LFNtqogI"
      },
      "source": [
        "This notebook introduces context caching with the Gemini API and provides examples of interacting with the Apollo 11 transcript using the Python SDK. For a more comprehensive look, check out [the caching guide](https://ai.google.dev/gemini-api/docs/caching?lang=python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "857d8bf104ed"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5yRLvyPhrXSf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU 'google-genai'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "4xdJSvLerazn"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJYxEpOrc3d"
      },
      "source": [
        "### Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see [Authentication](../quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "EgO56yWoriI0"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T-ZorTqrsoz"
      },
      "source": [
        "## Upload a file\n",
        "\n",
        "A common pattern with the Gemini API is to ask a number of questions of the same document. Context caching is designed to assist with this case, and can be more efficient by avoiding the need to pass the same tokens through the model for each new request.\n",
        "\n",
        "This example will be based on the transcript from the Apollo 11 mission.\n",
        "\n",
        "Start by downloading that transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sa-r2s_ltBXy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INTRODUCTION\n",
            "\n",
            "This is the transcription of the Technical Air-to-Ground Voice Transmission (GOSS NET 1) from the Apollo 11 mission.\n",
            "\n",
            "Communicators in the text may be identified according to the following list.\n",
            "\n",
            "Spacecraft:\n",
            "CDR\tCommander\tNeil A. Armstrong\n",
            "CMP\tCommand module pilot   \tMichael Collins\n",
            "LMP\tLunar module pilot\tEdwin E. ALdrin, Jr.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "!head a11.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "014697858670"
      },
      "source": [
        "Now upload the transcript using the [File API](../quickstarts/File_API.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b3bU9AvcvZ_x"
      },
      "outputs": [],
      "source": [
        "document = client.files.upload(file=\"a11.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4LP7unAvmce"
      },
      "source": [
        "## Cache the prompt\n",
        "\n",
        "Next create a [`CachedContent`](https://ai.google.dev/api/python/google/generativeai/protos/CachedContent) object specifying the prompt you want to use, including the file and other fields you wish to cache. In this example the [`system_instruction`](../quickstarts/System_instructions.ipynb) has been set, and the document was provided in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55V-QkaWv4tb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CachedContent(name='cachedContents/oxk2s92138kq', display_name='', model='models/gemini-2.5-flash-preview-04-17', create_time=datetime.datetime(2025, 4, 17, 19, 42, 28, 60972, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 4, 17, 19, 42, 28, 60972, tzinfo=TzInfo(UTC)), expire_time=datetime.datetime(2025, 4, 17, 20, 42, 25, 905872, tzinfo=TzInfo(UTC)), usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=322698, video_duration_seconds=None))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Note that caching requires a frozen model, e.g. one with a `-001` suffix.\n",
        "MODEL_ID=\"gemini-2.5-flash-preview-04-17\"  # @param [\"gemini-2.5-flash-preview-04-17\", \"gemini-2.5-pro-preview-03-25\", \"gemini-2.0-flash-001\", \"gemini-1.5-flash-002\",\"gemini-1.5-pro-002\",\"gemini-1.5-flash-8b-latest\"] {\"allow-input\":true, isTemplate: true}\n",
        "\n",
        "apollo_cache = client.caches.create(\n",
        "    model=MODEL_ID,\n",
        "    config={\n",
        "        'contents': [document],\n",
        "        'system_instruction': 'You are an expert at analyzing transcripts.',\n",
        "    },\n",
        ")\n",
        "\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "50kG3mJn50WM"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "As you can see in the output, you just cached **322698** tokens."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(f\"As you can see in the output, you just cached **{apollo_cache.usage_metadata.total_token_count}** tokens.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j5yzay5xyPC"
      },
      "source": [
        "## Manage the cache expiry\n",
        "\n",
        "Once you have a `CachedContent` object, you can update the expiry time to keep it alive while you need it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "cUJT2ESUyTGb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CachedContent(name='cachedContents/oxk2s92138kq', display_name='', model='models/gemini-2.5-flash-preview-04-17', create_time=datetime.datetime(2025, 4, 17, 19, 42, 28, 60972, tzinfo=TzInfo(UTC)), update_time=datetime.datetime(2025, 4, 17, 19, 42, 35, 967873, tzinfo=TzInfo(UTC)), expire_time=datetime.datetime(2025, 4, 17, 21, 42, 35, 959057, tzinfo=TzInfo(UTC)), usage_metadata=CachedContentUsageMetadata(audio_duration_seconds=None, image_count=None, text_count=None, total_token_count=322698, video_duration_seconds=None))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.caches.update(\n",
        "    name=apollo_cache.name,\n",
        "    config=types.UpdateCachedContentConfig(ttl=\"7200s\")  # 2 hours in seconds\n",
        ")\n",
        "apollo_cache = client.caches.get(name=apollo_cache.name) # Get the updated cache\n",
        "apollo_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_PWabuayrf-"
      },
      "source": [
        "## Use the cache for generation\n",
        "\n",
        "As the `CachedContent` object refers to a specific model and parameters, you must create a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) using [`from_cached_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#from_cached_content). Then, generate content as you would with a directly instantiated model object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EG8VNpuIzGwT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, here are a few lighthearted moments from the transcript, ranging from quick jokes to humorous exchanges:\n",
            "\n",
            "1.  **Cecil B. deAldrin:**\n",
            "    *   **Time:** 00 01 29 27\n",
            "    *   **Speaker:** LMP (Buzz Aldrin)\n",
            "    *   **Quote:** \"Cecil B. deAldrin is standing by for instructions.\"\n",
            "    *   **Why it's lighthearted:** Aldrin humorously refers to himself as a movie director (Cecil B. DeMille) while preparing for the first in-flight television broadcast, a lighthearted nod to the media aspect of the mission.\n",
            "\n",
            "2.  **The Unknown Tool:**\n",
            "    *   **Time:** 02 08 07 16 - 02 08 08 10\n",
            "    *   **Speakers:** LMP (Buzz Aldrin), CC (CAPCOM Charlie Duke)\n",
            "    *   **Quote:**\n",
            "        *   LMP: \"Houston, we're showing you something that came out of the Commander's checklist stowage packet. It's got a 16-millimeter camera in it, and it's got this little cylinder; and I guess - I don't understand what it is. Maybe you can tell us.\"\n",
            "        *   CC: \"Roger. Stand by. We can't figure it out either.\"\n",
            "        *   LMP: \"It's got an arrow on the back, and it says 'turn,' but I'm afraid to turn it.\"\n",
            "        *   CC: \"11, your friendly geologist says it's the camera cank- crank, excuse me, for the 16-sequence camera if it jams. Over.\"\n",
            "        *   LMP: \"All very well. Thank you.\"\n",
            "    *   **Why it's lighthearted:** The crew finds an unidentified tool and expresses humorous caution (\"I'm afraid to turn it\"), leading to a brief moment where Mission Control also has to figure out what it is before identifying the mundane camera crank.\n",
            "\n",
            "3.  **Reading Decals on TV:**\n",
            "    *   **Time:** 02 07 30 45 - 02 07 31 54\n",
            "    *   **Speakers:** CC (Charlie Duke), LMP (Buzz Aldrin)\n",
            "    *   **Quote:**\n",
            "        *   CC: \"11, Houston. We can even read the decals up there on the LM hatch.\"\n",
            "        *   LMP: \"Well, let me zoom it up and see how much you can read.\"\n",
            "        *   CC: \"Okay.\" (Later, CC reads the decal description) \"To reset, unlatch handle; latch behind grip and pull back two full strokes.\" That's about all we can make out.\"\n",
            "        *   LMP: \"Hey, you get an A-plus.\"\n",
            "        *   CC: \"Thank you very much, sir. At least I passed my eye test.\"\n",
            "        *   LMP: \"I'm standing 6 feet from it, Charlie, and you can read it better than I can. There's something wrong with the system.\"\n",
            "    *   **Why it's lighthearted:** This exchange highlights the surprising clarity of the TV picture and the crew's amusement at Mission Control's ability to read small text better than they can in the spacecraft.\n",
            "\n",
            "4.  **Struggling with the Leveling Ball:**\n",
            "    *   **Time:** 04 15 02 08 - 04 15 05 19 (Selected quotes from this longer exchange)\n",
            "    *   **Speakers:** LMP (Buzz Aldrin), CDR (Neil Armstrong), CC (CAPCOM)\n",
            "    *   **Quote:**\n",
            "        *   LMP: \"I hope you're watching how hard I have to hit this into the ground, to the tune of about 5 inches, Houston.\"\n",
            "        *   ... (later, trying to level the PSE experiment)\n",
            "        *   LMP: \"I'm not having too much success in leveling the PSE experiment.\"\n",
            "        *   ...\n",
            "        *   CDR: \"That little cup is convex now, instead of concave.\"\n",
            "        *   LMP: \"I think you're right. Houston, I don't think there's any hope for using this leveling device to come up with an accurate level. It looks to me as though the cup here that the B B is in is now convex instead of concave. Over.\"\n",
            "        *   CC: \"Roger, 11. Press on. If you think it looks level by eyeball, go ahead.\"\n",
            "    *   **Why it's lighthearted:** The crew's frustration and discovery that the leveling device is physically bent (\"convex instead of concave\") adds a touch of relatable, slightly absurd humor to the serious task of deploying scientific equipment. Mission Control's pragmatic response (\"If you think it looks level by eyeball, go ahead\") also adds to the moment.\n",
            "\n",
            "5.  **Joking about Being Over Timeline and Getting Trash Out:**\n",
            "    *   **Time:** 04 18 00 02 - 04 18 00 36\n",
            "    *   **Speakers:** CC (Charlie Duke), CDR (Neil Armstrong)\n",
            "    *   **Quote:**\n",
            "        *   CC: \"Roger. Just want to let you guys know that since you're an hour and a half over your timeline and we're all taking a day off tomorrow, we're going to leave you. See you later.\"\n",
            "        *   CDR (TRANQ): \"I don't blame you a bit.\"\n",
            "        *   CC: \"That's a real great day, guys. I really enjoyed it.\"\n",
            "        *   CDR (TRANQ): \"Thank you. You couldn't have enjoyed it as much as we did.\"\n",
            "        *   CC: \"Roger.\"\n",
            "        *   LMP (TRANQ): \"It was great.\"\n",
            "        *   CC: \"Sure wish you'd hurry up and get that trash out out of there, though.\"\n",
            "        *   CDR (TRANQ): \"Yes. We're just about to do it.\"\n",
            "        *   CC: \"Okay.\"\n",
            "    *   **Why it's lighthearted:** CAPCOM jokes about leaving the crew because they are \"over timeline\" and mentions wanting them to hurry with \"that trash,\" contrasting the monumental task with mundane housekeeping. The crew's relaxed response (\"I don't blame you a bit,\" \"We're just about to do it\") shows good humor and camaraderie.\n",
            "\n",
            "These examples provide glimpses of the crew's and Mission Control's ability to inject humor and personality into their high-pressure environment.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents='Find a lighthearted moment from this transcript',\n",
        "    config=types.GenerateContentConfig(\n",
        "        cached_content=apollo_cache.name,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzixLQhC3AO2"
      },
      "source": [
        "You can inspect token usage through `usage_metadata`. Note that the cached prompt tokens are included in `prompt_token_count`, but excluded from the `total_token_count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "MLFd8DFZ29lC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322698)], cached_content_token_count=322698, candidates_token_count=13671, candidates_tokens_details=None, prompt_token_count=322707, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322707)], thoughts_token_count=12219, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=336378, traffic_type=None)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "aXeyCrV56pfk"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "  As you can see in the `usage_metadata`, the token usage is split between:\n",
              "  *  322698 tokens for the cache,\n",
              "  *  322707 tokens for the input (including the cache, so 9 for the actual prompt),\n",
              "  *  13671 tokens for the output,\n",
              "  *  336378 tokens in total.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(f\"\"\"\n",
        "  As you can see in the `usage_metadata`, the token usage is split between:\n",
        "  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,\n",
        "  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),\n",
        "  *  {response.usage_metadata.candidates_token_count} tokens for the output,\n",
        "  *  {response.usage_metadata.total_token_count} tokens in total.\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-24t14t302N"
      },
      "source": [
        "You can ask new questions of the model, and the cache is reused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "pZngmGj13k9O"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the transcript, the most important part is arguably the moment Neil Armstrong steps onto the lunar surface.\n",
            "\n",
            "Here is the quote from that moment:\n",
            "\n",
            "04 13 24 48 **CDR (TRANQ)**\n",
            "THAT'S ONE SMALL STEP FOR (A) MAN, ONE GIANT LEAP FOR MANKIND.\n"
          ]
        }
      ],
      "source": [
        "chat = client.chats.create(\n",
        "  model = MODEL_ID,\n",
        "  config = {\"cached_content\": apollo_cache.name}\n",
        ")\n",
        "\n",
        "response = chat.send_message(message=\"Give me a quote from the most important part of the transcript.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "GhGTutW65u7h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Immediately after the iconic quote, Neil Armstrong recounted his first impressions of the lunar surface. He described:\n",
            "\n",
            "1.  **The surface texture:** It was \"fine and powdery,\" could be picked up \"loosely with my toe,\" and adhered to his boots \"in fine layers like powdered charcoal.\"\n",
            "2.  **Depth of penetration:** He sank in only \"a small fraction of an inch, maybe an eighth of an inch,\" but could clearly see his footprints and the treads.\n",
            "3.  **Ease of movement:** There seemed to be \"no difficulty in moving around,\" feeling \"perhaps easier than the simulations at one sixth g\" performed on the ground, stating it was \"actually no trouble to walk around.\"\n",
            "4.  **The descent engine's impact:** The engine \"did not leave a crater of any size,\" having \"about 1 foot clearance on the ground,\" noting \"some evidence of rays emanating from the descent engine, but a very insignificant amount.\"\n",
            "\n",
            "Houston then confirmed they were copying his observations.\n"
          ]
        }
      ],
      "source": [
        "response = chat.send_message(\n",
        "    message=\"What was recounted after that?\",\n",
        "    config = {\"cached_content\": apollo_cache.name}\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SB5Ywx2D6cOn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GenerateContentResponseUsageMetadata(cache_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322698)], cached_content_token_count=322698, candidates_token_count=902, candidates_tokens_details=None, prompt_token_count=322791, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=322791)], thoughts_token_count=693, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=323693, traffic_type=None)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "sMEzwz6eW-gp"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "  As you can see in the `usage_metadata`, the token usage is split between:\n",
              "  *  322698 tokens for the cache,\n",
              "  *  322791 tokens for the input (including the cache, so 93 for the actual prompt),\n",
              "  *  902 tokens for the output,\n",
              "  *  323693 tokens in total.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(f\"\"\"\n",
        "  As you can see in the `usage_metadata`, the token usage is split between:\n",
        "  *  {response.usage_metadata.cached_content_token_count} tokens for the cache,\n",
        "  *  {response.usage_metadata.prompt_token_count} tokens for the input (including the cache, so {response.usage_metadata.prompt_token_count - response.usage_metadata.cached_content_token_count} for the actual prompt),\n",
        "  *  {response.usage_metadata.candidates_token_count} tokens for the output,\n",
        "  *  {response.usage_metadata.total_token_count} tokens in total.\n",
        "\"\"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7acf6cd63d2d"
      },
      "source": [
        "Since the cached tokens are cheaper than the normal ones, it means this prompt was much cheaper that if you had not used caching. Check the [pricing here](https://ai.google.dev/pricing) for the up-to-date discount on cached tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeAxehx79ng"
      },
      "source": [
        "## Delete the cache\n",
        "\n",
        "The cache has a small recurring storage cost (cf. [pricing](https://ai.google.dev/pricing)) so by default it is only saved for an hour. In this case you even set it up for a shorter amont of time (using `\"ttl\"`) of 2h.\n",
        "\n",
        "Still, if you don't need you cache anymore, it is good practice to delete it proactively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "HdP83dj08Nb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cachedContents/oxk2s92138kq\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeleteCachedContentResponse()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(apollo_cache.name)\n",
        "client.caches.delete(name=apollo_cache.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7396e8bfdcf8"
      },
      "source": [
        "## Next Steps\n",
        "### Useful API references:\n",
        "\n",
        "If you want to know more about the caching API, you can check the full [API specifications](https://ai.google.dev/api/rest/v1beta/cachedContents) and the [caching documentation](https://ai.google.dev/gemini-api/docs/caching).\n",
        "\n",
        "### Continue your discovery of the Gemini API\n",
        "\n",
        "Check the File API notebook to know more about that API. The [vision capabilities](../quickstarts/Video.ipynb) of the Gemini API are a good reason to use the File API and the caching.\n",
        "The Gemini API also has configurable [safety settings](../quickstarts/Safety.ipynb) that you might have to customize when dealing with big files.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Caching.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
