{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Gemini API: Batching and Chunking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/batching_and_chunking.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyc3-gtJVEZo"
      },
      "source": [
        "<!-- Community Contributor Badge -->\n",
        "<table>\n",
        "  <tr>\n",
        "    <!-- Author Avatar Cell -->\n",
        "    <td bgcolor=\"#d7e6ff\">\n",
        "      <a href=\"https://github.com/phil-daniel\" target=\"_blank\" title=\"View Phillip's profile on GitHub\">\n",
        "        <img src=\"https://github.com/phil-daniel.png?size=100\"\n",
        "             alt=\"phil-daniel's GitHub avatar\"\n",
        "             width=\"100\"\n",
        "             height=\"100\">\n",
        "      </a>\n",
        "    </td>\n",
        "    <!-- Text Content Cell -->\n",
        "    <td bgcolor=\"#d7e6ff\">\n",
        "      <h2><font color='black'>This notebook was contributed by <a href=\"https://github.com/phil-daniel\" target=\"_blank\"><font color='#217bfe'><strong>Phillip</strong></font></a> as part of Google Summer of Code 2025.</font></h2>\n",
        "      <h5><font color='black'>Find more information about the Gemini-Batcher project and more chunking and batching examples <a href=\"URL\"><font color=\"#078efb\">here</font></a>.</h5></font><br>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "This notebook introduces two useful techniques for working with large inputs:\n",
        "- Batching - Combining multiple inputs into a single request\n",
        "- Chunking - Splitting long inputs into multiple smaller pieces.\n",
        "Together, these methods can help models process large inputs more efficiently whilst staying within their token limits.\n",
        "\n",
        "This guide provides simple examples for each of the techniques, as well as discussing more complex strategies that can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "monLpKy7423V"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mi7BrkD44MF"
      },
      "source": [
        "### Install SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQJjzmYgH3sX"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\"  # Install the Python SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](../quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wltbMJLIIXGk"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL54YG-kMDVF"
      },
      "source": [
        "Now select the model you want to use in this guide, either by selecting one in the list or writing it down. Keep in mind that some models, like the 2.5 ones are thinking models and thus take slightly more time to respond (cf. [thinking notebook](./Get_started_thinking.ipynb) for more details and in particular learn how to switch the thiking off)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lf6FamchMDsk"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite-preview-06-17\",\"gemini-2.0-flash\",\"gemini-2.5-flash\",\"gemini-2.5-pro\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c6a98b628d"
      },
      "source": [
        "### Loading example material & additional libraries\n",
        "\n",
        "Each example uses the same sample material - a video lecture transcript and a set of questions about the lecture - with the goal being for the model to answer each question using only information from the transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a8de9ede6fb",
        "outputId": "bde42a40-e363-46b6-8123-d59f7f9e58d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example question: What is the goal of MIT 6.00 (Introduction to Computer Science and Programming)?\n"
          ]
        }
      ],
      "source": [
        "from google.genai import types\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "\n",
        "questions = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/questions.txt\").text.split('\\n')\n",
        "content = requests.get(\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/examples/demo_files/content.txt\").text\n",
        "\n",
        "print(f'Example question: {questions[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5i8tMZKIeUu"
      },
      "source": [
        "## Batching\n",
        "\n",
        "Batching describes the process of combining multiple individual API calls together into a single API call. Imagine you need to buy three things from a shop, rather than going to the shop three separate times, buying one item each time, it would be more efficient to only go to the shop once, getting everything you need. The technique can provide multiple benefits, including:\n",
        "- Reduced latency - Rather than having to make repeated HTTP calls, only a single one must be made, reducing latency. In addition, since many LLM APIs have rate limits, the number of requests which can be made may be limited.\n",
        "- Improved cost efficiency - In some situations, combining your inputs into a single API call can reduce the number of tokens required. For example, given a paragraph costing 400 tokens to process, and 5 questions each costing 10 tokens, asking the questions one at a time would take ≈ (400 + 10) * 5 = 2050 tokens, whereas batching the questions would only take ≈ 400 + (10 * 5) = 450 tokens, giving a signficant improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e51cc6bca0a9"
      },
      "source": [
        "### Batching example 1 - no batching (baseline)\n",
        "\n",
        "In this example, the baseline number of tokens required to answer the first five questions is calculated. Each question is sent to the model sequentially, along with the entire transcript.\n",
        "\n",
        "The response is returned in JSON format for easier comparison to the batched example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0134056a9496",
        "outputId": "5fd68b4c-943d-43f1-97d4-1362f6343166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Question: How is the course structured (lectures, recitations, workload)?\n",
            "Response: The course is structured with two hours of lectures per week, held on Tuesdays and Thursdays at 11:00, and one hour of recitation per week on Fridays. Students are expected to dedicate nine hours a week to outside-the-class work, primarily focused on problem sets involving Python programming. Recitations cover material not found in lectures or readings, and attendance is expected.\n",
            "Total input tokens used with no batching: 66239\n",
            "Total output tokens used with no batching: 543\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"Answer the questions using *only* the content provided, with each answer being a different string in the JSON response.\"\n",
        "\n",
        "total_input_tokens_no_batching = 0\n",
        "total_output_tokens_no_batching = 0\n",
        "\n",
        "for question in questions[:5]:\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        config=types.GenerateContentConfig(\n",
        "            response_mime_type=\"application/json\",\n",
        "            response_schema=list[str],\n",
        "            system_instruction=system_prompt,\n",
        "        ),\n",
        "        contents=[f'Content:\\n{content}', f'\\nQuestion:\\n{question}']\n",
        "    )\n",
        "    total_input_tokens_no_batching += response.usage_metadata.prompt_token_count\n",
        "    total_output_tokens_no_batching += response.usage_metadata.candidates_token_count\n",
        "\n",
        "print(f'Sample Question: {questions[4]}\\nResponse: {json.loads(response.text)[0]}')\n",
        "\n",
        "print (f'Total input tokens used with no batching: {total_input_tokens_no_batching}')\n",
        "print (f'Total output tokens used with no batching: {total_output_tokens_no_batching}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fae3742ac39"
      },
      "source": [
        "### Batching example 2 - with batching\n",
        "In this example, the model is asked the same five questions, but rather than being asked individually, they are answered all at once. This results in a significant reduction in the number of input tokens used as the model is only provided with the large content once rather than five times.\n",
        "\n",
        "The response is returned in JSON format to allow for easier separation of each question's answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5c7cb186719",
        "outputId": "ac518e3d-a176-43fa-d3dd-76f5a90a3fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Question: How is the course structured (lectures, recitations, workload)?\n",
            "Response: The course is structured with two hours of lecture per week (Tuesdays and Thursdays at 11:00), one hour of recitation per week (on Fridays), and nine hours per week of outside-the-class work, primarily focused on problem sets involving programming in Python.\n",
            "Total input tokens used with batching: 13299\n",
            "Total output tokens used with batching: 404\n"
          ]
        }
      ],
      "source": [
        "system_prompt = \"Answer the questions using *only* the content provided, with each answer being a different string in the JSON response.\"\n",
        "\n",
        "batched_questions = (\"\\n\").join(questions[:5])\n",
        "\n",
        "batched_response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=list[str],\n",
        "        system_instruction=system_prompt,\n",
        "        thinking_config=types.ThinkingConfig(thinking_budget=0,)\n",
        "    ),\n",
        "    contents=[f'Content:\\n{content}', f'\\nQuestions:\\n{batched_questions}']\n",
        ")\n",
        "\n",
        "answers = batched_response.text\n",
        "batched_answers = json.loads(answers.strip())\n",
        "\n",
        "print(f'Sample Question: {questions[4]}\\nResponse: {batched_answers[-1]}')\n",
        "\n",
        "total_input_tokens_with_batching = batched_response.usage_metadata.prompt_token_count\n",
        "total_output_tokens_with_batching = batched_response.usage_metadata.candidates_token_count\n",
        "\n",
        "print (f'Total input tokens used with batching: {total_input_tokens_with_batching}')\n",
        "print (f'Total output tokens used with batching: {total_output_tokens_with_batching}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6VN_1YQVEZq"
      },
      "source": [
        "### Batching results\n",
        "From running the two examples above, it's clear that batching can significantly reduce the number of input tokens needed to handle the same number of queries, making requests more efficient and cost-effective.\n",
        "\n",
        "The larger the content chunk, the more effective batching becomes, since sending multiple queries individually requires repeating the same large content block each time. With batching, that shared context is only included once, so the relative savings grow with the chunk size.\n",
        "\n",
        "One side effect of batching is that, when using the exact same prompt, the responses tend to be shorter, as reflected in the lower number of output tokens. However, it is possible to adjust this behaviour by altering the system prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dd6be5ea312"
      },
      "source": [
        "## Chunking\n",
        "\n",
        "Chunking is the opposite of batching and describes the process of breaking down a large input into multiple smaller pieces, referred to as chunks. Once again taking a real life example, imagine you are eating a steak, it is too large to eat in a single mouthful so instead you cut it into pieces and eat a piece at a time.\n",
        "\n",
        "In the context of LLMs, models have token limits, which restrict the amount of data that can be injested in a single API call, so developers must be aware of the amount of content being transmitted to the model.\n",
        "\n",
        "There are also other benefits to using chunking, including:\n",
        "- Improved Performance - If an error occurs during API calls, only the individual chunk needs to be reprocessed rather than the entire input, which is significantly quicker."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c9bf1e3b933"
      },
      "source": [
        "### Chunking techniques\n",
        "\n",
        "Since the Gemini LLM is natively multimodal, the various media types will require custom chunking strategies. In the folowing examples, only simple text chunking methods are demonstrated, however techniques for other media types are discussed later.\n",
        "\n",
        "It is also worth noting that the Google Gemini Models come with large context windows (1,048,576 input tokens for 2.5 Pro and Flash), so chunking may not be needed in some use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "098b47a25f5f"
      },
      "source": [
        "#### Fixed chunking\n",
        "\n",
        "In fixed chunking, the content is split into non-overlapping chunks each containing a set number of characters, in this case 10,000. An example of this can be seen below.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/docs/concepts/images/fixed_chunking.svg\" alt=\"A visual example of fixed chunking.\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5e28c86aaa",
        "outputId": "8f154287-0aab-4c3f-bef2-427c265a63df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 6\n"
          ]
        }
      ],
      "source": [
        "chunk_char_size = 10000\n",
        "chunked_content = []\n",
        "chunk_count = math.ceil(len(content) / chunk_char_size)\n",
        "\n",
        "for i in range(chunk_count):\n",
        "    chunk_start_pos = i * chunk_char_size\n",
        "    chunk_end_pos = min(chunk_start_pos + chunk_char_size, len(content))\n",
        "    chunked_content.append(content[chunk_start_pos : chunk_end_pos])\n",
        "\n",
        "print(f'Number of chunks: {len(chunked_content)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d4875ea76c4"
      },
      "source": [
        "#### Sliding window chunking\n",
        "\n",
        "One disadvantage of fixed chunking is that since it breaks context at arbitrary positions, important information may get split between chunks, meaning that neither chunk contains enough information to fully answer a question.\n",
        "\n",
        "A simple solution to this is to follow a sliding window approach, where an overlap (called the window) between adjacent chunks is introduced. This increases the likelihood that a complete answer can be found within a single chunk, however can also increase the total number of chunks.\n",
        "\n",
        "An example of this can be seen below.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/phil-daniel/gemini-batcher/refs/heads/main/docs/concepts/images/sliding_window.svg\" alt=\"A visual example of sliding window chunking.\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5909a0a42cfb",
        "outputId": "a6d081d0-f044-409e-d987-7ecf99f08c1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 8\n"
          ]
        }
      ],
      "source": [
        "chunk_char_size = 10000\n",
        "window_char_size = 2500\n",
        "\n",
        "chunked_content = []\n",
        "chunk_count = math.ceil(len(content) / (chunk_char_size - window_char_size))\n",
        "\n",
        "for i in range(chunk_count):\n",
        "    chunk_start_pos = i * (chunk_char_size - window_char_size)\n",
        "    chunk_end_pos = min(chunk_start_pos + chunk_char_size, len(content))\n",
        "    chunked_content.append(content[chunk_start_pos : chunk_end_pos])\n",
        "\n",
        "print(f'Number of chunks: {len(chunked_content)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After generating the content chunks, each one can then be sent to the Gemini API individually, ensuring that the input stays within the token limit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f723a895b76b"
      },
      "source": [
        "### Other chunking techniques\n",
        "\n",
        "Below are several other chunking methods for different content types. It is not an exhaustive list and depending on your use case, a combination of these methods or an entirely different technique may provide better results.\n",
        "\n",
        "- Text\n",
        "    - Semantic Chunking: This involves breaking down the content into chunks based on semantic meaning. Here sentences are grouped together if they discuss similar topics, making it more likely that a question can be answered entirely by a single chunk. One implementation of this would involve calcuating the embeddings of each sentence using `SentenceTransformer` and then computing the cosine similarity of each sentence. This could also be extended to batching questions to chunks by comparing their cosine similarity.\n",
        "- Audio\n",
        "    - Fixed/Sliding Window Chunking by duration: For audio, similar techniques can be used, rather than chunking by the number of sentences, the input can be split based on time duration.\n",
        "    - Text Methods via Transcripts: Many models, such as Gemini, can be used to create a transcript of an audio file. This allows the text based methods (fixed/sliding window/semantic) to be used, as both models also provide timestamps for when each sentence occured.\n",
        "    - Speaker Diarization: Analysis can also be completed on the audio itself to detect when the speaker changes or there is a natural break in speech, which can also often act as good chunking positions. One common library for this use is `pyannote.audio`.\n",
        "- Video\n",
        "    - Audio Methods: Each of the methods mentioned when discussing the audio techniques can also be used for video content by isolating the audio.\n",
        "    - Visual Content: Finally, you could analyse the pictures shown in the video to detect a change in the scene, for example a camera cut, which could provide a good chunking position. A useful library for this is `PySceneDetect` which detects when visual scene changes occur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7944b133b4ca"
      },
      "source": [
        "## Combining batching and chunking\n",
        "\n",
        "In general, larger batches increase the number of output tokens required, while larger chunks increase the number of input tokens required. This means that by combining batching and chunking, you can maximise the model's token usage, reducing the number of API calls required to complete a task.\n",
        "\n",
        "A simple way to do this is to use a binary search-style algorithm: repeatedly call the model, and if the input token limit is exceeded, split the content into two smaller chunks; if the output token limit is exceeded, reduce the batch size by half. This is repeated until both the input and model's response fit within the token limits.\n",
        "\n",
        "This can be implemented using the following functionalities of the Gemini library:\n",
        "- The input token limit of the model being used. This can be accessed using `client.models.get(model = model_name).input_token_limit`\n",
        "- The `count_tokens()` function (described [here](https://ai.google.dev/api/tokens#example-request)) to check the input token size, this can then be compared to be limit.\n",
        "- The `FinishReason` of an API call (i.e. `response.candidates[0].finish_reason`). If this is equal to `types.FinishReason.MAX_TOKENS` then the response ended because the output token limit was exceeded. More information about this can be found [here](https://ai.google.dev/api/generate-content#FinishReason)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4677dd58e9b5"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Check out the [Gemini Documentation](https://ai.google.dev/gemini-api/docs) for more information.\n",
        "- Check out the [Gemini-Batcher](https://github.com/phil-daniel/gemini-batcher) Google Summer of Code project for more chunking and batching examples.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "batching_and_chunking.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "gsoc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
