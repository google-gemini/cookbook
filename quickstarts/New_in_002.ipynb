{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8968a502d25e"
      },
      "source": [
        "##### Copyright 2025 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ca23c3f523a7"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwH5rYtX5Xco"
      },
      "source": [
        "# What's new in Gemini-1.5-pro-002 and Gemini-1.5-flash-002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ad518df45c"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/New_in_002.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4RV6XuQLjCc"
      },
      "source": [
        "This notebook explores the new options added with the 002 versions of the 1.5 series models:\n",
        "\n",
        "* Candidate count\n",
        "* Presence and frequency penalties\n",
        "* Response logprobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5ZjsAuEGK6i"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnl6PeGKOfcA"
      },
      "source": [
        "Install a `002` compatible version of the SDK:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xVTnXHg_nxMC"
      },
      "outputs": [],
      "source": [
        "%pip install -q \"google-genai>=1.7.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLASVngmOm8K"
      },
      "source": [
        "import the package and give it your API-key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B6fY-RbxJl78"
      },
      "outputs": [],
      "source": [
        "from google import genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sRmN-qzhM47E"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d-e1KfaOxlJ"
      },
      "source": [
        "Import other packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "giN4GuufOu02"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJO3Cz7UO0Ms"
      },
      "source": [
        "Check available 002 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CTBDsceYMh2S",
        "outputId": "3d52be91-c8b0-4677-9775-a827ab817f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-flash-002\n",
            "models/imagen-3.0-generate-002\n"
          ]
        }
      ],
      "source": [
        "for model in genai.list_models():\n",
        "  if '002' in model.name:\n",
        "    print(model.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RCFdQUXsIDcm"
      },
      "outputs": [],
      "source": [
        "model_name = \"models/gemini-1.5-flash-002\"\n",
        "test_prompt=\"Why don't people have tails\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBiweaL6LLKm"
      },
      "source": [
        "## Quick refresher on `config` [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bwwgGRtnLOqJ"
      },
      "outputs": [],
      "source": [
        "from google.genai import types\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=\"hello\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=5,\n",
        "        temperature=1.0\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sDxl0g-LQdx"
      },
      "source": [
        "Note:\n",
        "\n",
        "* Each `generate_content` request is sent with a `config` (`chat.send_message` uses `generate_content`).\n",
        "* You can set the `config` by passing it in the arguments to `generate_content` (or `chat.send_message`).\n",
        "* Any `config` attributes set in `generate_content` override the attributes set on the model.\n",
        "* You can pass the `generation_config` as either a Python `dict`, or a `genai.GenerationConfig`.\n",
        "* If you're ever unsure about the parameters of `generation_config` check `genai.GenerationConfig`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cu9hYM4H8Cz"
      },
      "source": [
        "## Candidate count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_D56ekoO4pd"
      },
      "source": [
        "With 002 models you can now use `candidate_count > 1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xr6DXm9_IGEJ"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=model_name,\n",
        "    contents=test_prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        candidate_count=2\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg9Qdl1oPBX9"
      },
      "source": [
        "But note that the `.text` quick-accessor only works for the simple 1-candidate case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gxNgE6_SI7az",
        "outputId": "8f387aa9-97de-4c2a-f1d7-9f5566a5a0ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_genai.types:there are 2 candidates, returning text result from the first candidate. Access response.candidates directly to get the result from other candidates.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  response.text # Fails with multiple candidates, sorry!\n",
        "except ValueError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDCqgP7YPHur"
      },
      "source": [
        "With multiple candidates you have to handle the list of candidates yourself:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b8Ud37EjJCfc",
        "outputId": "6b4e8f30-cb5c-4450-9faf-5806f589ca86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Humans don't have tails because of evolutionary changes over millions of years.  Our primate ancestors had tails, but as humans evolved, the genes responsible for tail development were suppressed.  This wasn't a single event, but a gradual process driven by natural selection.  While the exact reasons are still being researched, several hypotheses contribute:\n\n* **Bipedalism:**  As our ancestors transitioned to walking upright, a tail became less advantageous.  Tails are helpful for balance in quadrupedal animals, but for bipeds, they would have been more of a hindrance.  The energy required to maintain and control a tail might have been better allocated to other aspects of survival and reproduction.\n\n* **Reduced need for arboreal locomotion:**  Our ancestors spent less time in trees and more time on the ground.  Tails are useful for gripping branches, so as arboreal life decreased, the selective pressure for a tail lessened.\n\n* **Genetic mutations:**  Random genetic mutations that affected tail development may have been favored by natural selection, especially if they offered other benefits or didn't create significant disadvantages.  These mutations could have gradually reduced tail size until it was essentially vestigial, and then eventually disappeared altogether.\n\nIt's important to note that while humans typically lack external tails, the coccyx (tailbone) is a remnant of our tailed ancestry.  It's a vestigial structure, meaning it's a leftover from our evolutionary past that no longer serves its original function.  In rare cases, humans are born with rudimentary tails, which usually require surgical removal.  This shows that the genetic mechanisms for tail development haven't completely disappeared, but they are usually suppressed.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "-------------"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Humans don't have tails because of evolutionary changes over millions of years.  Our ancestors had tails, but as humans evolved, the genes controlling tail development were suppressed.  The exact reasons are complex and not fully understood, but several theories exist:\n\n* **Loss of Functionality:**  As our primate ancestors transitioned to upright walking and arboreal life became less crucial, the need for a prehensile tail diminished.  Natural selection favored individuals with less prominent or even absent tails, as they may have offered no significant survival advantage and might even have presented disadvantages (like increased vulnerability during falls).\n\n* **Developmental Changes:**  Changes in the timing and expression of genes during embryonic development led to the shortening of the tail.  The coccyx (tailbone) is a vestigial remnant of this tail.\n\n* **Energetic Costs:** Maintaining a tail requires energy.  As humans evolved larger brains and more complex behaviors, resources may have been better allocated to other functions than maintaining a tail.\n\nIn short, the absence of a tail in humans is a result of a gradual evolutionary process where the selective pressures favored individuals with reduced or absent tails, leading to the eventual suppression of tail development in our species.  It's a testament to the power of natural selection and adaptation over vast timescales.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "-------------"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for candidate in response.candidates:\n",
        "  display(Markdown(candidate.content.parts[0].text))\n",
        "  display(Markdown(\"-------------\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUxBkmsPYvq"
      },
      "source": [
        "The response contains multiple full `Candidate` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fOxbI7oSRNV_",
        "outputId": "72886604-ec6b-41f6-8c26-1c36b6347190",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Humans don't have tails because of evolutionary changes over millions of years.  Our primate ancestors had tails, but as humans evolved, the genes responsible for tail development were suppressed.  This wasn't a single event, but a gradual process driven by natural selection.  While the exact reasons are still being researched, several hypotheses contribute:\\n\\n* **Bipedalism:**  As our ancestors transitioned to walking upright, a tail became less advantageous.  Tails are helpful for balance in quadrupedal animals, but for bipeds, they would have been more of a hindrance.  The energy required to maintain and control a tail might have been better allocated to other aspects of survival and reproduction.\\n\\n* **Reduced need for arboreal locomotion:**  Our ancestors spent less time in trees and more time on the ground.  Tails are useful for gripping branches, so as arboreal life decreased, the selective pressure for a tail lessened.\\n\\n* **Genetic mutations:**  Random genetic mutations that affected tail development may have been favored by natural selection, especially if they offered other benefits or didn't create significant disadvantages.  These mutations could have gradually reduced tail size until it was essentially vestigial, and then eventually disappeared altogether.\\n\\nIt's important to note that while humans typically lack external tails, the coccyx (tailbone) is a remnant of our tailed ancestry.  It's a vestigial structure, meaning it's a leftover from our evolutionary past that no longer serves its original function.  In rare cases, humans are born with rudimentary tails, which usually require surgical removal.  This shows that the genetic mechanisms for tail development haven't completely disappeared, but they are usually suppressed.\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.3698437304260432, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None), Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Humans don't have tails because of evolutionary changes over millions of years.  Our ancestors had tails, but as humans evolved, the genes controlling tail development were suppressed.  The exact reasons are complex and not fully understood, but several theories exist:\\n\\n* **Loss of Functionality:**  As our primate ancestors transitioned to upright walking and arboreal life became less crucial, the need for a prehensile tail diminished.  Natural selection favored individuals with less prominent or even absent tails, as they may have offered no significant survival advantage and might even have presented disadvantages (like increased vulnerability during falls).\\n\\n* **Developmental Changes:**  Changes in the timing and expression of genes during embryonic development led to the shortening of the tail.  The coccyx (tailbone) is a vestigial remnant of this tail.\\n\\n* **Energetic Costs:** Maintaining a tail requires energy.  As humans evolved larger brains and more complex behaviors, resources may have been better allocated to other functions than maintaining a tail.\\n\\nIn short, the absence of a tail in humans is a result of a gradual evolutionary process where the selective pressures favored individuals with reduced or absent tails, leading to the eventual suppression of tail development in our species.  It's a testament to the power of natural selection and adaptation over vast timescales.\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, avg_logprobs=-0.3745238564231179, grounding_metadata=None, index=1, logprobs_result=None, safety_ratings=None)], create_time=None, response_id=None, model_version='gemini-1.5-flash-002', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=607, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=607)], prompt_token_count=7, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=7)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=614), automatic_function_calling_history=[], parsed=None)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g86txlIpW3FE"
      },
      "source": [
        "## Penalties\n",
        "\n",
        "The `002` models expose `penalty` arguments that let you affect the statistics of output tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw5ZwCCCUR2d"
      },
      "source": [
        "### Presence penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PeJ2GhBW537"
      },
      "source": [
        "The `presence_penalty` penalizes tokens that have already been used in the output, so it induces variety in the model's output. This is detectible if you count the unique words in the output.\n",
        "\n",
        "Here's a function to run a prompt a few times and report the fraction of unique words (words don't map perfectly to tokens but it's a simple way to see the effect)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ecolt7BNHUr2"
      },
      "outputs": [],
      "source": [
        "from statistics import mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1FbjPwM3ZIaH"
      },
      "outputs": [],
      "source": [
        "def unique_words(prompt, N=10):\n",
        "  responses = []\n",
        "  vocab_fractions = []\n",
        "\n",
        "  for n in range(N):\n",
        "    response = client.models.generate_content(\n",
        "        model=model_name,\n",
        "        contents=prompt\n",
        "    )\n",
        "\n",
        "    responses.append(response)\n",
        "\n",
        "    words = response.text.lower().split()\n",
        "    score = len(set(words))/len(words)\n",
        "    print(score)\n",
        "    vocab_fractions.append(score)\n",
        "\n",
        "  return vocab_fractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "n2PqpDPIZpr6"
      },
      "outputs": [],
      "source": [
        "prompt='Tell me a story'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6GTXnUVlYyY9",
        "outputId": "7b48ab64-91d9-44a6-9ec6-440f02f68f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.562962962962963\n",
            "0.5968586387434555\n",
            "0.5833333333333334\n",
            "0.5846867749419954\n",
            "0.5643153526970954\n",
            "0.5961995249406176\n",
            "0.6004016064257028\n",
            "0.6240786240786241\n",
            "0.6356382978723404\n",
            "0.6209476309226932\n"
          ]
        }
      ],
      "source": [
        "# baseline\n",
        "v = unique_words(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZwFZBP68wKp3",
        "outputId": "73b8dced-5625-4cd5-e22a-56326d9ab6a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5969422746918821"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlM8BVflUpAj",
        "outputId": "ae5b1936-0a48-40b4-eff4-7e4852410206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6214833759590793\n",
            "0.5617529880478087\n",
            "0.5894495412844036\n",
            "0.5789473684210527\n",
            "0.5781990521327014\n",
            "0.6389684813753582\n",
            "0.6061320754716981\n",
            "0.5727482678983834\n",
            "0.5864485981308412\n",
            "0.565410199556541\n"
          ]
        }
      ],
      "source": [
        "# the penalty encourages diversity in the oputput tokens.\n",
        "v = unique_words(prompt, generation_config=dict(presence_penalty=1.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJo-_6_owQfu",
        "outputId": "96c72c43-b10d-431c-b6f6-7eeb36d96047"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5899539948277868"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-CztHdqTc__",
        "outputId": "94b5c15e-5337-483f-df9d-5a6bae2aa0c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5555555555555556\n",
            "0.6472148541114059\n",
            "0.5839598997493735\n",
            "0.6132075471698113\n",
            "0.5858369098712446\n",
            "0.5823389021479713\n",
            "0.5895691609977324\n",
            "0.5978021978021978\n",
            "0.5604166666666667\n",
            "0.5741626794258373\n"
          ]
        }
      ],
      "source": [
        "# a negative penalty discourages diversity in the output tokens.\n",
        "v = unique_words(prompt, generation_config=dict(presence_penalty=-1.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1eWaOAOwWrE",
        "outputId": "70c0e5f7-2fea-4bb5-e537-d57cc6212b5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5890064373497796"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7LkwvBMCjd"
      },
      "source": [
        "The `presence_penalty` has a small effect on the vocabulary statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_Anpg6YRPEx"
      },
      "source": [
        "### Frequency Penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-m1BaNuXVoY"
      },
      "source": [
        "Frequency penalty is similar to the `presence_penalty` but  the penalty is multiplied by the number of times a token is used. This effect is much stronger than the `presence_penalty`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZw9D5ZvWnmq"
      },
      "source": [
        "The easiest way to see that it works is to ask the model to do something repetitive. The model has to get creative while trying to complete the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju9AIzBJ_-dR"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(model_name)\n",
        "response = model.generate_content(contents='please repeat \"Cat\" 50 times, 10 per line',\n",
        "                                  generation_config=dict(frequency_penalty=1.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9eixhvwS6It",
        "outputId": "12105c8c-eeb8-410a-c16c-3552e8dcd715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat CaT CaT\n",
            "Cat cat cat cat cat cat cat cat cat cat\n",
            "Cat Cat Cat Cat Cat Cat Cat Cat Cat Cat\n",
            "Cat Cat Cat Cat Cat Cat Cat CaT CaT CaT\n",
            "Cat cat cat cat cat cat cat cat cat cat\n",
            "Cat Cat Cat Cat Cat Cat Cat CaT CaT CaT\n",
            "Cat CAT CAT CAT CAT CAT cAT cAT cAT CA\n",
            "t Cat Cat Cat Cat cat cat cat cat CAT\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMQhl2bJnk1"
      },
      "source": [
        "Since the frequency penalty accumulates with usage, it can have a much stronger effect on the output compared to the presence penalty.\n",
        "\n",
        "> Caution: Be careful with negative frequency penalties: A negative penalty makes a token more likely the more it's used. This positive feedback quickly leads the model to just repeat a common token until it hits the `max_output_tokens` limit (once it starts the model can't produce the `<STOP>` token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48HdqCUK2oT9"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\n",
        "    prompt,\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=400,\n",
        "        frequency_penalty=-2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QpGQoHU27zK",
        "outputId": "59e5f9a5-d454-4c14-89cd-ea32ed216e2a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "Elara, a wisp of a girl with eyes the colour of a stormy sea, lived in a lighthouse perched precariously on the edge of the Whispering Cliffs.  Her only companions were the relentless rhythm of the waves and the lonely cries of the gulls.  Her father, the lighthouse keeper, was a man of the sea, his face etched with the map of the ocean's moods.  He’d taught her the language of the waves, the the the the way the wind whispered secrets to the rocks, and the constellations that guided lost ships home.\n\nOne day, a storm unlike any Elara had ever seen descended.  The lighthouse shuddered, the wind howled like a banshee, and the waves crashed against the cliffs with the fury of a thousand angry giants.  During the tempest, a ship, its masts splintered and its sails ripped, was tossed onto the rocks below.  Elara’s father, his face grim, prepared his small, sturdy boat, defying the monstrous waves to reach the stricken vessel.\n\nHe never returned.\n\nDays bled into weeks.  Elara, her heart a frozen wasteland, kept the light burning, a tiny, defiant flame against the overwhelming darkness.  She scanned the horizon every day, hoping, praying, for a sign, a glimpse of a familiar sail, a flicker of a known light.\n\nOne evening, a faint, almost imperceptible glow appeared on the horizon.  It was weak, flickering, but undeniably there.  It was a signal, a desperate plea for help.  Elara, her heart pounding, launched her father’s boat, her small form a mere speck against the immensity of the ocean.\n\nThe storm, though, had subsided.  The sea was calm. The glow was guiding.\n\nShe reached the ship, a small fishing trawler, battered, but afloat.  A lone figure, an old woman with silver hair, lay clinging to the",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Markdown(response.text)  # the, the, the, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmntZO3gNt6r",
        "outputId": "1bff6bef-bafb-41ad-ee44-80b55fb01ec4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<FinishReason.MAX_TOKENS: 2>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.candidates[0].finish_reason"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "New_in_002.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}