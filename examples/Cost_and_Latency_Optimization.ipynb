{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9752ad8f",
      "metadata": {
        "id": "62f5797ad760"
      },
      "source": [
        "##### Copyright 2026 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039a4d3a",
      "metadata": {
        "cellView": "form",
        "id": "ca23c3f523a7"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba44801c",
      "metadata": {
        "id": "73a507d02ad3"
      },
      "source": [
        "# Gemini API: Cost and latency optimization patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29be6b9b",
      "metadata": {
        "id": "21c4fb8deb6d"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Cost_and_Latency_Optimization.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=30/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe2a434",
      "metadata": {
        "id": "36389d843787"
      },
      "source": [
        "<!-- Community Contributor Badge -->\n",
        "<table>\n",
        "  <tr>\n",
        "    <!-- Author Avatar Cell -->\n",
        "    <td bgcolor=\"#d7e6ff\">\n",
        "      <a href=\"https://github.com/pankaj0695\" target=\"_blank\" title=\"View Pankaj's profile on GitHub\">\n",
        "        <img src=\"https://github.com/pankaj0695.png?size=100\"\n",
        "             alt=\"pankaj0695's GitHub avatar\"\n",
        "             width=\"100\"\n",
        "             height=\"100\">\n",
        "      </a>\n",
        "    </td>\n",
        "    <!-- Text Content Cell -->\n",
        "    <td bgcolor=\"#d7e6ff\">\n",
        "      <h2><font color='black'>This notebook was contributed by <a href=\"https://github.com/pankaj0695\" target=\"_blank\"><font color='#217bfe'><strong>Pankaj Gupta</strong></font></a>.</font></h2>\n",
        "      <h5><font color='black'><a href=\"https://www.linkedin.com/in/pankajgupta0695/\" target=\"_blank\"><font color=\"#078efb\">LinkedIn</font></a> - See <a href=\"https://github.com/pankaj0695\" target=\"_blank\"><font color=\"#078efb\"><strong>Pankaj</strong></font></a>'s other notebooks <a href=\"https://github.com/search?q=repo%3Agoogle-gemini%2Fcookbook%20%22pankaj0695%22&type=code\" target=\"_blank\"><font color=\"#078efb\">here</font></a>.</h5></font><br>\n",
        "      <!-- Footer -->\n",
        "      <font color='black'><small><em>Have a cool Gemini example? Feel free to <a href=\"https://github.com/google-gemini/cookbook/blob/main/CONTRIBUTING.md\" target=\"_blank\"><font color=\"#078efb\">share it too</font></a>!</em></small></font>\n",
        "    </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fd9fc34",
      "metadata": {
        "id": "de353802530e"
      },
      "source": [
        "This notebook demonstrates practical techniques to reduce **cost** and **latency** when using the Gemini API. You will run the same tasks using different optimization strategies and compare results with measurable metrics (tokens, time).\n",
        "\n",
        "**What you will learn:**\n",
        "1. Count and estimate tokens before making requests.\n",
        "2. Use **streaming** for faster perceived latency (time-to-first-token).\n",
        "3. Reduce context size with prompt trimming and summarization.\n",
        "4. Compare models (Flash vs Pro) for cost/latency tradeoffs.\n",
        "5. Use the **Batch API** for high-throughput, non-urgent workloads.\n",
        "\n",
        "By the end, you will have a reusable \"playbook\" for making your Gemini apps faster and cheaper."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d70860e6",
      "metadata": {
        "id": "41fbd6a3290a"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e2e827f",
      "metadata": {
        "id": "2c9e2f9af46a"
      },
      "source": [
        "### Install SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96626c6d",
      "metadata": {
        "id": "27bde1f8b07a"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-genai>=1.0.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c493f666",
      "metadata": {
        "id": "09a4ce1cb373"
      },
      "source": [
        "### Set up your API key\n",
        "\n",
        "To run the following cell, your API key must be stored in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication ![image](https://storage.googleapis.com/generativeai-downloads/images/colab_icon16.png)](../quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d36c3ab",
      "metadata": {
        "id": "26f8fcc877ac"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f78299",
      "metadata": {
        "id": "96b0da0bc5f5"
      },
      "source": [
        "### Choose a model\n",
        "\n",
        "Select a model to use throughout this guide. You will compare different models later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33258530",
      "metadata": {
        "id": "37d5307e2815"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.5-flash\" # @param [\"gemini-2.5-flash-lite\", \"gemini-2.5-flash\", \"gemini-2.5-pro\", \"gemini-3-flash-preview\", \"gemini-3-pro-preview\"] {\"allow-input\":true, isTemplate: true}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d38a4ee3",
      "metadata": {
        "id": "49f4cee2872d"
      },
      "source": [
        "### Helper functions for timing\n",
        "\n",
        "These helpers measure request latency and display results in a consistent format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d517fa8c",
      "metadata": {
        "id": "f83443316cd9"
      },
      "outputs": [],
      "source": [
        "# @title Timing helpers\n",
        "import time\n",
        "\n",
        "def timed_generate(model_id, contents, config=None):\n",
        "    start = time.perf_counter()\n",
        "    response = client.models.generate_content(\n",
        "        model=model_id,\n",
        "        contents=contents,\n",
        "        config=config\n",
        "    )\n",
        "    elapsed = time.perf_counter() - start\n",
        "    return response, elapsed\n",
        "\n",
        "\n",
        "def print_metrics(label, response, elapsed):\n",
        "    usage = response.usage_metadata\n",
        "    print(f\"\\n=== {label} ===\")\n",
        "    print(f\"Input tokens:  {usage.prompt_token_count}\")\n",
        "    print(f\"Output tokens: {usage.candidates_token_count}\")\n",
        "    print(f\"Total tokens:  {usage.total_token_count}\")\n",
        "    print(f\"Time:          {elapsed:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf34faa",
      "metadata": {
        "id": "f68e10a40367"
      },
      "source": [
        "### Define a sample task\n",
        "\n",
        "Use a consistent prompt throughout to compare optimization strategies fairly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1282853",
      "metadata": {
        "id": "5603f929862d"
      },
      "outputs": [],
      "source": [
        "SAMPLE_PROMPT = \"\"\"\n",
        "Explain the concept of neural networks to a high school student.\n",
        "Cover: what they are, how they learn and give one real-world example.\n",
        "Keep your answer under 200 words.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de8d5ed",
      "metadata": {
        "id": "a753d50d12b5"
      },
      "source": [
        "## 1. Counting tokens before requests\n",
        "\n",
        "Knowing your token usage **before** making a request helps you estimate cost and stay within context limits. Use `client.models.count_tokens` to count tokens without generating a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22bcd89a",
      "metadata": {
        "id": "b73f03bea827"
      },
      "outputs": [],
      "source": [
        "token_count = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=SAMPLE_PROMPT\n",
        ")\n",
        "\n",
        "print(f\"Prompt token count: {token_count.total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24db61a4",
      "metadata": {
        "id": "d9b7043cba33"
      },
      "source": [
        "You can also check the model's context window to ensure your input fits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88fe4cd6",
      "metadata": {
        "id": "e0679e490848"
      },
      "outputs": [],
      "source": [
        "model_info = client.models.get(model=MODEL_ID)\n",
        "\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"Input token limit:  {model_info.input_token_limit:,} tokens\")\n",
        "print(f\"Output token limit: {model_info.output_token_limit:,} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d38065",
      "metadata": {
        "id": "7aa0f16f365f"
      },
      "source": [
        "## 2. Baseline: standard synchronous request\n",
        "\n",
        "Start with a standard (non-streaming) request to establish a baseline for latency and token usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1cb53f3",
      "metadata": {
        "id": "3f8ca6f02e06"
      },
      "outputs": [],
      "source": [
        "baseline_response, baseline_time = timed_generate(MODEL_ID, SAMPLE_PROMPT)\n",
        "\n",
        "print_metrics(\"Baseline (synchronous)\", baseline_response, baseline_time)\n",
        "print(f\"\\nResponse preview: {baseline_response.text[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2d2d05",
      "metadata": {
        "id": "9b56e55a9150"
      },
      "source": [
        "## 3. Streaming for faster time-to-first-token\n",
        "\n",
        "Streaming returns chunks as they are generated, reducing **perceived latency**. The total generation time may be similar, but users see output faster.\n",
        "\n",
        "**When to use streaming:**\n",
        "- Chat or conversational UIs\n",
        "- Long responses where users want to start reading immediately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379d3cc7",
      "metadata": {
        "id": "ad2689562f8e"
      },
      "outputs": [],
      "source": [
        "start = time.perf_counter()\n",
        "first_chunk_time = None\n",
        "full_text = \"\"\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=SAMPLE_PROMPT\n",
        "):\n",
        "    if first_chunk_time is None:\n",
        "        first_chunk_time = time.perf_counter() - start\n",
        "    if chunk.text:\n",
        "        full_text += chunk.text\n",
        "\n",
        "total_time = time.perf_counter() - start\n",
        "\n",
        "print(f\"=== Streaming ===\")\n",
        "print(f\"Time to first chunk: {first_chunk_time:.2f}s\")\n",
        "print(f\"Total time:          {total_time:.2f}s\")\n",
        "print(f\"\\nResponse preview: {full_text[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351672e6",
      "metadata": {
        "id": "476859944a03"
      },
      "source": [
        "**Result:** Streaming provides faster time-to-first-token while total time remains similar."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8541c3",
      "metadata": {
        "id": "6dc4c1961245"
      },
      "source": [
        "## 4. Context reduction: prompt trimming\n",
        "\n",
        "Fewer input tokens mean lower cost and often faster responses. Techniques include:\n",
        "- Remove unnecessary context\n",
        "- Summarize long documents before including them\n",
        "- Use specific, concise instructions\n",
        "\n",
        "Compare a verbose prompt with a concise one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10405bd8",
      "metadata": {
        "id": "efa22b6c75b3"
      },
      "outputs": [],
      "source": [
        "verbose_prompt = \"\"\"\n",
        "I would really like you to help me understand something. I'm a high school \n",
        "student and I've been hearing a lot about artificial intelligence and machine \n",
        "learning lately. Could you please explain to me, in simple terms that I can \n",
        "understand, what neural networks are? I'd like to know what they are, how they \n",
        "actually learn from data, and maybe you could give me one example of how they \n",
        "are used in the real world? Please try to keep your explanation relatively \n",
        "brief, maybe around 200 words or so if possible. Thank you so much!\n",
        "\"\"\"\n",
        "\n",
        "concise_prompt = \"\"\"\n",
        "Explain neural networks to a high school student: what they are, how they \n",
        "learn, one real-world example. Under 200 words.\n",
        "\"\"\"\n",
        "\n",
        "verbose_tokens = client.models.count_tokens(model=MODEL_ID, contents=verbose_prompt)\n",
        "concise_tokens = client.models.count_tokens(model=MODEL_ID, contents=concise_prompt)\n",
        "\n",
        "print(f\"Verbose prompt: {verbose_tokens.total_tokens} tokens\")\n",
        "print(f\"Concise prompt: {concise_tokens.total_tokens} tokens\")\n",
        "print(f\"Token savings:  {verbose_tokens.total_tokens - concise_tokens.total_tokens} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e9df128",
      "metadata": {
        "id": "04afbf0bc61e"
      },
      "outputs": [],
      "source": [
        "verbose_response, verbose_time = timed_generate(MODEL_ID, verbose_prompt)\n",
        "concise_response, concise_time = timed_generate(MODEL_ID, concise_prompt)\n",
        "\n",
        "print_metrics(\"Verbose prompt\", verbose_response, verbose_time)\n",
        "print_metrics(\"Concise prompt\", concise_response, concise_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d072f77",
      "metadata": {
        "id": "856bd8ef3357"
      },
      "source": [
        "**Takeaway:** Concise prompts reduce input tokens and can improve latency, especially for large contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e55470c",
      "metadata": {
        "id": "bd58babe844f"
      },
      "source": [
        "## 5. Summarization for long documents\n",
        "\n",
        "When working with long documents, summarize them first to reduce context size for downstream tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e72f603",
      "metadata": {
        "id": "2bf716402298"
      },
      "outputs": [],
      "source": [
        "long_document = \"\"\"\n",
        "Neural networks are a subset of machine learning and are at the heart of deep \n",
        "learning algorithms. Their name and structure are inspired by the human brain, \n",
        "mimicking the way that biological neurons signal to one another. Neural networks \n",
        "are composed of node layers, containing an input layer, one or more hidden layers, \n",
        "and an output layer. Each node, or artificial neuron, connects to another and has \n",
        "an associated weight and threshold. If the output of any individual node is above \n",
        "the specified threshold value, that node is activated, sending data to the next \n",
        "layer of the network. Otherwise, no data is passed along to the next layer.\n",
        "\n",
        "Neural networks rely on training data to learn and improve their accuracy over time. \n",
        "Once these learning algorithms are fine-tuned for accuracy, they are powerful tools \n",
        "in computer science and artificial intelligence, allowing us to classify and cluster \n",
        "data at a high velocity. Tasks in speech recognition or image recognition can take \n",
        "minutes versus hours when compared to the manual identification by human experts.\n",
        "\n",
        "Deep learning neural networks, or artificial neural networks, attempt to mimic the \n",
        "human brain through a combination of data inputs, weights, and bias. These elements \n",
        "work together to accurately recognize, classify, and describe objects within the data.\n",
        "\"\"\" * 3\n",
        "\n",
        "print(f\"Original document tokens: {client.models.count_tokens(model=MODEL_ID, contents=long_document).total_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2ba84b",
      "metadata": {
        "id": "9e189e056dc0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Summarize the document\n",
        "summary_response, _ = timed_generate(\n",
        "    MODEL_ID,\n",
        "    f\"Summarize this in 2-3 sentences:\\n\\n{long_document}\"\n",
        ")\n",
        "summary = summary_response.text\n",
        "\n",
        "print(f\"Summary tokens: {client.models.count_tokens(model=MODEL_ID, contents=summary).total_tokens}\")\n",
        "print(f\"\\nSummary: {summary}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fca854e",
      "metadata": {
        "id": "e3fdb31ccecf"
      },
      "outputs": [],
      "source": [
        "# Step 2: Use summary for downstream task (instead of full document)\n",
        "question = \"Based on this context, what makes neural networks powerful?\"\n",
        "\n",
        "# Using full document\n",
        "full_context_response, full_time = timed_generate(\n",
        "    MODEL_ID,\n",
        "    f\"Context: {long_document}\\n\\nQuestion: {question}\"\n",
        ")\n",
        "\n",
        "# Using summary\n",
        "summary_context_response, summary_time = timed_generate(\n",
        "    MODEL_ID,\n",
        "    f\"Context: {summary}\\n\\nQuestion: {question}\"\n",
        ")\n",
        "\n",
        "print_metrics(\"Full document context\", full_context_response, full_time)\n",
        "print_metrics(\"Summary context\", summary_context_response, summary_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad46566",
      "metadata": {
        "id": "0c0b54ac8326"
      },
      "source": [
        "## 6. Model comparison: Flash vs Pro\n",
        "\n",
        "Different models offer tradeoffs between speed, cost, and capability:\n",
        "- **Flash/Flash-Lite**: Faster and cheaper, good for simpler tasks\n",
        "- **Pro**: Higher capability, better for complex reasoning\n",
        "\n",
        "Compare the same task across models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da1ab696",
      "metadata": {
        "id": "d0327820c478"
      },
      "outputs": [],
      "source": [
        "models_to_compare = [\n",
        "    \"gemini-2.5-flash-lite\",\n",
        "    \"gemini-2.5-flash\",\n",
        "    \"gemini-2.5-pro\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for model in models_to_compare:\n",
        "    try:\n",
        "        response, elapsed = timed_generate(model, SAMPLE_PROMPT)\n",
        "        results.append({\n",
        "            \"model\": model,\n",
        "            \"input_tokens\": response.usage_metadata.prompt_token_count,\n",
        "            \"output_tokens\": response.usage_metadata.candidates_token_count,\n",
        "            \"time\": elapsed\n",
        "        })\n",
        "        print(f\"{model}: {elapsed:.2f}s\")\n",
        "    except Exception as e:\n",
        "        print(f\"{model}: Error - {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da7ddae",
      "metadata": {
        "id": "984bf3dd473e"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== Model Comparison ===\")\n",
        "print(f\"{'Model':<25} {'Input':<10} {'Output':<10} {'Time':<10}\")\n",
        "print(\"-\" * 55)\n",
        "for r in results:\n",
        "    print(f\"{r['model']:<25} {r['input_tokens']:<10} {r['output_tokens']:<10} {r['time']:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382210a1",
      "metadata": {
        "id": "c75131f8fb79"
      },
      "source": [
        "**Guidance:**\n",
        "- Use **Flash-Lite** for simple tasks requiring speed\n",
        "- Use **Flash** for balanced performance\n",
        "- Use **Pro** when quality/reasoning is critical"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0414455d",
      "metadata": {
        "id": "0e0f06f2a79e"
      },
      "source": [
        "## 7. Batch API for offline workloads\n",
        "\n",
        "The [Batch API](../quickstarts/Batch_mode.ipynb) is ideal for non-latency-critical tasks:\n",
        "- **50% cost discount** compared to standard API\n",
        "- Process large volumes asynchronously (24-hour SLO)\n",
        "- Great for pre-processing datasets, evaluations, bulk generation\n",
        "\n",
        "**When to use Batch API:**\n",
        "- You have many requests that don't need immediate responses\n",
        "- Cost savings are more important than latency\n",
        "- Processing datasets or running evaluations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95ff6cb0",
      "metadata": {
        "id": "e1f139252b8c"
      },
      "outputs": [],
      "source": [
        "batch_prompts = [\n",
        "    \"Explain photosynthesis in one sentence.\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Describe gravity to a child.\",\n",
        "    \"What causes rainbows?\",\n",
        "    \"Explain why the sky is blue.\"\n",
        "]\n",
        "\n",
        "# Format as inline requests (list of request dicts)\n",
        "batch_requests = [\n",
        "    {\"contents\": [{\"parts\": [{\"text\": prompt}]}]}\n",
        "    for prompt in batch_prompts\n",
        "]\n",
        "\n",
        "print(f\"Prepared {len(batch_requests)} batch requests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5375af9a",
      "metadata": {
        "id": "079923ea3ebd"
      },
      "outputs": [],
      "source": [
        "# Create the batch job with inline requests\n",
        "batch_job = client.batches.create(\n",
        "    model=MODEL_ID,\n",
        "    src=batch_requests,\n",
        "    config={\"display_name\": \"cost-latency-example-batch\"}\n",
        ")\n",
        "\n",
        "print(f\"Batch job created: {batch_job.name}\")\n",
        "print(f\"State: {batch_job.state.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f35bc5d5",
      "metadata": {
        "id": "151626a4126c"
      },
      "outputs": [],
      "source": [
        "# @title Poll for batch completion (may take a few minutes)\n",
        "\n",
        "while batch_job.state.name in [\"JOB_STATE_PENDING\", \"JOB_STATE_RUNNING\"]:\n",
        "    print(f\"Status: {batch_job.state.name}... waiting 30s\")\n",
        "    time.sleep(30)\n",
        "    batch_job = client.batches.get(name=batch_job.name)\n",
        "\n",
        "print(f\"\\nFinal state: {batch_job.state.name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099deb90",
      "metadata": {
        "id": "df8aba8f11f1"
      },
      "outputs": [],
      "source": [
        "# Retrieve results (inline responses for inline batch jobs)\n",
        "if batch_job.state.name == \"JOB_STATE_SUCCEEDED\":\n",
        "    print(\"=== Batch Results ===\")\n",
        "    for i, inline_response in enumerate(batch_job.dest.inlined_responses):\n",
        "        print(f\"\\n[{i+1}] {batch_prompts[i]}\")\n",
        "        if inline_response.response:\n",
        "            text = inline_response.response.candidates[0].content.parts[0].text\n",
        "            print(f\"    → {text[:100]}...\")\n",
        "        else:\n",
        "            print(f\"    → Error: {inline_response.error}\")\n",
        "else:\n",
        "    print(f\"Batch job did not succeed. State: {batch_job.state.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b78a2552",
      "metadata": {
        "id": "0990fb4c7e62"
      },
      "source": [
        "## Summary: when to use what\n",
        "\n",
        "| Technique | Best for | Cost impact | Latency impact |\n",
        "|-----------|----------|-------------|----------------|\n",
        "| **Token counting** | Budget planning, staying within limits | Prevents overages | None |\n",
        "| **Streaming** | Chat UIs, long responses | None | Faster perceived latency |\n",
        "| **Prompt trimming** | All requests | Lower input cost | Faster |\n",
        "| **Summarization** | Long document workflows | Lower downstream cost | Faster downstream |\n",
        "| **Flash-Lite model** | Simple, speed-critical tasks | ~3x cheaper than Pro | Fastest |\n",
        "| **Flash model** | Balanced workloads | ~2x cheaper than Pro | Fast |\n",
        "| **Batch API** | Offline, bulk processing | 50% discount | Async (up to 24h) |\n",
        "\n",
        "**General recommendations:**\n",
        "1. Always count tokens to understand your usage\n",
        "2. Use streaming for user-facing applications\n",
        "3. Trim prompts and summarize long contexts\n",
        "4. Choose the right model for your task complexity\n",
        "5. Use Batch API for anything that doesn't need real-time responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a224c446",
      "metadata": {
        "id": "d4bcb788adb4"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "### Useful API references\n",
        "- [Pricing](https://ai.google.dev/pricing) - Understand token costs per model\n",
        "- [Rate limits and quotas](https://ai.google.dev/gemini-api/docs/rate-limits)\n",
        "- [Batch API documentation](https://ai.google.dev/gemini-api/docs/batch-mode)\n",
        "\n",
        "### Related examples\n",
        "- [Counting Tokens](../quickstarts/Counting_Tokens.ipynb) - Deep dive on token counting\n",
        "- [Streaming](../quickstarts/Streaming.ipynb) - More streaming patterns\n",
        "- [Batch Mode](../quickstarts/Batch_mode.ipynb) - Advanced batch workflows\n",
        "\n",
        "### Continue your discovery of the Gemini API\n",
        "- [Get started](../quickstarts/Get_started.ipynb) - Introduction to the Gemini API\n",
        "- [Caching](../quickstarts/Caching.ipynb) - Reduce costs with context caching"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Cost_and_Latency_Optimization.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
