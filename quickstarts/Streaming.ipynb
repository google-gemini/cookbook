{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Streaming Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f5bc95b9107"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Streaming.ipynb\"><img src=\"../images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "**Important note: Download this notebook and run it locally (not in Google Colab)**\n",
        "\n",
        "Streaming is not handled correctly in Google Colab yet. Currently all the stream chunks are returned together, not as they are generated. To see the correct behavior, download this notebook and run it locally using Jupyter, instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df1767a3d1cc"
      },
      "source": [
        "This notebook demonstrates streaming in the Python SDK. By default, the Python SDK returns a response after the model completes the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xuiLSV7amy3P",
        "outputId": "ba77218a-f587-4b6b-9be1-438babf62541",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q \"google-genai\" # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "79EWm0DAmy-g"
      },
      "outputs": [],
      "source": [
        "from google import genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkeZNMrw6kPD"
      },
      "source": [
        "You'll need an API key stored in an environment variable to run this notebook. See the the [Authentication quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t9O-OzeAKC_m"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUoa5q0iUuE1"
      },
      "source": [
        "## Handle streaming responses\n",
        "\n",
        "To stream responses, use [`GenerativeModel.generate_content(..., stream=True)`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content).\n",
        "\n",
        "**Note**: This cell runs with a Google Colab runtime, but does not properly show streaming due to implementation details of Colab runtimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nVWWGBsBok3m",
        "outputId": "a8e64ee8-3399-4a29-a197-fbc0f9b0b29b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "________________________________________________________________________________\n",
            " old lighthouse\n",
            "________________________________________________________________________________\n",
            " keeper, Silas, squinted at the churning grey expanse. For sixty years, the\n",
            "________________________________________________________________________________\n",
            " rhythmic pulse of the light had been his only companion, his only purpose. He'd\n",
            "________________________________________________________________________________\n",
            " seen ships swallowed whole by storms, and tiny sailboats dancing on the horizon. Tonight, however, the sea held a different unease. A silence, deeper than the usual\n",
            "________________________________________________________________________________\n",
            " lull between waves, pressed against the tower.\n",
            "\n",
            "He double-checked the lamp, polished the lens until it gleamed like a giant, benevolent eye. But the\n",
            "________________________________________________________________________________\n",
            " silence persisted. A creeping chill seeped into the stone walls, settling in his bones. He brewed himself a pot of strong tea, the familiar ritual doing little to dispel the premonition clinging to him.\n",
            "\n",
            "Then he heard it. A\n",
            "________________________________________________________________________________\n",
            " faint, almost ethereal song, carried on the wind. It was a melody he’d never heard before, haunting and beautiful, weaving tales of deep currents and forgotten shores. He climbed the winding stairs, drawn by the sound, his heart hammering\n",
            "________________________________________________________________________________\n",
            " against his ribs.\n",
            "\n",
            "Reaching the lamp room, he saw not darkness, but a shimmering, iridescent glow surrounding the light. The song grew louder, and he understood. The sea wasn't angry, it was calling. Calling to him.\n",
            "\n",
            "He knew then that his time was near. The sea, the witness\n",
            "________________________________________________________________________________\n",
            " to his life, was beckoning him home. He turned off the lamp, plunging the coast into darkness. A wave of relief washed over him, gentle and accepting. He closed his eyes, the ethereal song enveloping him, and stepped out into the storm. The last thing he felt was the embrace of the cold, dark water\n",
            "________________________________________________________________________________\n",
            ", and the comforting whisper of the sea, welcoming him into its endless depths.\n",
            "\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "  model='gemini-2.0-flash',\n",
        "  contents='Tell me a story in 300 words.'\n",
        "):\n",
        "    print(chunk.text)\n",
        "    print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswwVyHCKC_n"
      },
      "source": [
        "## Handle streaming responses asynchronously\n",
        "\n",
        "To stream responses asynchronously, use [`GenerativeModel.generate_content_async(..., stream=True)`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content_async).\n",
        "\n",
        "**Note**: These cells do NOT work with a Google Colab runtime, but do work in a local Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async for chunk in await client.aio.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=\"Write a cute story about cats.\", stream=True):\n",
        "    if chunk.text:\n",
        "        print(chunk.text)\n",
        "    print(\"_\"*80)"
      ],
      "metadata": {
        "id": "DHbwhXi2nvnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpK3p1B4KC_o"
      },
      "source": [
        "Here's a simple example of two asynchronous functions running simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "async def get_response():\n",
        "    async for chunk in await client.aio.models.generate_content(\"Write a cute story about cats.\", stream=True):\n",
        "        if chunk.text:\n",
        "            print(chunk.text)\n",
        "        print(\"_\"*80)\n",
        "\n",
        "async def something_else():\n",
        "    for i in range(5):\n",
        "        print(\"==========not blocked!==========\")\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "async def async_demo():\n",
        "    # Create tasks\n",
        "    task1 = asyncio.create_task(get_response())\n",
        "    task2 = asyncio.create_task(something_else())\n",
        "\n",
        "    # Wait for tasks to complete\n",
        "    await asyncio.gather(task1, task2)\n",
        "\n",
        "# Jupyter notebooks handle event loops for you, so await directly\n",
        "await async_demo()"
      ],
      "metadata": {
        "id": "1n5qFwvBpyQ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Streaming.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "google": {
      "image_path": "/site-assets/images/share.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed",
        "function"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
